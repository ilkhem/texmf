
@misc{190305789,
  title = {[1903.05789] {{Diagnosing}} and {{Enhancing VAE Models}}},
  file = {/Users/ilyes/Zotero/storage/26BZTM6U/1903.html},
  howpublished = {https://arxiv.org/abs/1903.05789}
}

@article{agueh2011barycenters,
  title = {Barycenters in the {{Wasserstein Space}}},
  author = {Agueh, M. and Carlier, G.},
  year = {2011},
  month = jan,
  volume = {43},
  pages = {904--924},
  issn = {0036-1410},
  doi = {10.1137/100805741},
  abstract = {In this paper, we introduce a notion of barycenter in the Wasserstein space which generalizes McCann's interpolation to the case of more than two measures. We provide existence, uniqueness, characterizations, and regularity of the barycenter and relate it to the multimarginal optimal transport problem considered by Gangbo and {\'S}wi{\k{e}}ch in [Comm. Pure Appl. Math., 51 (1998), pp. 23\textendash{}45]. We also consider some examples and, in particular, rigorously solve the Gaussian case. We finally discuss convexity of functionals in the Wasserstein space.},
  file = {/Users/ilyes/Zotero/storage/NBCZ98NJ/100805741.html},
  journal = {SIAM Journal on Mathematical Analysis},
  number = {2}
}

@article{alemi2017fixing,
  title = {Fixing a {{Broken ELBO}}},
  author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  year = {2017},
  month = nov,
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  archivePrefix = {arXiv},
  eprint = {1711.00464},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VI/Alemi_et_al_2017_Fixing_a_Broken_ELBO.pdf;/Users/ilyes/Zotero/storage/XUT58EFC/1711.html},
  journal = {arXiv:1711.00464 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,VAE},
  primaryClass = {cs, stat}
}

@article{altschuler2017nearlinear,
  title = {Near-Linear Time Approximation Algorithms for Optimal Transport via {{Sinkhorn}} Iteration},
  author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
  year = {2017},
  month = may,
  abstract = {Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances, and provides guidance towards parameter tuning for this algorithm. This result relies on a new analysis of Sinkhorn iterations that also directly suggests a new algorithm Greenkhorn with the same theoretical guarantees. Numerical simulations illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice.},
  archivePrefix = {arXiv},
  eprint = {1705.09634},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Altschuler_et_al_2017_Near-linear_time_approximation_algorithms_for_optimal_transport_via_Sinkhorn.pdf;/Users/ilyes/Zotero/storage/6JNNSIYU/1705.html},
  journal = {arXiv:1705.09634 [cs, stat]},
  keywords = {Computer Science - Data Structures and Algorithms,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{arbel2018gradient,
  title = {On Gradient Regularizers for {{MMD GANs}}},
  author = {Arbel, Michael and Sutherland, Dougal J. and Bi{\'n}kowski, Miko{\l}aj and Gretton, Arthur},
  year = {2018},
  month = nov,
  abstract = {We propose a principled method for gradient-based regularization of the critic of GAN-like models trained by adversarially optimizing the kernel of a Maximum Mean Discrepancy (MMD). We show that controlling the gradient of the critic is vital to having a sensible loss function, and devise a method to enforce exact, analytical gradient constraints at no additional cost compared to existing approximate techniques based on additive regularizers. The new loss function is provably continuous, and experiments show that it stabilizes and accelerates training, giving image generation models that outperform state-of-the art methods on \$160 \textbackslash{}times 160\$ CelebA and \$64 \textbackslash{}times 64\$ unconditional ImageNet.},
  archivePrefix = {arXiv},
  eprint = {1805.11565},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Arbel_et_al_2018_On_gradient_regularizers_for_MMD_GANs.pdf;/Users/ilyes/Zotero/storage/LEUSP8RT/1805.html},
  journal = {arXiv:1805.11565 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{arbel2018kernel,
  title = {Kernel {{Conditional Exponential Family}}},
  author = {Arbel, Michael and Gretton, Arthur},
  year = {2018},
  month = apr,
  abstract = {A nonparametric family of conditional distributions is introduced, which generalizes conditional exponential families using functional parameters in a suitable RKHS. An algorithm is provided for learning the generalized natural parameter, and consistency of the estimator is established in the well specified case. In experiments, the new method generally outperforms a competing approach with consistency guarantees, and is competitive with a deep conditional density model on datasets that exhibit abrupt transitions and heteroscedasticity.},
  archivePrefix = {arXiv},
  eprint = {1711.05363},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Arbel_Gretton_2018_Kernel_Conditional_Exponential_Family.pdf;/Users/ilyes/Zotero/storage/NMGDLKT9/1711.html},
  journal = {arXiv:1711.05363 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{arjovsky2017wasserstein,
  title = {Wasserstein {{GAN}}},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  year = {2017},
  month = jan,
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  archivePrefix = {arXiv},
  eprint = {1701.07875},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Arjovsky_et_al_2017_Wasserstein_GAN.pdf;/Users/ilyes/Zotero/storage/82VXM5JT/1701.html},
  journal = {arXiv:1701.07875 [cs, stat]},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{arjovsky2019invariant,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2019},
  month = jul,
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archivePrefix = {arXiv},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Arjovsky_et_al_2019_Invariant_Risk_Minimization.pdf;/Users/ilyes/Zotero/storage/LB9UT8SR/1907.html},
  journal = {arXiv:1907.02893 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{arwini2008information,
  title = {Information {{Geometry}}: {{Near Randomness}} and {{Near Independence}}},
  shorttitle = {Information {{Geometry}}},
  author = {Arwini, Khadiga and Dodson, Christopher T. J.},
  year = {2008},
  publisher = {{Springer-Verlag}},
  address = {{Berlin Heidelberg}},
  abstract = {This volume will be useful to practising scientists and students working in the application of statistical models to real materials or to processes with perturbations of a Poisson process, a uniform process, or a state of independence for a bivariate process. We use information geometry to provide a common differential geometric framework for a wide range of illustrative applications including amino acid sequence spacings in protein chains, cryptology studies, clustering of communications and galaxies, cosmological voids, coupled spatial statistics in stochastic fibre networks and stochastic porous media, quantum chaology. Introduction sections are provided to mathematical statistics, differential geometry and the information geometry of spaces of probability density functions.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Arwini_Dodson_2008_Information_Geometry_-_Near_Randomness_and_Near_Independence.pdf;/Users/ilyes/Zotero/storage/FMKWD7TD/9783540693918.html},
  isbn = {978-3-540-69391-8},
  language = {en},
  series = {Lecture {{Notes}} in {{Mathematics}}}
}

@article{aurenhammer1998minkowskitype,
  title = {Minkowski-{{Type Theorems}} and {{Least}}-{{Squares Clustering}}},
  author = {Aurenhammer, F. and Hoffmann, F. and Aronov, B.},
  year = {1998},
  month = jan,
  volume = {20},
  pages = {61--76},
  issn = {0178-4617, 1432-0541},
  doi = {10.1007/PL00009187},
  abstract = {.Dissecting Euclidean d -space with the power diagram of n weighted point sites partitions a given m -point set into clusters, one cluster for each region of the diagram. In this manner, an assignment of points to sites is induced. We show the equivalence of such assignments to constrained Euclidean least-squares assignments. As a corollary, there always exists a power diagram whose regions partition a given d -dimensional m -point set into clusters of prescribed sizes, no matter where the sites are placed. Another consequence is that constrained least-squares assignments can be computed by finding suitable weights for the sites. In the plane, this takes roughly O(n2m) time and optimal space O(m) , which improves on previous methods. We further show that a constrained least-squares assignment can be computed by solving a specially structured linear program in n+1 dimensions. This leads to an algorithm for iteratively improving the weights, based on the gradient-descent method. Besides having the obvious optimization property, least-squares assignments are shown to be useful in solving a certain transportation problem, and in finding a least-squares fitting of two point sets where translation and scaling are allowed. Finally, we extend the concept of a constrained least-squares assignment to continuous distributions of points, thereby obtaining existence results for power diagrams with prescribed region volumes. These results are related to Minkowski's theorem for convex polytopes. The aforementioned iterative method for approximating the desired power diagram applies to continuous distributions as well.},
  file = {/Users/ilyes/Zotero/storage/ZBQZV37J/10.html},
  journal = {Algorithmica},
  language = {en},
  number = {1}
}

@article{bach2012equivalence,
  title = {On the {{Equivalence}} between {{Herding}} and {{Conditional Gradient Algorithms}}},
  author = {Bach, Francis and {Lacoste-Julien}, Simon and Obozinski, Guillaume},
  year = {2012},
  month = mar,
  abstract = {We show that the herding procedure of Welling (2009) takes exactly the form of a standard convex optimization algorithm--namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. The experiments indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm tends to approach more often the maximum entropy distribution, shedding more light on the learning bias behind herding.},
  archivePrefix = {arXiv},
  eprint = {1203.4523},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Bach_et_al_2012_On_the_Equivalence_between_Herding_and_Conditional_Gradient_Algorithms.pdf;/Users/ilyes/Zotero/storage/L3SCTUZL/1203.html},
  journal = {arXiv:1203.4523 [cs, math, stat]},
  keywords = {Computer Science - Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{barp2017geometry,
  title = {Geometry and {{Dynamics}} for {{Markov Chain Monte Carlo}}},
  author = {Barp, Alessandro and Briol, Fran{\c c}ois-Xavier and Kennedy, Anthony D. and Girolami, Mark},
  year = {2017},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Barp_et_al_2017_Geometry_and_Dynamics_for_Markov_Chain_Monte_Carlo.pdf;/Users/ilyes/Zotero/storage/5ZT4EDPV/annurev-statistics-031017-100141.html},
  journal = {Annual Review of Statistics and Its Application},
  keywords = {ideas,projet1},
  number = {0}
}

@article{baydin2015automatic,
  title = {Automatic Differentiation in Machine Learning: A Survey},
  shorttitle = {Automatic Differentiation in Machine Learning},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2015},
  month = feb,
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives of numeric functions expressed as computer programs efficiently and accurately, used in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on some recent developments in the adoption of this technique. We aim to dispel some misconceptions that we contend have impeded the use of AD within the machine learning community.},
  archivePrefix = {arXiv},
  eprint = {1502.05767},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Baydin_et_al_2015_Automatic_differentiation_in_machine_learning_-_a_survey.pdf;/Users/ilyes/Zotero/storage/3XW4QAU8/1502.html},
  journal = {arXiv:1502.05767 [cs]},
  keywords = {68W30; 65D25; 68T05,Computer Science - Learning,Computer Science - Symbolic Computation,G.1.4,I.2.6},
  primaryClass = {cs}
}

@article{bell1995informationmaximization,
  title = {An Information-Maximization Approach to Blind Separation and Blind Deconvolution},
  author = {Bell, Anthony J. and Sejnowski, Terrence J.},
  year = {1995},
  volume = {7},
  pages = {1129--1159},
  publisher = {{MIT Press}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Bell_Sejnowski_1995_An_information-maximization_approach_to_blind_separation_and_blind_deconvolution.pdf;/Users/ilyes/Zotero/storage/FUB3SWJQ/neco.1995.7.6.html},
  journal = {Neural computation},
  number = {6}
}

@article{ben-israel1999changeofvariables,
  title = {The {{Change}}-of-{{Variables Formula Using Matrix Volume}}},
  author = {{Ben-Israel}, Adi},
  year = {1999},
  month = oct,
  volume = {21},
  pages = {300--312},
  issn = {0895-4798},
  doi = {10.1137/S0895479895296896},
  abstract = {The matrix volume is a generalization, to rectangular matrices, of the absolute value of the determinant. In particular, the matrix volume can be used in change-of-variables formulae, instead of the determinant (if the Jacobi matrix of the underlying transformation is rectangular). This result is applicable to integration on surfaces, illustrated here by several examples.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Maths/Ben-Israel_1999_The_Change-of-Variables_Formula_Using_Matrix_Volume.pdf},
  journal = {SIAM J. Matrix Anal. Appl.},
  keywords = {change-of-variables in integration,determinants,Fourier transform,generalized Pythagorean theorem,Jacobians,matrix volume,Radon transform,surface integrals},
  number = {1}
}

@article{benamou2000computational,
  title = {A Computational Fluid Mechanics Solution to the {{Monge}}-{{Kantorovich}} Mass Transfer Problem},
  author = {Benamou, Jean-David and Brenier, Yann},
  year = {2000},
  month = jan,
  volume = {84},
  pages = {375--393},
  issn = {0029-599X, 0945-3245},
  doi = {10.1007/s002110050002},
  abstract = {Summary. The L2L2L\^2 Monge-Kantorovich mass transfer problem [31] is reset in a fluid mechanics framework and numerically solved by an augmented Lagrangian method.},
  file = {/Users/ilyes/Zotero/storage/TG9I9HID/10.html},
  journal = {Numerische Mathematik},
  language = {en},
  number = {3}
}

@article{benamou2015iterative,
  title = {Iterative {{Bregman Projections}} for {{Regularized Transportation Problems}}},
  author = {Benamou, J. and Carlier, G. and Cuturi, M. and Nenna, L. and Peyr{\'e}, G.},
  year = {2015},
  month = jan,
  volume = {37},
  pages = {A1111-A1138},
  issn = {1064-8275},
  doi = {10.1137/141000439},
  abstract = {This paper details a general numerical framework to approximate solutions to linear programs related to optimal transport. The general idea is to introduce an entropic regularization of the initial linear program. This regularized problem corresponds to a  Kullback--Leibler Bregman divergence projection of a vector (representing some initial joint distribution) on the polytope of constraints. We show that for many problems related to optimal transport, the set of linear constraints can be split in an intersection of a few simple constraints, for which the projections can be computed in closed form. This allows us to make use of iterative Bregman projections (when there are only equality constraints) or, more generally, Bregman--Dykstra iterations (when  inequality constraints are involved). We illustrate the usefulness of this approach for several variational problems related to optimal transport: barycenters for the optimal transport metric, tomographic reconstruction, multimarginal optimal transport, and in particular its application to Brenier's relaxed solutions of incompressible Euler equations, partial unbalanced optimal transport, and optimal transport with capacity constraints.},
  file = {/Users/ilyes/Zotero/storage/MHZSR6D7/141000439.html},
  journal = {SIAM Journal on Scientific Computing},
  number = {2}
}

@article{bengio2012representation,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2012},
  month = jun,
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archivePrefix = {arXiv},
  eprint = {1206.5538},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Bengio_et_al_2012_Representation_Learning_-_A_Review_and_New_Perspectives.pdf;/Users/ilyes/Zotero/storage/VUD48SSK/1206.html},
  journal = {arXiv:1206.5538 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{bengio2013representation,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  shorttitle = {Representation Learning},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2013},
  volume = {35},
  pages = {1798--1828},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Bengio_et_al_2013_Representation_learning_-_A_review_and_new_perspectives.pdf;/Users/ilyes/Zotero/storage/XTQS6QIR/6472238.html},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  number = {8}
}

@article{bernstein2019orderingbased,
  title = {Ordering-{{Based Causal Structure Learning}} in the {{Presence}} of {{Latent Variables}}},
  author = {Bernstein, Daniel Irving and Saeed, Basil and Squires, Chandler and Uhler, Caroline},
  year = {2019},
  month = oct,
  abstract = {We consider the task of learning a causal graph in the presence of latent confounders given i.i.d.\textasciitilde{}samples from the model. While current algorithms for causal structure discovery in the presence of latent confounders are constraint-based, we here propose a score-based approach. We prove that under assumptions weaker than faithfulness, any sparsest independence map (IMAP) of the distribution belongs to the Markov equivalence class of the true model. This motivates the \textbackslash{}emph\{Sparsest Poset\} formulation - that posets can be mapped to minimal IMAPs of the true model such that the sparsest of these IMAPs is Markov equivalent to the true model. Motivated by this result, we propose a greedy algorithm over the space of posets for causal structure discovery in the presence of latent confounders and compare its performance to the current state-of-the-art algorithms FCI and FCI+ on synthetic data.},
  archivePrefix = {arXiv},
  eprint = {1910.09014},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Bernstein_et_al_2019_Ordering-Based_Causal_Structure_Learning_in_the_Presence_of_Latent_Variables.pdf;/Users/ilyes/Zotero/storage/48W88Z5Q/1910.html},
  journal = {arXiv:1910.09014 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{bertsekas1992auction,
  title = {Auction Algorithms for Network Flow Problems: {{A}} Tutorial Introduction},
  shorttitle = {Auction Algorithms for Network Flow Problems},
  author = {Bertsekas, Dimitri P.},
  year = {1992},
  volume = {1},
  pages = {7--66},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Bertsekas_1992_Auction_algorithms_for_network_flow_problems_-_A_tutorial_introduction.pdf;/Users/ilyes/Zotero/storage/VZ88WKKM/BF00247653.html},
  journal = {Computational optimization and applications},
  number = {1}
}

@article{bishop1994mixture,
  title = {Mixture Density Networks},
  author = {Bishop, Christopher M.},
  year = {1994},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Bishop_1994_Mixture_density_networks.pdf;/Users/ilyes/Zotero/storage/7NTRZH6C/373.html}
}

@article{blobaum2019analysis,
  title = {Analysis of Cause-Effect Inference by Comparing Regression Errors},
  author = {Bl{\"o}baum, Patrick and Janzing, Dominik and Washio, Takashi and Shimizu, Shohei and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = jan,
  volume = {5},
  pages = {e169},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.169},
  abstract = {We address the problem of inferring the causal direction between two variables by comparing the least-squares errors of the predictions in both possible directions. Under the assumption of an independence between the function relating cause and effect, the conditional noise distribution, and the distribution of the cause, we show that the errors are smaller in causal direction if both variables are equally scaled and the causal relation is close to deterministic. Based on this, we provide an easily applicable algorithm that only requires a regression in both possible causal directions and a comparison of the errors. The performance of the algorithm is compared with various related causal inference methods in different artificial and real-world data sets.},
  archivePrefix = {arXiv},
  eprint = {1802.06698},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Blöbaum_et_al_2019_Analysis_of_cause-effect_inference_by_comparing_regression_errors.pdf;/Users/ilyes/Zotero/storage/6UWHG5ET/1802.html},
  journal = {PeerJ Computer Science},
  keywords = {Computer Science - Artificial Intelligence}
}

@inproceedings{bloebaum2018causeeffect,
  title = {Cause-Effect Inference by Comparing Regression Errors},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Bloebaum, Patrick and Janzing, Dominik and Washio, Takashi and Shimizu, Shohei and Sch{\"o}lkopf, Bernhard},
  year = {2018},
  pages = {900--909},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Bloebaum_et_al_2018_Cause-effect_inference_by_comparing_regression_errors.pdf;/Users/ilyes/Zotero/storage/88HW2YZF/bloebaum18a.html}
}

@article{bogachev2012mongekantorovich,
  title = {The {{Monge}}-{{Kantorovich}} Problem: Achievements, Connections, and Perspectives},
  shorttitle = {The {{Monge}}-{{Kantorovich}} Problem},
  author = {Bogachev, Vladimir I and Kolesnikov, Aleksandr V},
  year = {2012},
  volume = {67},
  pages = {785},
  issn = {0036-0279},
  doi = {10.1070/RM2012v067n05ABEH004808},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Bogachev_Kolesnikov_2012_The_Monge-Kantorovich_problem_-_achievements,_connections,_and_perspectives.pdf;/Users/ilyes/Zotero/storage/DWM74D3I/meta.html},
  journal = {Russian Mathematical Surveys},
  language = {en},
  number = {5}
}

@article{bott2017nonparametric,
  title = {Nonparametric Estimation of a Conditional Density},
  author = {Bott, Ann-Kathrin and Kohler, Michael},
  year = {2017},
  volume = {69},
  pages = {189--214},
  abstract = {Abstract In this paper, we estimate a conditional density. In contrast to standard results in the literature in this context we assume that for each observed value of the covariate we observe a sample of the corresponding conditional distribution of size larger than one. A density estimate is defined taking into account the data from all the samples by computing a weighted average using weights depending on the covariates. The error of the density estimate is measured by the \$\$L\_1\$\$ L 1 -error. Results concerning consistency and rate of convergence of the estimate are presented, and the performance of the estimate for finite sample size is illustrated using simulated data. Furthermore, the estimate is applied to a problem in fatigue analysis.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Bott_Kohler_2017_Nonparametric_estimation_of_a_conditional_density.pdf;/Users/ilyes/Zotero/storage/I89IZAZF/v69y2017i1d10.1007_s10463-015-0535-8.html},
  journal = {Annals of the Institute of Statistical Mathematics},
  keywords = {$$L_1$$ L 1 -error,Conditional density estimation,Consistency,Rate of convergence},
  language = {en},
  number = {1}
}

@book{boucheron2013concentration,
  title = {Concentration Inequalities: {{A}} Nonasymptotic Theory of Independence},
  shorttitle = {Concentration Inequalities},
  author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year = {2013},
  publisher = {{Oxford university press}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Boucheron_et_al_2013_Concentration_inequalities_-_A_nonasymptotic_theory_of_independence.pdf;/Users/ilyes/Zotero/storage/MDJSZV7V/books.html}
}

@article{brakel2017learning,
  title = {Learning Independent Features with Adversarial Nets for Non-Linear {{ICA}}},
  author = {Brakel, Philemon and Bengio, Yoshua},
  year = {2017},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Brakel_Bengio_2017_Learning_independent_features_with_adversarial_nets_for_non-linear_ica.pdf;/Users/ilyes/Zotero/storage/YLT8ZBXP/1710.html},
  journal = {arXiv preprint arXiv:1710.05050}
}

@article{brosowski1981elementary,
  title = {An Elementary Proof of the {{Stone}}-{{Weierstrass}} Theorem},
  author = {Brosowski, Bruno and Deutsch, Frank},
  year = {1981},
  pages = {89--92},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Brosowski_Deutsch_1981_An_elementary_proof_of_the_Stone-Weierstrass_theorem.pdf;/Users/ilyes/Zotero/storage/MGJZALZH/2043993.html},
  journal = {Proceedings of the American Mathematical Society}
}

@article{buhlmann2018invariance,
  title = {Invariance, {{Causality}} and {{Robustness}}},
  author = {B{\"u}hlmann, Peter},
  year = {2018},
  month = dec,
  abstract = {We discuss recent work for causal inference and predictive robustness in a unifying way. The key idea relies on a notion of probabilistic invariance or stability: it opens up new insights for formulating causality as a certain risk minimization problem with a corresponding notion of robustness. The invariance itself can be estimated from general heterogeneous or perturbation data which frequently occur with nowadays data collection. The novel methodology is potentially useful in many applications, offering more robustness and better `causal-oriented' interpretation than machine learning or estimation in standard regression or classification frameworks.},
  archivePrefix = {arXiv},
  eprint = {1812.08233},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Causality/Bühlmann_2018_Invariance,_Causality_and_Robustness.pdf;/Users/ilyes/Zotero/storage/PMIMDHD2/1812.html},
  journal = {arXiv:1812.08233 [stat]},
  keywords = {62J99,Statistics - Methodology},
  primaryClass = {stat}
}

@book{buoncristiano2003fragments,
  title = {Fragments of Geometric Topology from the Sixties},
  author = {Buoncristiano, S.},
  year = {2003},
  publisher = {{University of Warwick, Mathematics Institute}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Buoncristiano_2003_Fragments_of_geometric_topology_from_the_sixties.pdf},
  googlebooks = {eyA3MwEACAAJ},
  language = {en}
}

@article{burda2015importance,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2015},
  month = sep,
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1509.00519},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Burda_et_al_2015_Importance_Weighted_Autoencoders.pdf;/Users/ilyes/Zotero/storage/K49LE6KY/1509.html},
  journal = {arXiv:1509.00519 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{burel1992blind,
  title = {Blind Separation of Sources: {{A}} Nonlinear Neural Algorithm},
  shorttitle = {Blind Separation of Sources},
  author = {Burel, Gilles},
  year = {1992},
  month = nov,
  volume = {5},
  pages = {937--947},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(05)80090-5},
  abstract = {In many signal processing applications, the signals provided by the sensors are mixtures of many sources. The problem of separation of sources is to extract the original signals from these mixtures. A new algorithm, based on ideas of back propagation learning, is proposed for source separation. No a priori information on the sources themselves is required, and the algorithm can deal even with nonlinear mixtures. After a short overview of previous works in that field, we will describe the proposed algorithm, then some experimental results will be discussed.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Burel_1992_Blind_separation_of_sources_-_A_nonlinear_neural_algorithm.pdf;/Users/ilyes/Zotero/storage/2DX5AXIU/S0893608005800905.html},
  journal = {Neural Networks},
  keywords = {Back propagation,BSS,High order moments,ICA,Independent component analysis,Mixture of sources,Neural networks,NICA,Nonlinear algorithms,old NICA,Separation of sources},
  number = {6}
}

@article{burgess2018understanding,
  title = {Understanding Disentangling in {$\beta$}-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  year = {2018},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Burgess_et_al_2018_Understanding_disentangling_in_β-VAE.pdf},
  journal = {arXiv preprint arXiv:1804.03599},
  keywords = {disentanglement,VAE}
}

@article{caffarelli2010free,
  title = {Free Boundaries in Optimal Transport and {{Monge}}-{{Amp{\`e}re}} Obstacle Problems},
  author = {Caffarelli, Luis A. and McCann, Robert J.},
  year = {2010},
  volume = {171},
  pages = {673--730},
  issn = {0003-486X},
  abstract = {Given compactly supported 0 {$\leq$} f, g {$\in$} L{$^1$}({$\mathbb{R}$} n ), the problem of transporting a fraction m {$\leq$} min\{{$\parallel$} f {$\parallel$} L 1 , {$\parallel$} g {$\parallel$} L 1 \} of the mass of f onto g as cheaply as possible is considered, where cost per unit mass transported is given by a cost function c, typically quadratic c(x, y) = ǀx \textemdash{} yǀ{$^2$}/2. This question is shown to be equivalent to a double obstacle problem for the Monge-Amp{\`e}re equation, for which sufficient conditions are given to guarantee uniqueness of the solution, such as f vanishing on spt g in the quadratic case. The part of f to be transported increases monotonically with m, and if spt f and spt g are separated by a hyperplane H, then this part will be separated from the balance of f by a semiconcave Lipschitz graph over the hyperplane. If f = f{$\chi$} {$\Omega$} and g = g{$\chi$} {$\Lambda$} are bounded away from zero and infinity on separated strictly convex domains {$\Omega$}, {$\Lambda$} {$\subset$} R n , for the quadratic cost this graph is shown to be a \$C\_\{\textbackslash{}text\{loc\}\}\^\{1,\textbackslash{}alpha\}\$ hypersurface in {$\Omega$} whose normal coincides with the direction transported; the optimal map between f and g is shown to be H{\"o}lder continuous up to this free boundary, and to those parts of the fixed boundary {$\partial\Omega$} which map to locally convex parts of the path-connected target region.},
  journal = {Annals of Mathematics},
  number = {2}
}

@article{camuto2020learning,
  title = {Learning {{Bijective Feature Maps}} for {{Linear ICA}}},
  author = {Camuto, Alexander and Willetts, Matthew and Paige, Brooks and Holmes, Chris and Roberts, Stephen},
  year = {2020},
  month = feb,
  abstract = {Separating high-dimensional data like images into independent latent factors remains an open research problem. Here we develop a method that jointly learns a linear independent component analysis (ICA) model with non-linear bijective feature maps. By combining these two methods, ICA can learn interpretable latent structure for images. For non-square ICA, where we assume the number of sources is less than the dimensionality of data, we achieve better unsupervised latent factor discovery than flow-based models and linear ICA. This performance scales to large image datasets such as CelebA.},
  archivePrefix = {arXiv},
  eprint = {2002.07766},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Camuto_et_al_2020_Learning_Bijective_Feature_Maps_for_Linear_ICA.pdf;/Users/ilyes/Zotero/storage/WZ75H4UM/2002.html},
  journal = {arXiv:2002.07766 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{carlier2015numerical,
  title = {Numerical Methods for Matching for Teams and {{Wasserstein}} Barycenters},
  author = {Carlier, Guillaume and Oberman, Adam and Oudet, Edouard},
  year = {2015},
  month = nov,
  volume = {49},
  pages = {1621--1642},
  issn = {0764-583X, 1290-3841},
  doi = {10.1051/m2an/2015033},
  abstract = {ESAIM: Mathematical Modelling and Numerical Analysis, an international journal on applied mathematics},
  copyright = {\textcopyright{} EDP Sciences, SMAI 2015},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Carlier_et_al_2015_Numerical_methods_for_matching_for_teams_and_Wasserstein_barycenters.pdf;/Users/ilyes/Zotero/storage/2X4DVAGM/m2an150084.html},
  journal = {ESAIM: Mathematical Modelling and Numerical Analysis},
  language = {en},
  number = {6}
}

@incollection{carrasco2007linear,
  title = {Linear {{Inverse Problems}} in {{Structural Econometrics Estimation Based}} on {{Spectral Decomposition}} and {{Regularization}}},
  booktitle = {Handbook of {{Econometrics}}},
  author = {Carrasco, Marine and Florens, Jean-Pierre and Renault, Eric},
  editor = {Heckman, James J. and Leamer, Edward E.},
  year = {2007},
  month = jan,
  volume = {6},
  pages = {5633--5751},
  publisher = {{Elsevier}},
  doi = {10.1016/S1573-4412(07)06077-1},
  abstract = {Inverse problems can be described as functional equations where the value of the function is known or easily estimable but the argument is unknown. Many problems in econometrics can be stated in the form of inverse problems where the argument itself is a function. For example, consider a nonlinear regression where the functional form is the object of interest. One can readily estimate the conditional expectation of the dependent variable given a vector of instruments. From this estimate, one would like to recover the unknown functional form. This chapter provides an introduction to the estimation of the solution to inverse problems. It focuses mainly on integral equations of the first kind. Solving these equations is particularly challenging as the solution does not necessarily exist, may not be unique, and is not continuous. As a result, a regularized (or smoothed) solution needs to be implemented. We review different regularization methods and study the properties of the estimator. Integral equations of the first kind appear, for example, in the generalized method of moments when the number of moment conditions is infinite, and in the nonparametric estimation of instrumental variable regressions. In the last section of this chapter, we investigate integral equations of the second kind, whose solutions may not be unique but are continuous. Such equations arise when additive models and measurement error models are estimated nonparametrically.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Carrasco_et_al_2007_Linear_Inverse_Problems_in_Structural_Econometrics_Estimation_Based_on_Spectral.pdf;/Users/ilyes/Zotero/storage/B75F528B/S1573441207060771.html},
  keywords = {additive models,generalized method of moments,instrumental variables,integral equation,many regressors,nonparametric estimation,Tikhonov and Landweber–Fridman regularizations}
}

@misc{centerforhistoryandnewmediazotero,
  title = {Zotero {{Quick Start Guide}}},
  author = {{Center for History and New Media}},
  howpublished = {http://zotero.org/support/quick\_start\_guide}
}

@article{centorrino2019nonparametric,
  title = {Nonparametric {{Instrumental Regressions}} with ({{Potentially Discrete}}) {{Instruments Independent}} of the {{Error Term}}},
  author = {Centorrino, Samuele and F{\`e}ve, Fr{\'e}d{\'e}rique and Florens, Jean-Pierre},
  year = {2019},
  month = may,
  abstract = {We consider a nonparametric instrumental regression model with continuous endogenous regressor where instruments are fully independent of the error term. This assumption allows us to extend the reach of this model to cases where the instrumental variable is discrete, and therefore to substantially enlarge its potential empirical applications. Under our assumptions, the regression function becomes solution to a nonlinear integral equation. We contribute to existing literature by providing an exhaustive analysis of identification and a simple iterative estimation procedure. Details on the implementation and on the asymptotic properties of this estimation algorithm are given. We conclude the paper with a simulation experiment for a binary instrument and an empirical application to the estimation of the Engel curve for food, where we show that our estimator delivers results that are consistent with existing evidence under several discretizations of the instrumental variable.},
  archivePrefix = {arXiv},
  eprint = {1905.07812},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Centorrino_et_al_2019_Nonparametric_Instrumental_Regressions_with_(Potentially_Discrete)_Instruments.pdf;/Users/ilyes/Zotero/storage/CDQGQGY9/1905.html},
  journal = {arXiv:1905.07812 [econ, stat]},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  primaryClass = {econ, stat}
}

@article{ceylan2018conditional,
  title = {Conditional {{Noise}}-{{Contrastive Estimation}} of {{Unnormalised Models}}},
  author = {Ceylan, Ciwan and Gutmann, Michael U.},
  year = {2018},
  month = jun,
  abstract = {Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.},
  archivePrefix = {arXiv},
  eprint = {1806.03664},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Ceylan_Gutmann_2018_Conditional_Noise-Contrastive_Estimation_of_Unnormalised_Models.pdf;/Users/ilyes/Zotero/storage/67P9TBFI/1806.html},
  journal = {arXiv:1806.03664 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chechik2005information,
  title = {Information {{Bottleneck}} for {{Gaussian Variables}}},
  author = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair},
  year = {2005},
  volume = {6},
  pages = {165--188},
  issn = {ISSN 1533-7928},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chechik_et_al_2005_Information_Bottleneck_for_Gaussian_Variables.pdf;/Users/ilyes/Zotero/storage/ANHUTSGS/chechik05a.html},
  journal = {Journal of Machine Learning Research},
  keywords = {IB,Information Bottleneck},
  number = {Jan}
}

@techreport{chen2001estimation,
  title = {The {{Estimation}} of {{Conditional Densities}}},
  author = {Chen, Xiaohong and Linton, Oliver B.},
  year = {2001},
  month = may,
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  abstract = {We discuss a number of issues in the smoothed nonparametric estimation of kernel conditional probability density functions for stationary processes. The kernel conditional density estimate is a ratio of joint and marginal density estimates. We point out the different implications of leading choices of bandwidths in numerator and denominator for the ability of the estimate to integrate to one and to have finite moments. Again bearing in mind different bandwidth possibilities, we discuss asymptotic theory for the estimate: asymptotic bias and variance are calculated under various conditions, an extended discussion of bandwidth choice is included, and a central limit theorem is given.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Chen_Linton_2001_The_Estimation_of_Conditional_Densities.pdf;/Users/ilyes/Zotero/storage/CP4785UF/papers.html},
  keywords = {Oliver B. Linton,SSRN,The Estimation of Conditional Densities,Xiaohong Chen},
  language = {en},
  number = {ID 1162597},
  type = {{{SSRN Scholarly Paper}}}
}

@article{chen2013causality,
  title = {Causality in {{Linear Nongaussian Acyclic Models}} in the {{Presence}} of {{Latent Gaussian Confounders}}},
  author = {Chen, Zhitang and Chan, Laiwan},
  year = {2013},
  month = mar,
  volume = {25},
  pages = {1605--1641},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00444},
  abstract = {LiNGAM has been successfully applied to some real-world causal discovery problems. Nevertheless, causal sufficiency is assumed; that is, there is no latent confounder of the observations, which may be unrealistic for real-world problems. Taking into the consideration latent confounders will improve the reliability and accuracy of estimations of the real causal structures. In this letter, we investigate a model called linear nongaussian acyclic models in the presence of latent gaussian confounders (LiNGAM-GC) which can be seen as a specific case of lvLiNGAM. This model includes the latent confounders, which are assumed to be independent gaussian distributed and statistically independent of the disturbances. To tackle the causal discovery problem of this model, first we propose a pairwise cumulant-based measure of causal directions for cause-effect pairs. We prove that in spite of the presence of latent gaussian confounders, the causal direction of the observed cause-effect pair can be identified under the mild condition that the disturbances are simultaneously supergaussian or subgaussian. We propose a simple and efficient method to detect the violation of this condition. We extend our work to multivariate causal network discovery problems. Specifically we propose algorithms to estimate the causal network structure, including causal ordering and causal strengths, using an iterative root finding-removing scheme based on pairwise measure. To address the redundant edge problem due to the finite sample size effect, we develop an efficient bootstrapping-based pruning algorithm. Experiments on synthetic data and real-world data have been conducted to show the applicability of our model and the effectiveness of our proposed algorithms.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Chen_Chan_2013_Causality_in_Linear_Nongaussian_Acyclic_Models_in_the_Presence_of_Latent.pdf;/Users/ilyes/Zotero/storage/L44YFIDK/NECO_a_00444.html},
  journal = {Neural Computation},
  number = {6}
}

@article{chen2016infogan,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  shorttitle = {{{InfoGAN}}},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  month = jun,
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  archivePrefix = {arXiv},
  eprint = {1606.03657},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chen_et_al_2016_InfoGAN_-_Interpretable_Representation_Learning_by_Information_Maximizing.pdf;/Users/ilyes/Zotero/storage/YRT56DUE/1606.html},
  journal = {arXiv:1606.03657 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chen2016interpolation,
  title = {Interpolation of {{Density Matrices}} and {{Matrix}}-{{Valued Measures}}: {{The Unbalanced Case}}},
  shorttitle = {Interpolation of {{Density Matrices}} and {{Matrix}}-{{Valued Measures}}},
  author = {Chen, Yongxin and Georgiou, Tryphon T. and Tannenbaum, Allen},
  year = {2016},
  month = dec,
  abstract = {In this note, we propose an unbalanced version of the quantum mechanical version of optimal mass transport that was based on the Lindblad equation. We formulate a natural interpolation framework between density matrices and matrix-valued measures via a quantum mechanical formulation of Fisher-Rao information and the matricial Wasserstein distance.},
  archivePrefix = {arXiv},
  eprint = {1612.05914},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chen_et_al_2016_Interpolation_of_Density_Matrices_and_Matrix-Valued_Measures_-_The_Unbalanced.pdf;/Users/ilyes/Zotero/storage/R87PD386/1612.html},
  journal = {arXiv:1612.05914 [math]},
  keywords = {Mathematics - Functional Analysis},
  primaryClass = {math}
}

@article{chen2018isolating,
  title = {Isolating {{Sources}} of {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  year = {2018},
  month = feb,
  abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \$\textbackslash{}beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \$\textbackslash{}beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
  archivePrefix = {arXiv},
  eprint = {1802.04942},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Chen_et_al_2018_Isolating_Sources_of_Disentanglement_in_Variational_Autoencoders.pdf;/Users/ilyes/Zotero/storage/3S2WZXT2/1802.html},
  journal = {arXiv:1802.04942 [cs, stat]},
  keywords = {_tablet,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chickering2002optimal,
  title = {Optimal Structure Identification with Greedy Search},
  author = {Chickering, David Maxwell},
  year = {2002},
  volume = {3},
  pages = {507--554},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Chickering_2002_Optimal_structure_identification_with_greedy_search.pdf;/Users/ilyes/Zotero/storage/S92JCC46/Chickering - 2002 - Optimal structure identification with greedy searc.pdf;/Users/ilyes/Zotero/storage/GFGDUTIE/chickering02b.html},
  journal = {Journal of machine learning research},
  number = {Nov}
}

@article{chickering2003optimal,
  title = {Optimal Structure Identification with Greedy Search},
  author = {Chickering, David Maxwell},
  year = {2003},
  month = mar,
  volume = {3},
  pages = {507--554},
  issn = {1532-4435},
  doi = {10.1162/153244303321897717},
  abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Chickering_2003_Optimal_structure_identification_with_greedy_search.pdf},
  journal = {The Journal of Machine Learning Research},
  number = {null}
}

@article{chitescu2014mongekantorovich,
  title = {Monge-{{Kantorovich}} Norms on Spaces of Vector Measures},
  author = {Chitescu, Ion and Miculescu, Radu and Nita, Lucian and Ioana, Loredana},
  year = {2014},
  month = apr,
  abstract = {One considers Hilbert space valued measures on the Borel sets of a compact metric space. A natural numerical valued integral of vector valued continuous functions with respect to vector valued functions is defined. Using this integral, different norms (we called them Monge-Kantorovich norm, modified Monge-Kantorovich norm and Hanin norm) on the space of measures are introduced, generalizing the theory of (weak) convergence for probability measures on metric spaces. These norms introduce new (equivalent) metrics on the initial compact metric space.},
  archivePrefix = {arXiv},
  eprint = {1404.4980},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chitescu_et_al_2014_Monge-Kantorovich_norms_on_spaces_of_vector_measures.pdf;/Users/ilyes/Zotero/storage/I2UB3PPU/1404.html},
  journal = {arXiv:1404.4980 [math]},
  keywords = {Mathematics - Functional Analysis,Primary: 28B05; 46G10; 46E10; 28C15. Secondary: 46B25; 46C05},
  primaryClass = {math}
}

@article{chizat2015interpolating,
  title = {An Interpolating Distance between Optimal Transport and {{Fisher}}-{{Rao}}},
  author = {Chizat, Lenaic and Schmitzer, Bernhard and Peyr{\'e}, Gabriel and Vialard, Fran{\c c}ois-Xavier},
  year = {2015},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chizat_et_al_2015_An_interpolating_distance_between_optimal_transport_and_Fisher-Rao.pdf;/Users/ilyes/Zotero/storage/HSN94F2E/1506.html},
  journal = {arXiv preprint arXiv:1506.06430}
}

@article{chizat2016scaling,
  title = {Scaling {{Algorithms}} for {{Unbalanced Transport Problems}}},
  author = {Chizat, Lenaic and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c c}ois-Xavier},
  year = {2016},
  month = jul,
  abstract = {This article introduces a new class of fast algorithms to approximate variational problems involving unbalanced optimal transport. While classical optimal transport considers only normalized probability distributions, it is important for many applications to be able to compute some sort of relaxed transportation between arbitrary positive measures. A generic class of such "unbalanced" optimal transport problems has been recently proposed by several authors. In this paper, we show how to extend the, now classical, entropic regularization scheme to these unbalanced problems. This gives rise to fast, highly parallelizable algorithms that operate by performing only diagonal scaling (i.e. pointwise multiplications) of the transportation couplings. They are generalizations of the celebrated Sinkhorn algorithm. We show how these methods can be used to solve unbalanced transport, unbalanced gradient flows, and to compute unbalanced barycenters. We showcase applications to 2-D shape modification, color transfer, and growth models.},
  archivePrefix = {arXiv},
  eprint = {1607.05816},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chizat_et_al_2016_Scaling_Algorithms_for_Unbalanced_Transport_Problems.pdf;/Users/ilyes/Zotero/storage/ZAX7JGST/1607.html},
  journal = {arXiv:1607.05816 [math]},
  keywords = {_read,65K10,Mathematics - Optimization and Control},
  primaryClass = {math}
}

@article{chizat2018global,
  title = {On the {{Global Convergence}} of {{Gradient Descent}} for {{Over}}-Parameterized {{Models}} Using {{Optimal Transport}}},
  author = {Chizat, Lenaic and Bach, Francis},
  year = {2018},
  month = may,
  abstract = {Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.},
  archivePrefix = {arXiv},
  eprint = {1805.09545},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chizat_Bach_2018_On_the_Global_Convergence_of_Gradient_Descent_for_Over-parameterized_Models.pdf;/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chizat_Bach_2018_On_the_Global_Convergence_of_Gradient_Descent_for_Over-parameterized_Models2.pdf;/Users/ilyes/Zotero/storage/R8VSAF8I/1805.html},
  journal = {arXiv:1805.09545 [cs, math, stat]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{chwialkowski2015fast,
  title = {Fast {{Two}}-{{Sample Testing}} with {{Analytic Representations}} of {{Probability Measures}}},
  author = {Chwialkowski, Kacper and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
  year = {2015},
  month = jun,
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chwialkowski_et_al_2015_Fast_Two-Sample_Testing_with_Analytic_Representations_of_Probability_Measures.pdf;/Users/ilyes/Zotero/storage/672JCJTI/1506.html},
  language = {en}
}

@article{comon1994independent,
  title = {Independent Component Analysis, a New Concept?},
  author = {Comon, Pierre},
  year = {1994},
  volume = {36},
  pages = {287--314},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Comon_1994_Independent_component_analysis,_a_new_concept.pdf;/Users/ilyes/Zotero/storage/JACVHKGF/0165168494900299.html},
  journal = {Signal processing},
  number = {3}
}

@article{cover1994elements,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M. and Thomas, Joy A. and Kieffer, John},
  year = {1994},
  volume = {36},
  pages = {509--510},
  file = {/Users/ilyes/Zotero/storage/IN5ZK9N6/scholar.html},
  journal = {SIAM Review},
  number = {3}
}

@article{cuturi1987independent,
  title = {Independent Regulation of Tumor Necrosis Factor and Lymphotoxin Production by Human Peripheral Blood Lymphocytes.},
  author = {Cuturi, M. C. and Murphy, M. and {Costa-Giomi}, M. P. and Weinmann, R. and Perussia, B. and Trinchieri, G.},
  year = {1987},
  month = jun,
  volume = {165},
  pages = {1581--1594},
  issn = {0022-1007, 1540-9538},
  doi = {10.1084/jem.165.6.1581},
  abstract = {We present evidence that human peripheral blood lymphocytes, free of contaminating monocytes, rapidly produce high levels of tumor necrosis factor (TNF) when stimulated with phorbol diester and calcium ionophore, and lower but significant levels of TNF when stimulated with mitogens. These two types of inducers act preferentially on T cells, both CD4+ and CD8+. NK cells produce TNF only when stimulated with phorbol diester and calcium ionophore, and they do so at a much lower level than T cells. The procedures used in the purification of lymphocytes and the differential ability to respond to various inducers allow us to exclude that monocytes or basophils contaminating the lymphocyte preparation participate in the production of TNF. In particular, LPS, a potent inducer of TNF production from monocytes, is unable to induce significant levels of TNF in the lymphocyte preparations. The TNF produced by lymphocytes has antigenic, physicochemical, and biochemical characteristics identical to those of the TNF produced by myeloid cell lines or monocytes upon stimulation with LPS. LT is also produced by lymphocyte preparations. Production of TNF and LT proteins in response to the different inducers is paralleled by accumulation of cytoplasmic TNF and LT mRNA. Both at mRNA and at protein levels, stimulation of T lymphocytes with phorbol diester and calcium ionophore preferentially induces TNF, whereas mitogen stimulation preferentially induces LT. Our data suggest that the TNF and LT genes, two closely linked genes encoding two partially homologous proteins with almost identical biological functions, are independently regulated in lymphocytes.},
  copyright = {\textcopyright{} 1987 Rockefeller University Press},
  file = {/Users/ilyes/Zotero/storage/5VUDV93Z/1581.html},
  journal = {Journal of Experimental Medicine},
  language = {en},
  number = {6},
  pmid = {3108447}
}

@inproceedings{cuturi2004mutual,
  title = {A Mutual Information Kernel for Sequences},
  booktitle = {Neural {{Networks}}, 2004. {{Proceedings}}. 2004 {{IEEE International Joint Conference}} On},
  author = {Cuturi, Marco and Vert, J.-P.},
  year = {2004},
  volume = {3},
  pages = {1905--1910},
  publisher = {{IEEE}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Cuturi_Vert_2004_A_mutual_information_kernel_for_sequences.pdf;/Users/ilyes/Zotero/storage/IFVVXS25/1380902.html}
}

@article{cuturi2005contexttree,
  title = {The Context-Tree Kernel for Strings},
  author = {Cuturi, Marco and Vert, Jean-Philippe},
  year = {2005},
  volume = {18},
  pages = {1111--1123},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Vert_2005_The_context-tree_kernel_for_strings.pdf;/Users/ilyes/Zotero/storage/2ZCA37F2/S089360800500170X.html},
  journal = {Neural Networks},
  number = {8}
}

@inproceedings{cuturi2005semigroup,
  title = {Semigroup Kernels on Finite Sets},
  booktitle = {Advances in {{Neural Information Processing Systems}} 17: {{Proceedings}} of the 2004 {{Conference}}},
  author = {Cuturi, Marco and Vert, Jean-Philippe},
  year = {2005},
  volume = {17},
  pages = {329},
  publisher = {{MIT Press}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Vert_2005_Semigroup_kernels_on_finite_sets.pdf}
}

@article{cuturi2005semigroupa,
  title = {Semigroup Kernels on Measures},
  author = {Cuturi, Marco and Fukumizu, Kenji and Vert, Jean-Philippe},
  year = {2005},
  volume = {6},
  pages = {1169--1198},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_et_al_2005_Semigroup_kernels_on_measures.pdf;/Users/ilyes/Zotero/storage/WKAN63C3/cuturi05a.html},
  journal = {Journal of Machine Learning Research},
  number = {Jul}
}

@inproceedings{cuturi2007kernel,
  title = {A Kernel for Time Series Based on Global Alignments},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}}, 2007. {{ICASSP}} 2007. {{IEEE International Conference}} On},
  author = {Cuturi, Marco and Vert, Jean-Philippe and Birkenes, Oystein and Matsui, Tomoko},
  year = {2007},
  volume = {2},
  pages = {II--413},
  publisher = {{IEEE}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_et_al_2007_A_kernel_for_time_series_based_on_global_alignments.pdf;/Users/ilyes/Zotero/storage/3599QFCW/4217433.html}
}

@inproceedings{cuturi2011fast,
  title = {Fast Global Alignment Kernels},
  booktitle = {International {{Conference}} in {{Machine Learning}} 2011},
  author = {Cuturi, Marco},
  year = {2011},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_2011_Fast_global_alignment_kernels.pdf}
}

@article{cuturi2011ground,
  title = {Ground {{Metric Learning}}},
  author = {Cuturi, Marco and Avis, David},
  year = {2011},
  month = oct,
  abstract = {Transportation distances have been used for more than a decade now in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, transportation distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of transportation distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two polyhedral convex functions over a convex set of distance matrices. We follow the presentation of our algorithms with promising experimental results on binary classification tasks using GIST descriptors of images taken in the Caltech-256 set.},
  archivePrefix = {arXiv},
  eprint = {1110.2306},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Avis_2011_Ground_Metric_Learning.pdf;/Users/ilyes/Zotero/storage/7PFMPSST/1110.html},
  journal = {arXiv:1110.2306 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{cuturi2013sinkhorn,
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transport}}},
  shorttitle = {Sinkhorn {{Distances}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Cuturi, Marco},
  year = {2013},
  pages = {2292--2300},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_2013_Sinkhorn_Distances_-_Lightspeed_Computation_of_Optimal_Transport.pdf;/Users/ilyes/Zotero/storage/GSDA4WN7/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.html},
  keywords = {_read}
}

@inproceedings{cuturi2014fast,
  title = {Fast Computation of Wasserstein Barycenters},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} 2014, {{JMLR W}}\&{{CP}}},
  author = {Cuturi, Marco and Doucet, Arnaud},
  year = {2014},
  volume = {32},
  pages = {685--693},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Doucet_2014_Fast_computation_of_wasserstein_barycenters.pdf}
}

@article{cuturi2016smoothed,
  title = {A Smoothed Dual Approach for Variational Wasserstein Problems},
  author = {Cuturi, Marco and Peyr{\'e}, Gabriel},
  year = {2016},
  volume = {9},
  pages = {320--343},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Peyré_2016_A_smoothed_dual_approach_for_variational_wasserstein_problems.pdf;/Users/ilyes/Zotero/storage/RNE5NT6R/15M1032600.html},
  journal = {SIAM J. Imaging Sciences},
  number = {1}
}

@article{cuturi2017softdtw,
  title = {Soft-{{DTW}}: A {{Differentiable Loss Function}} for {{Time}}-{{Series}}},
  shorttitle = {Soft-{{DTW}}},
  author = {Cuturi, Marco and Blondel, Mathieu},
  year = {2017},
  month = mar,
  abstract = {We propose in this paper a differentiable learning loss between time series. Our proposal builds upon the celebrated Dynamic Time Warping (DTW) discrepancy. Unlike the Euclidean distance, DTW is able to compare asynchronous time series of varying size and is robust to elastic transformations in time. To be robust to such invariances, DTW computes a minimal cost alignment between time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and its gradient can be computed with quadratic time/space complexity (DTW has quadratic time and linear space complexity). We show that our regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.},
  archivePrefix = {arXiv},
  eprint = {1703.01541},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Cuturi_Blondel_2017_Soft-DTW_-_a_Differentiable_Loss_Function_for_Time-Series.pdf},
  journal = {arXiv:1703.01541 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{dabrowska1989uniform,
  title = {Uniform {{Consistency}} of the {{Kernel Conditional Kaplan}}-{{Meier Estimate}}},
  author = {Dabrowska, Dorota M.},
  year = {1989},
  month = sep,
  volume = {17},
  pages = {1157--1167},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176347261},
  abstract = {We consider a class of nonparametric regression estimates introduced by Beran to estimate conditional survival functions in the presence of right censoring. An exponential probability bound for the tails of distributions of kernel estimates of conditional survival functions is derived. This inequality is next used to prove weak and strong uniform consistency results. The developments rest on sharp exponential bounds for the oscillation modulus of multivariate empirical processes obtained by Stute.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Dabrowska_1989_Uniform_Consistency_of_the_Kernel_Conditional_Kaplan-Meier_Estimate.pdf;/Users/ilyes/Zotero/storage/LGE9JIUC/1176347261.html},
  journal = {The Annals of Statistics},
  keywords = {Kernel regression,oscillation modulus,right censoring},
  language = {EN},
  mrnumber = {MR1015143},
  number = {3},
  zmnumber = {0687.62035}
}

@article{dai2019diagnosing,
  title = {Diagnosing and {{Enhancing VAE Models}}},
  author = {Dai, Bin and Wipf, David},
  year = {2019},
  month = oct,
  abstract = {Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/ TwoStageVAE.},
  archivePrefix = {arXiv},
  eprint = {1903.05789},
  eprinttype = {arxiv},
  file = {/nfs/ghome/live/ilyesk/google-drive/zotero/VI/Dai_Wipf_2019_Diagnosing_and_Enhancing_VAE_Models.pdf;/Users/ilyes/Zotero/storage/XC6LIMY2/1903.html},
  journal = {arXiv:1903.05789 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{devlin2018bert,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  month = oct,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Devlin_et_al_2018_BERT_-_Pre-training_of_Deep_Bidirectional_Transformers_for_Language_Understanding.pdf;/Users/ilyes/Zotero/storage/XF4KCSA7/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{dinh2014nice,
  title = {{{NICE}}: {{Non}}-Linear {{Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  year = {2014},
  month = oct,
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archivePrefix = {arXiv},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Dinh_et_al_2014_NICE_-_Non-linear_Independent_Components_Estimation.pdf;/Users/ilyes/Zotero/storage/K5EVWRZ8/1410.html},
  journal = {arXiv:1410.8516 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{dinh2016density,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2016},
  month = may,
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archivePrefix = {arXiv},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Dinh_et_al_2016_Density_estimation_using_Real_NVP.pdf;/Users/ilyes/Zotero/storage/6PRSXGL5/1605.html},
  journal = {arXiv:1605.08803 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{docarmo1992riemannian,
  title = {Riemannian {{Geometry}}},
  author = {{do Carmo}, Manfredo},
  year = {1992},
  publisher = {{Birkh{\"a}user Basel}},
  abstract = {Riemannian Geometry is an expanded edition of a highly acclaimed and successful textbook (originally published in Portuguese) for first-year graduate students in mathematics and physics. The author's treatment goes very directly to the basic language of Riemannian geometry and immediately presents some of its most fundamental theorems. It is elementary, assuming only a modest background from readers, making it suitable for a wide variety of students and course structures. Its selection of topics has been deemed "superb" by teachers who have used the text. A significant feature of the book is its powerful and revealing structure, beginning simply with the definition of a differentiable manifold and ending with one of the most important results in Riemannian geometry, a proof of the Sphere Theorem. The text abounds with basic definitions and theorems, examples, applications, and numerous exercises to test the student's understanding and extend knowledge and insight into the subject. Instructors and students alike will find the work to be a significant contribution to this highly applicable and stimulating subject.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/do_Carmo_1992_Riemannian_Geometry.pdf;/Users/ilyes/Zotero/storage/2NMSFJWI/9780817634902.html},
  isbn = {978-0-8176-3490-2},
  language = {en},
  series = {Mathematics: {{Theory}} \& {{Applications}}}
}

@article{doersch2016tutorial,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = jun,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archivePrefix = {arXiv},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Doersch_2016_Tutorial_on_Variational_Autoencoders.pdf;/Users/ilyes/Zotero/storage/XRVC9BW2/1606.html},
  journal = {arXiv:1606.05908 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,VAE,Variational autoencoders},
  primaryClass = {cs, stat}
}

@article{donahue2016adversarial,
  title = {Adversarial {{Feature Learning}}},
  author = {Donahue, Jeff and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
  year = {2016},
  month = may,
  abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
  archivePrefix = {arXiv},
  eprint = {1605.09782},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Donahue_et_al_2016_Adversarial_Feature_Learning.pdf;/Users/ilyes/Zotero/storage/8LFPDHD7/1605.html},
  journal = {arXiv:1605.09782 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{dudley2002real,
  title = {Real {{Analysis}} and {{Probability}}},
  author = {Dudley, R. M.},
  year = {2002},
  month = oct,
  doi = {10.1017/CBO9780511755347},
  abstract = {Cambridge Core - Real and Complex Analysis - Real Analysis and Probability -  by R. M. Dudley},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Dudley_2002_Real_Analysis_and_Probability.pdf;/Users/ilyes/Zotero/storage/3APF9BIK/26DDF2D09E526185F2347AA5658B96F6.html},
  language = {en}
}

@article{dumoulin2016adversarially,
  title = {Adversarially {{Learned Inference}}},
  author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  year = {2016},
  month = jun,
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
  archivePrefix = {arXiv},
  eprint = {1606.00704},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Dumoulin_et_al_2016_Adversarially_Learned_Inference.pdf;/Users/ilyes/Zotero/storage/JSWJMCRM/1606.html},
  journal = {arXiv:1606.00704 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{eagleton2011literary,
  title = {Literary Theory: {{An}} Introduction},
  shorttitle = {Literary Theory},
  author = {Eagleton, Terry},
  year = {2011},
  publisher = {{John Wiley \& Sons}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Random/Eagleton_2011_Literary_theory_-_An_introduction.pdf;/Users/ilyes/Zotero/storage/7VWY5VZE/books.html}
}

@article{ecker2011effect,
  title = {The Effect of Noise Correlations in Populations of Diversely Tuned Neurons},
  author = {Ecker, Alexander S. and Berens, Philipp and Tolias, Andreas S. and Bethge, Matthias},
  year = {2011},
  month = oct,
  volume = {31},
  pages = {14272--14283},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.2539-11.2011},
  abstract = {The amount of information encoded by networks of neurons critically depends on the correlation structure of their activity. Neurons with similar stimulus preferences tend to have higher noise correlations than others. In homogeneous populations of neurons, this limited range correlation structure is highly detrimental to the accuracy of a population code. Therefore, reduced spike count correlations under attention, after adaptation, or after learning have been interpreted as evidence for a more efficient population code. Here, we analyze the role of limited range correlations in more realistic, heterogeneous population models. We use Fisher information and maximum-likelihood decoding to show that reduced correlations do not necessarily improve encoding accuracy. In fact, in populations with more than a few hundred neurons, increasing the level of limited range correlations can substantially improve encoding accuracy. We found that this improvement results from a decrease in noise entropy that is associated with increasing correlations if the marginal distributions are unchanged. Surprisingly, for constant noise entropy and in the limit of large populations, the encoding accuracy is independent of both structure and magnitude of noise correlations.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Ecker_et_al_2011_The_effect_of_noise_correlations_in_populations_of_diversely_tuned_neurons.pdf},
  journal = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  keywords = {Action Potentials,Animals,Electricity,Models; Neurological,Neurons,Random Allocation},
  language = {eng},
  number = {40},
  pmcid = {PMC3221941},
  pmid = {21976512}
}

@article{emde1999active,
  title = {Active Electrolocation of Objects in Weakly Electric Fish},
  author = {von der Emde, G.},
  year = {1999},
  month = may,
  volume = {202},
  pages = {1205--1215},
  issn = {0022-0949, 1477-9145},
  abstract = {Skip to Next Section
Weakly electric fish produce electric signals (electric organ discharges, EODs) with a specialised electric organ creating an electric field around their body. Objects within this field alter the EOD-induced current at epidermal electroreceptor organs, which are distributed over almost the entire body surface. The detection, localisation and analysis of objects performed by monitoring self-produced electric signals is called active electrolocation. Electric fish employ active electrolocation to detect objects that are less than 12 cm away and have electric properties that are different from those of the surrounding water. Within this range, the mormyrid Gnathonemus petersii can also perceive the distance of objects. Depth perception is independent of object parameters such as size, shape and material. The mechanism for distance determination through electrolocation involves calculating the ratio between two parameters of the electric image that the object projects onto the fish's skin. Electric fish can not only locate objects but can also analyse their electrical properties. Fish are informed about object impedance by measuring local amplitude changes at their receptor organs evoked by an object. In addition, all electric fish studied so far can independently determine the capacitative and resistive components of objects that possess complex impedances. This ability allows the fish to discriminate between living and non-living matter, because capacitance is a property of living organisms. African mormyrids and South American gymnotiforms use different mechanisms for capacitance detection. Mormyrids detect capacitance-evoked EOD waveform distortions, whereas gymnotiforms perform time measurements. Gymnotiforms measure the temporal phase shift of their EODs induced at body parts close to the object relative to unaffected body parts further away.},
  copyright = {\textcopyright{} 1999 by Company of Biologists},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Emde_1999_Active_electrolocation_of_objects_in_weakly_electric_fish.pdf;/Users/ilyes/Zotero/storage/DIPFEDCE/1205.html},
  journal = {Journal of Experimental Biology},
  language = {en},
  number = {10},
  pmid = {10210662}
}

@article{esmaeili2018structured,
  title = {Structured {{Disentangled Representations}}},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and {van de Meent}, Jan-Willem},
  year = {2018},
  month = apr,
  abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
  archivePrefix = {arXiv},
  eprint = {1804.02086},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Esmaeili_et_al_2018_Structured_Disentangled_Representations.pdf;/Users/ilyes/Zotero/storage/QIEMDHYJ/1804.html},
  journal = {arXiv:1804.02086 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,VAE},
  primaryClass = {cs, stat}
}

@article{figalli2010optimala,
  title = {The {{Optimal Partial Transport Problem}}},
  author = {Figalli, Alessio},
  year = {2010},
  month = feb,
  volume = {195},
  pages = {533--560},
  issn = {0003-9527, 1432-0673},
  doi = {10.1007/s00205-008-0212-7},
  abstract = {Given two densities f and g, we consider the problem of transporting a fraction \{m \textbackslash{}in [0,\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\}m{$\in$}[0,min\{{$\Vert$}f{$\Vert$}L1,{$\Vert$}g{$\Vert$}L1\}]\{m \textbackslash{}in [0,\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\} of the mass of f onto g minimizing a transportation cost. If the cost per unit of mass is given by |x - y|2, we will see that uniqueness of solutions holds for \{m \textbackslash{}in [\textbackslash{}|f\textbackslash{}wedge g\textbackslash{}|\_\{L\^1\},\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\}\{m \textbackslash{}in [\textbackslash{}|f\textbackslash{}wedge g\textbackslash{}|\_\{L\^1\},\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\} . This extends the result of Caffarelli and McCann in Ann Math (in print), where the authors consider two densities with disjoint supports. The free boundaries of the active regions are shown to be (n - 1)-rectifiable (provided the supports of f and g have Lipschitz boundaries), and under some weak regularity assumptions on the geometry of the supports they are also locally semiconvex. Moreover, assuming f and g supported on two bounded strictly convex sets \{\{\textbackslash{}Omega,\textbackslash{}Lambda \textbackslash{}subset \textbackslash{}mathbb \{R\}\^n\}\}\{\{\textbackslash{}Omega,\textbackslash{}Lambda \textbackslash{}subset \textbackslash{}mathbb \{R\}\^n\}\} , and bounded away from zero and infinity on their respective supports, \{C\^\{0,\textbackslash{}alpha\}\_\{\textbackslash{}rm loc\}\}\{C\^\{0,\textbackslash{}alpha\}\_\{\textbackslash{}rm loc\}\} regularity of the optimal transport map and local C1 regularity of the free boundaries away from \{\{\textbackslash{}Omega\textbackslash{}cap \textbackslash{}Lambda\}\}\{\{\textbackslash{}Omega\textbackslash{}cap \textbackslash{}Lambda\}\} are shown. Finally, the optimal transport map extends to a global homeomorphism between the active regions.},
  file = {/Users/ilyes/Dropbox/Zotero/OT/Figalli_2010_The_Optimal_Partial_Transport_Problem.pdf;/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Figalli_2010_The_Optimal_Partial_Transport_Problem.pdf;/Users/ilyes/Zotero/storage/DFI5M3DH/10.html;/Users/ilyes/Zotero/storage/TH2AQRES/s00205-008-0212-7.html},
  journal = {Archive for Rational Mechanics and Analysis},
  language = {en},
  number = {2}
}

@techreport{florens2005engogeneity,
  title = {Engogeneity in Nonseparable Models. {{Application}} to Treatment Models Where the Outcomes Are Durations},
  author = {Florens, J. P.},
  year = {2005},
  institution = {{mimeo, University of Toulouse}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Florens_2005_Engogeneity_in_nonseparable_models._Application_to_treatment_models_where_the.pdf}
}

@incollection{florens2011nonparametric,
  title = {Non-Parametric Models with Instrumental Variables},
  booktitle = {Inverse {{Problems}} and {{High}}-{{Dimensional Estimation}}},
  author = {Florens, Jean-Pierre},
  year = {2011},
  pages = {99--117},
  publisher = {{Springer}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Florens_2011_Non-parametric_models_with_instrumental_variables.pdf;/Users/ilyes/Zotero/storage/VNZB7DNX/978-3-642-19989-9_2.html}
}

@book{friedman2001elements,
  title = {The Elements of Statistical Learning},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year = {2001},
  volume = {1},
  publisher = {{Springer series in statistics New York}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Friedman_et_al_2001_The_elements_of_statistical_learning.pdf;/Users/ilyes/Zotero/storage/8NSQJPWT/Friedman et al. - 2001 - The elements of statistical learning.ps;/Users/ilyes/Zotero/storage/FND7MQNT/Friedman et al. - 2001 - The elements of statistical learning.ps}
}

@article{gal2014variational,
  title = {Variational {{Inference}} in {{Sparse Gaussian Process Regression}} and {{Latent Variable Models}} - a {{Gentle Tutorial}}},
  author = {Gal, Yarin and {van der Wilk}, Mark},
  year = {2014},
  month = feb,
  abstract = {In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias \& Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.},
  archivePrefix = {arXiv},
  eprint = {1402.1412},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gal_van_der_Wilk_2014_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable.pdf;/Users/ilyes/Zotero/storage/NT9IKWIN/1402.html},
  journal = {arXiv:1402.1412 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{gao2019deep,
  title = {Deep {{Generative Learning}} via {{Variational Gradient Flow}}},
  author = {Gao, Yuan and Jiao, Yuling and Wang, Yang and Wang, Yao and Yang, Can and Zhang, Shunkang},
  year = {2019},
  month = jan,
  abstract = {We propose a general framework to learn deep generative models via \textbackslash{}textbf\{V\}ariational \textbackslash{}textbf\{Gr\}adient Fl\textbackslash{}textbf\{ow\} (VGrow) on probability spaces. The evolving distribution that asymptotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the \$f\$-divergence between them. We prove that the evolving distribution coincides with the pushforward distribution through the infinitesimal time composition of residual maps that are perturbations of the identity map along the vector field. The vector field depends on the density ratio of the pushforward distribution and the target distribution, which can be consistently learned from a binary classification problem. Connections of our proposed VGrow method with other popular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of deep generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffrey divergences as well as our newly discovered `logD' divergence which serves as the objective function of the logD-trick GAN. Experimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving competitive performance with state-of-the-art GANs.},
  archivePrefix = {arXiv},
  eprint = {1901.08469},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Gao_et_al_2019_Deep_Generative_Learning_via_Variational_Gradient_Flow.pdf;/Users/ilyes/Zotero/storage/W2HMEJFZ/1901.html},
  journal = {arXiv:1901.08469 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{gao2019flow,
  title = {Flow {{Contrastive Estimation}} of {{Energy}}-{{Based Models}}},
  author = {Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P. and Xu, Zhen and Dai, Andrew M. and Wu, Ying Nian},
  year = {2019},
  month = dec,
  abstract = {This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.},
  archivePrefix = {arXiv},
  eprint = {1912.00589},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Zotero/storage/78AISQFZ/Gao et al. - 2019 - Flow Contrastive Estimation of Energy-Based Models.pdf;/Users/ilyes/Zotero/storage/C4QUTMP8/1912.html},
  journal = {arXiv:1912.00589 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{garnelo2018neural,
  title = {Neural {{Processes}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  year = {2018},
  month = jul,
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archivePrefix = {arXiv},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Garnelo_et_al_2018_Neural_Processes.pdf;/Users/ilyes/Zotero/storage/GLEX482H/1807.html},
  journal = {arXiv:1807.01622 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{genevay2017gan,
  title = {{{GAN}} and {{VAE}} from an {{Optimal Transport Point}} of {{View}}},
  author = {Genevay, Aude and Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2017},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Genevay_et_al_2017_GAN_and_VAE_from_an_Optimal_Transport_Point_of_View.pdf;/Users/ilyes/Zotero/storage/W6F7WQBK/1706.html},
  journal = {arXiv preprint arXiv:1706.01807}
}

@article{genevay2017sinkhornautodiff,
  title = {Sinkhorn-{{AutoDiff}}: {{Tractable Wasserstein Learning}} of {{Generative Models}}},
  shorttitle = {Sinkhorn-{{AutoDiff}}},
  author = {Genevay, Aude and Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2017},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Genevay_et_al_2017_Sinkhorn-AutoDiff_-_Tractable_Wasserstein_Learning_of_Generative_Models.pdf;/Users/ilyes/Zotero/storage/3F6EHSJ4/1706.html},
  journal = {arXiv preprint arXiv:1706.00292}
}

@article{germain2015made,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  shorttitle = {{{MADE}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2015},
  month = feb,
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  archivePrefix = {arXiv},
  eprint = {1502.03509},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Germain_et_al_2015_MADE_-_Masked_Autoencoder_for_Distribution_Estimation.pdf;/Users/ilyes/Zotero/storage/E4TM4JK4/1502.html},
  journal = {arXiv:1502.03509 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{gilbert1993automatic,
  title = {Automatic Differentiation and the Step Computation in the Limited Memory {{BFGS}} Method},
  author = {Gilbert, Jean Charles and Nocedal, Jorge},
  year = {1993},
  month = may,
  volume = {6},
  pages = {47--50},
  issn = {0893-9659},
  doi = {10.1016/0893-9659(93)90032-I},
  abstract = {It is shown that the two-loop recursion for computing the search direction of a limited memory method for optimization can be derived by means of the reverse mode of automatic differentiation applied to an auxilliary function.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gilbert_Nocedal_1993_Automatic_differentiation_and_the_step_computation_in_the_limited_memory_BFGS.pdf;/Users/ilyes/Zotero/storage/DFGUTPCC/089396599390032I.html},
  journal = {Applied Mathematics Letters},
  number = {3}
}

@book{giraud2014introduction,
  title = {Introduction to {{High}}-{{Dimensional Statistics}}},
  author = {Giraud, Christophe},
  year = {2014},
  month = dec,
  publisher = {{Taylor \& Francis}},
  abstract = {Ever-greater computing technologies have given rise to an exponentially growing volume of data. Today massive data sets (with potentially thousands of variables) play an important role in almost every branch of modern human activity, including networks, finance, and genetics. However, analyzing such data has presented a challenge for statisticians and data analysts and has required the development of new statistical methods capable of separating the signal from the noise. Introduction to High-Dimensional Statistics is a concise guide to state-of-the-art models, techniques, and approaches for handling high-dimensional data. The book is intended to expose the reader to the key concepts and ideas in the most simple settings possible while avoiding unnecessary technicalities.  Offering a succinct presentation of the mathematical foundations of high-dimensional statistics, this highly accessible text:  Describes the challenges related to the analysis of high-dimensional data Covers cutting-edge statistical methods including model selection, sparsity and the lasso, aggregation, and learning theory Provides detailed exercises at the end of every chapter with collaborative solutions on a wikisite Illustrates concepts with simple but clear practical examples Introduction to High-Dimensional Statistics is suitable for graduate students and researchers interested in discovering modern statistics for massive data. It can be used as a graduate text or for self-study.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Giraud_2014_Introduction_to_High-Dimensional_Statistics.pdf},
  googlebooks = {qRuVoAEACAAJ},
  isbn = {978-1-4822-3794-8},
  keywords = {Business \& Economics / Statistics,Computers / Machine Theory,Mathematics / Probability \& Statistics / General,Technology \& Engineering / Automation},
  language = {en}
}

@article{goodfellow2014generative,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archivePrefix = {arXiv},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Goodfellow_et_al_2014_Generative_Adversarial_Networks.pdf;/Users/ilyes/Zotero/storage/B5YRXQ5T/1406.html},
  journal = {arXiv:1406.2661 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{gowers2002mathematics,
  title = {Mathematics: {{A Very Short Introduction}}},
  shorttitle = {Mathematics},
  author = {Gowers, Timothy},
  year = {2002},
  month = aug,
  publisher = {{OUP Oxford}},
  address = {{Oxford ; New York}},
  abstract = {The aim of this book is to explain, carefully but not technically, the differences between advanced, research-level mathematics, and the sort of mathematics we learn at school. The most fundamental differences are philosophical, and readers of this book will emerge with a clearer understanding of paradoxical-sounding concepts such as infinity, curved space, and imaginary numbers. The first few chapters are about general aspects of mathematical thought. These are followed by discussions of more specific topics, and the book closes with a chapter answering common sociological questions about the mathematical community (such as "Is it true that mathematicians burn out at the age of 25?")  ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Gowers_2002_Mathematics_-_A_Very_Short_Introduction.pdf},
  isbn = {978-0-19-285361-5},
  language = {Anglais}
}

@article{graupner2012calciumbased,
  title = {Calcium-Based Plasticity Model Explains Sensitivity of Synaptic Changes to Spike Pattern, Rate, and Dendritic Location},
  author = {Graupner, Michael and Brunel, Nicolas},
  year = {2012},
  month = mar,
  volume = {109},
  pages = {3991--3996},
  issn = {1091-6490},
  doi = {10.1073/pnas.1109359109},
  abstract = {Multiple stimulation protocols have been found to be effective in changing synaptic efficacy by inducing long-term potentiation or depression. In many of those protocols, increases in postsynaptic calcium concentration have been shown to play a crucial role. However, it is still unclear whether and how the dynamics of the postsynaptic calcium alone determine the outcome of synaptic plasticity. Here, we propose a calcium-based model of a synapse in which potentiation and depression are activated above calcium thresholds. We show that this model gives rise to a large diversity of spike timing-dependent plasticity curves, most of which have been observed experimentally in different systems. It accounts quantitatively for plasticity outcomes evoked by protocols involving patterns with variable spike timing and firing rate in hippocampus and neocortex. Furthermore, it allows us to predict that differences in plasticity outcomes in different studies are due to differences in parameters defining the calcium dynamics. The model provides a mechanistic understanding of how various stimulation protocols provoke specific synaptic changes through the dynamics of calcium concentration and thresholds implementing in simplified fashion protein signaling cascades, leading to long-term potentiation and long-term depression. The combination of biophysical realism and analytical tractability makes it the ideal candidate to study plasticity at the synapse, neuron, and network levels.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Graupner_Brunel_2012_Calcium-based_plasticity_model_explains_sensitivity_of_synaptic_changes_to.pdf},
  journal = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords = {Calcium,Calcium Signaling,Computer Simulation,Dendrites,Hippocampus,Humans,Long-Term Potentiation,Long-Term Synaptic Depression,Models; Biological,Models; Statistical,Neocortex,Neuronal Plasticity,Synapses,Synaptic Transmission},
  language = {eng},
  number = {10},
  pmcid = {PMC3309784},
  pmid = {22357758}
}

@article{gremse2016gpuaccelerated,
  title = {{{GPU}}-Accelerated Adjoint Algorithmic Differentiation},
  author = {Gremse, Felix and H{\"o}fter, Andreas and Razik, Lukas and Kiessling, Fabian and Naumann, Uwe},
  year = {2016},
  month = mar,
  volume = {200},
  pages = {300--311},
  issn = {0010-4655},
  doi = {10.1016/j.cpc.2015.10.027},
  abstract = {Many scientific problems such as classifier training or medical image reconstruction can be expressed as minimization of differentiable real-valued cost functions and solved with iterative gradient-based methods. Adjoint algorithmic differentiation (AAD) enables automated computation of gradients of such cost functions implemented as computer programs. To backpropagate adjoint derivatives, excessive memory is potentially required to store the intermediate partial derivatives on a dedicated data structure, referred to as the ``tape''. Parallelization is difficult because threads need to synchronize their accesses during taping and backpropagation. This situation is aggravated for many-core architectures, such as Graphics Processing Units (GPUs), because of the large number of light-weight threads and the limited memory size in general as well as per thread. We show how these limitations can be mediated if the cost function is expressed using GPU-accelerated vector and matrix operations which are recognized as intrinsic functions by our AAD software. We compare this approach with naive and vectorized implementations for CPUs. We use four increasingly complex cost functions to evaluate the performance with respect to memory consumption and gradient computation times. Using vectorization, CPU and GPU memory consumption could be substantially reduced compared to the naive reference implementation, in some cases even by an order of complexity. The vectorization allowed usage of optimized parallel libraries during forward and reverse passes which resulted in high speedups for the vectorized CPU version compared to the naive reference implementation. The GPU version achieved an additional speedup of 7.5{$\pm$}4.4, showing that the processing power of GPUs can be utilized for AAD using this concept. Furthermore, we show how this software can be systematically extended for more complex problems such as nonlinear absorption reconstruction for fluorescence-mediated tomography. Program title: AD-GPU Catalogue identifier: AEYX\_v1\_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYX\_v1\_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 16715 No. of bytes in distributed program, including test data, etc.: 143683 Distribution format: tar.gz Programming language: C++ and CUDA. Computer: Any computer with a compatible C++ compiler and a GPU with CUDA capability 3.0 or higher. Operating system: Windows 7 or Linux. RAM: 16 Gbyte Classification: 4.9, 4.12, 6.1, 6.5. External routines: CUDA 6.5, Intel MKL (optional) and routines from BLAS, LAPACK and CUBLAS Nature of problem: Gradients are required for many optimization problems, e.g.~classifier training or nonlinear image reconstruction. Often, the function, of which the gradient is required, can be implemented as a computer program. Then, algorithmic differentiation methods can be used to compute the gradient. Depending on the approach this may result in excessive requirements of computational resources, i.e.~memory and arithmetic computations. GPUs provide massive computational resources but require special considerations to distribute the workload onto many light-weight threads. Solution method: Adjoint algorithmic differentiation allows efficient computation of gradients of cost functions given as computer programs. The gradient can be theoretically computed using a similar amount of arithmetic operations as one function evaluation. Optimal usage of parallel processors and limited memory is a major challenge which can be mediated by the use of vectorization. Restrictions: To use the GPU-accelerated adjoint algorithmic differentiation method, the cost function must be implemented using the provided AD-GPU intrinsics for matrix and vector operations. Unusual features: GPU-acceleration. Additional comments: The code uses some features of C++11, e.g.~std::shared ptr. Alternatively, the boost library can be used. Running time: The time to run the example program is a few minutes or up to a few hours to reproduce the performance measurements.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gremse_et_al_2016_GPU-accelerated_adjoint_algorithmic_differentiation.pdf;/Users/ilyes/Zotero/storage/TP5F5RCE/S0010465515004099.html},
  journal = {Computer Physics Communications},
  keywords = {Adjoint algorithmic differentiation,GPU programming}
}

@article{gresele2019incomplete,
  title = {The {{Incomplete Rosetta Stone Problem}}: {{Identifiability Results}} for {{Multi}}-{{View Nonlinear ICA}}},
  shorttitle = {The {{Incomplete Rosetta Stone Problem}}},
  author = {Gresele, Luigi and Rubenstein, Paul K. and Mehrjou, Arash and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = may,
  abstract = {We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.},
  archivePrefix = {arXiv},
  eprint = {1905.06642},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Gresele_et_al_2019_The_Incomplete_Rosetta_Stone_Problem_-_Identifiability_Results_for_Multi-View.pdf;/Users/ilyes/Zotero/storage/EULSJDT2/1905.html},
  journal = {arXiv:1905.06642 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{gretton2005measuring,
  title = {Measuring Statistical Dependence with {{Hilbert}}-{{Schmidt}} Norms},
  booktitle = {International Conference on Algorithmic Learning Theory},
  author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  year = {2005},
  pages = {63--77},
  publisher = {{Springer}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Gretton_et_al_2005_Measuring_statistical_dependence_with_Hilbert-Schmidt_norms.pdf;/Users/ilyes/Zotero/storage/TMH85JRL/11564089_7.html}
}

@inproceedings{gretton2005measuringa,
  title = {Measuring Statistical Dependence with {{Hilbert}}-{{Schmidt}} Norms},
  booktitle = {International Conference on Algorithmic Learning Theory},
  author = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  year = {2005},
  pages = {63--77},
  publisher = {{Springer}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gretton_et_al_2005_Measuring_statistical_dependence_with_Hilbert-Schmidt_norms2.pdf;/Users/ilyes/Zotero/storage/YLENNBK3/11564089_7.html}
}

@article{gretton2012kernel,
  title = {A {{Kernel Two}}-{{Sample Test}}},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  year = {2012},
  volume = {13},
  pages = {723--773},
  issn = {ISSN 1533-7928},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gretton_et_al_2012_A_Kernel_Two-Sample_Test.pdf;/Users/ilyes/Zotero/storage/2HFEFKTC/gretton12a.html},
  journal = {Journal of Machine Learning Research},
  number = {Mar}
}

@inproceedings{gutmann2010noisecontrastive,
  title = {Noise-Contrastive Estimation: {{A}} New Estimation Principle for Unnormalized Statistical Models},
  shorttitle = {Noise-Contrastive Estimation},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gutmann, Michael and Hyv{\"a}rinen, Aapo},
  year = {2010},
  pages = {297--304},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Gutmann_Hyvärinen_2010_Noise-contrastive_estimation_-_A_new_estimation_principle_for_unnormalized.pdf},
  keywords = {logistic regression,noise,non linear logistic regression,read}
}

@article{gutmann2012noisecontrastive,
  title = {Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics},
  author = {Gutmann, Michael U. and Hyv{\"a}rinen, Aapo},
  year = {2012},
  volume = {13},
  pages = {307--361},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Gutmann_Hyvärinen_2012_Noise-contrastive_estimation_of_unnormalized_statistical_models,_with.pdf;/Users/ilyes/Zotero/storage/AIQLFAN2/gutmann12a.html},
  journal = {Journal of Machine Learning Research},
  number = {Feb}
}

@article{hanin1992kantorovichrubinstein,
  title = {Kantorovich-{{Rubinstein Norm}} and {{Its Application}} in the {{Theory}} of {{Lipschitz Spaces}}},
  author = {Hanin, Leonid G.},
  year = {1992},
  volume = {115},
  pages = {345--352},
  issn = {0002-9939},
  doi = {10.2307/2159251},
  abstract = {We obtain necessary and sufficient conditions on a compact metric space (K, {$\rho$}) that provide a natural isometric isomorphism between completion of the space of Borel measures on K with the Kantorovich-Rubinstein norm and the space \$(\textbackslash{}operatorname\{lip\}(K, \textbackslash{}rho))\^\textbackslash{}ast\$ or equivalently between the spaces \$\textbackslash{}operatorname\{Lip\}(K, \textbackslash{}rho)\$ and \$(\textbackslash{}operatorname\{lip\}(K, \textbackslash{}rho))\^\{\textbackslash{}ast\textbackslash{}ast\}\$. Such metric spaces are studied and related properties of Lipschitz spaces are established.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Hanin_1992_Kantorovich-Rubinstein_Norm_and_Its_Application_in_the_Theory_of_Lipschitz.pdf},
  journal = {Proceedings of the American Mathematical Society},
  number = {2}
}

@article{hare2016struck,
  title = {Struck: {{Structured Output Tracking}} with {{Kernels}}},
  shorttitle = {Struck},
  author = {Hare, S. and Golodetz, S. and Saffari, A. and Vineet, V. and Cheng, M. M. and Hicks, S. L. and Torr, P. H. S.},
  year = {2016},
  month = oct,
  volume = {38},
  pages = {2096--2109},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2015.2509974},
  abstract = {Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we avoid the need for an intermediate classification step. Our method uses a kernelised structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow our tracker to run at high frame rates, we (a) introduce a budgeting mechanism that prevents the unbounded growth in the number of support vectors that would otherwise occur during tracking, and (b) show how to implement tracking on the GPU. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased tracking performance.},
  file = {/Users/ilyes/Zotero/storage/EXD2U7JC/7360205.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Adaptation models,adaptive tracking-by-detection methods,adaptive visual object tracking,budget maintenance,computer vision,GPU-based tracking,Graphics processing units,intermediate classification step,Kernel,kernelised structured output support vector machine,object detection,object tracking,online learning techniques,structured output prediction,structured output SVMs,structured output tracking with kernels,support vector machines,SVM,Target tracking,Tracking-by-detection,Training},
  number = {10}
}

@article{henaff2019dataefficient,
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  author = {H{\'e}naff, Olivier J. and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and van den Oord, Aaron},
  year = {2019},
  month = may,
  abstract = {Large scale deep learning excels when labeled images are abundant, yet data-efficient learning remains a longstanding challenge. While biological vision is thought to leverage vast amounts of unlabeled data to solve classification problems with limited supervision, computer vision has so far not succeeded in this `semi-supervised' regime. Our work tackles this challenge with Contrastive Predictive Coding, an unsupervised objective which extracts stable structure from still images. The result is a representation which, equipped with a simple linear classifier, separates ImageNet categories better than all competing methods, and surpasses the performance of a fully-supervised AlexNet model. When given a small number of labeled images (as few as 13 per class), this representation retains a strong classification performance, outperforming state-of-the-art semi-supervised methods by 10\% Top-5 accuracy and supervised methods by 20\%. Finally, we find our unsupervised representation to serve as a useful substrate for image detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset. We expect these results to open the door to pipelines that use scalable unsupervised representations as a drop-in replacement for supervised ones for real-world vision tasks where labels are scarce.},
  archivePrefix = {arXiv},
  eprint = {1905.09272},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Hénaff_et_al_2019_Data-Efficient_Image_Recognition_with_Contrastive_Predictive_Coding.pdf;/Users/ilyes/Zotero/storage/C5MXJ8VL/1905.html},
  journal = {arXiv:1905.09272 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@inproceedings{higgins2017betavae,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  shorttitle = {Beta-{{VAE}}},
  booktitle = {{{ICLR}}},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year = {2017},
  volume = {2},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial...},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Higgins_et_al_2016_Beta-VAE_-_Learning_Basic_Visual_Concepts_with_a_Constrained_Variational.pdf;/Users/ilyes/Zotero/storage/DGWTCAL6/forum.html},
  keywords = {disentanglement,VAE}
}

@article{higgins2018definition,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archivePrefix = {arXiv},
  eprint = {1812.02230},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Higgins_et_al_2018_Towards_a_Definition_of_Disentangled_Representations.pdf;/Users/ilyes/Zotero/storage/4759NAVM/1812.html},
  journal = {arXiv:1812.02230 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{hinton1994autoencoders,
  title = {Autoencoders, Minimum Description Length and {{Helmholtz}} Free Energy},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hinton, Geoffrey E. and Zemel, Richard S.},
  year = {1994},
  pages = {3--10},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Hinton_Zemel_1994_Autoencoders,_minimum_description_length_and_Helmholtz_free_energy.pdf}
}

@article{hinton2006reducing,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  year = {2006},
  month = jul,
  volume = {313},
  pages = {504--507},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ``autoencoder'' networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
  copyright = {American Association for the Advancement of Science},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hinton_Salakhutdinov_2006_Reducing_the_Dimensionality_of_Data_with_Neural_Networks.pdf;/Users/ilyes/Zotero/storage/ER4RMP2M/504.html},
  journal = {Science},
  language = {en},
  number = {5786},
  pmid = {16873662}
}

@article{hinton2007learning,
  title = {Learning Multiple Layers of Representation},
  author = {Hinton, Geoffrey E.},
  year = {2007},
  month = oct,
  volume = {11},
  pages = {428--434},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2007.09.004},
  abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hinton_2007_Learning_multiple_layers_of_representation.pdf;/Users/ilyes/Zotero/storage/WM8HE9LD/S1364661307002173.html},
  journal = {Trends in Cognitive Sciences},
  number = {10}
}

@article{hinton2014where,
  title = {Where {{Do Features Come From}}?},
  author = {Hinton, Geoffrey},
  year = {2014},
  month = aug,
  volume = {38},
  pages = {1078--1101},
  issn = {1551-6709},
  doi = {10.1111/cogs.12049},
  abstract = {It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hinton_2014_Where_Do_Features_Come_From.pdf;/Users/ilyes/Zotero/storage/NHKDT3U8/abstract.html},
  journal = {Cognitive Science},
  keywords = {Backpropagation,Boltzmann machines,Contrastive divergence,Deep learning,Distributed representations,Learning features,Learning graphical models,Variational learning},
  language = {en},
  number = {6}
}

@inproceedings{hirayama2017splice,
  title = {{{SPLICE}}: {{Fully}} Tractable Hierarchical Extension of {{ICA}} with Pooling},
  shorttitle = {{{SPLICE}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  author = {Hirayama, Jun-ichiro and Hyvarinen, A. J. and Kawanabe, Motoaki},
  year = {2017},
  volume = {70},
  pages = {1491--1500},
  publisher = {{Machine Learning Research}}
}

@article{hjelm2018learning,
  title = {Learning Deep Representations by Mutual Information Estimation and Maximization},
  author = {Hjelm, R. Devon and Fedorov, Alex and {Lavoie-Marchildon}, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  year = {2018},
  month = aug,
  abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
  archivePrefix = {arXiv},
  eprint = {1808.06670},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hjelm_et_al_2018_Learning_deep_representations_by_mutual_information_estimation_and_maximization.pdf;/Users/ilyes/Zotero/storage/GSLCIGMP/1808.html},
  journal = {arXiv:1808.06670 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{hochreiter2001maximum,
  title = {Beyond Maximum Likelihood and Density Estimation: {{A}} Sample-Based Criterion for Unsupervised Learning of Complex Models},
  shorttitle = {Beyond Maximum Likelihood and Density Estimation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hochreiter, Sepp and Mozer, Michael C.},
  year = {2001},
  pages = {535--541},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hochreiter_Mozer_2001_Beyond_maximum_likelihood_and_density_estimation_-_A_sample-based_criterion_for.pdf},
  keywords = {ideas}
}

@inproceedings{hochreiter2005optimal,
  title = {Optimal Kernels for Unsupervised Learning},
  booktitle = {Neural {{Networks}}, 2005. {{IJCNN}}'05. {{Proceedings}}. 2005 {{IEEE International Joint Conference}} On},
  author = {Hochreiter, Sepp and Obermayer, Klaus},
  year = {2005},
  volume = {3},
  pages = {1895--1899},
  publisher = {{IEEE}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hochreiter_Obermayer_2005_Optimal_kernels_for_unsupervised_learning.pdf;/Users/ilyes/Zotero/storage/WN7V6XVS/1556169.html},
  keywords = {read}
}

@article{holmes2012fast,
  title = {Fast {{Nonparametric Conditional Density Estimation}}},
  author = {Holmes, Michael P. and Gray, Alexander G. and Isbell, Charles Lee},
  year = {2012},
  month = jun,
  abstract = {Conditional density estimation generalizes regression by modeling a full density f(yjx) rather than only the expected value E(yjx). This is important for many tasks, including handling multi-modality and generating prediction intervals. Though fundamental and widely applicable, nonparametric conditional density estimators have received relatively little attention from statisticians and little or none from the machine learning community. None of that work has been applied to greater than bivariate data, presumably due to the computational difficulty of data-driven bandwidth selection. We describe the double kernel conditional density estimator and derive fast dual-tree-based algorithms for bandwidth selection using a maximum likelihood criterion. These techniques give speedups of up to 3.8 million in our experiments, and enable the first applications to previously intractable large multivariate datasets, including a redshift prediction problem from the Sloan Digital Sky Survey.},
  archivePrefix = {arXiv},
  eprint = {1206.5278},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Holmes_et_al_2012_Fast_Nonparametric_Conditional_Density_Estimation.pdf;/Users/ilyes/Zotero/storage/JLQMRCN7/1206.html},
  journal = {arXiv:1206.5278 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@inproceedings{hoyer2009nonlinear,
  title = {Nonlinear Causal Discovery with Additive Noise Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hoyer, Patrik O. and Janzing, Dominik and Mooij, Joris M. and Peters, Jonas and Sch{\"o}lkopf, Bernhard},
  year = {2009},
  pages = {689--696},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Hoyer_et_al_2009_Nonlinear_causal_discovery_with_additive_noise_models.pdf;/Users/ilyes/Zotero/storage/5LMCIWD8/3548-nonlinear-causal-discovery-with-additive-noise-models.html}
}

@article{hyndman1996estimating,
  title = {Estimating and {{Visualizing Conditional Densities}}},
  author = {Hyndman, Rob J. and Bashtannyk, David M. and Grunwald, Gary K.},
  year = {1996},
  volume = {5},
  pages = {315--336},
  issn = {1061-8600},
  doi = {10.2307/1390887},
  abstract = {We consider the kernel estimator of conditional density and derive its asymptotic bias, variance, and mean-square error. Optimal bandwidths (with respect to integrated mean-square error) are found and it is shown that the convergence rate of the density estimator is order n\textsuperscript{-2/3}. We also note that the conditional mean function obtained from the estimator is equivalent to a kernel smoother. Given the undesirable bias properties of kernel smoothers, we seek a modified conditional density estimator that has mean equivalent to some other nonparametric regression smoother with better bias properties. It is also shown that our modified estimator has smaller mean square error than the standard estimator in some commonly occurring situations. Finally, three graphical methods for visualizing conditional density estimators are discussed and applied to a data set consisting of maximum daily temperatures in Melbourne, Australia.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Hyndman_et_al_1996_Estimating_and_Visualizing_Conditional_Densities.pdf},
  journal = {Journal of Computational and Graphical Statistics},
  number = {4}
}

@article{hyvarinen1999fast,
  title = {Fast and Robust Fixed-Point Algorithms for Independent Component Analysis},
  author = {Hyv{\"a}rinen, Aapo},
  year = {1999},
  volume = {10},
  pages = {626--634},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_1999_Fast_and_robust_fixed-point_algorithms_for_independent_component_analysis.pdf;/Users/ilyes/Zotero/storage/E7L63HHY/761722.html},
  journal = {IEEE transactions on Neural Networks},
  keywords = {ICA},
  number = {3}
}

@article{hyvarinen1999nonlinear,
  title = {Nonlinear Independent Component Analysis: {{Existence}} and Uniqueness Results},
  shorttitle = {Nonlinear Independent Component Analysis},
  author = {Hyv{\"a}rinen, Aapo and Pajunen, Petteri},
  year = {1999},
  month = apr,
  volume = {12},
  pages = {429--439},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00140-3},
  abstract = {The question of existence and uniqueness of solutions for nonlinear independent component analysis is addressed. It is shown that if the space of mixing functions is not limited there exists always an infinity of solutions. In particular, it is shown how to construct parameterized families of solutions. The indeterminacies involved are not trivial, as in the linear case. Next, it is shown how to utilize some results of complex analysis to obtain uniqueness of solutions. We show that for two dimensions, the solution is unique up to a rotation, if the mixing function is constrained to be a conformal mapping together with some other assumptions. We also conjecture that the solution is strictly unique except in some degenerate cases, as the indeterminacy implied by the rotation is essentially similar to estimating the model of linear ICA.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Pajunen_1999_Nonlinear_independent_component_analysis_-_Existence_and_uniqueness_results.pdf;/Users/ilyes/Zotero/storage/X5AP36UX/S0893608098001403.html},
  journal = {Neural Networks},
  keywords = {Blind source separation,Feature extraction,Independent component analysis,Redundancy reduction},
  number = {3}
}

@article{hyvarinen2000independent,
  title = {Independent Component Analysis: Algorithms and Applications},
  shorttitle = {Independent Component Analysis},
  author = {Hyv{\"a}rinen, Aapo and Oja, E.},
  year = {2000},
  month = jun,
  volume = {13},
  pages = {411--430},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(00)00026-5},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Oja_2000_Independent_component_analysis_-_algorithms_and_applications.pdf;/Users/ilyes/Zotero/storage/S5EZ9MQS/S0893608000000265.html},
  journal = {Neural Networks},
  keywords = {Blind signal separation,Factor analysis,ICA,Independent component analysis,Projection pursuit,Representation,Source separation},
  number = {4}
}

@book{hyvarinen2001independent,
  title = {Independent Component Analysis},
  author = {Hyv{\"a}rinen, Aapo and Karhunen, Juha and Oja, Erkki},
  year = {2001},
  publisher = {{John Wiley \& Sons}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Hyvärinen_et_al_2001_Independent_component_analysis.pdf;/Users/ilyes/Zotero/storage/Q8APBQQH/books.html},
  keywords = {ICA}
}

@article{hyvarinen2003bubbles,
  title = {Bubbles: A Unifying Framework for Low-Level Statistical Properties of Natural Image Sequences},
  shorttitle = {Bubbles},
  author = {Hyv{\"a}rinen, Aapo and Hurri, Jarmo and V{\"a}yrynen, Jaakko},
  year = {2003},
  volume = {20},
  pages = {1237--1252},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Hyvärinen_et_al_2003_Bubbles_-_a_unifying_framework_for_low-level_statistical_properties_of_natural.pdf;/Users/ilyes/Zotero/storage/ASWETCP4/abstract.html},
  journal = {JOSA A},
  number = {7}
}

@article{hyvarinen2004blind,
  title = {Blind Separation of Sources That Have Spatiotemporal Variance Dependencies},
  author = {Hyv{\"a}rinen, Aapo and Hurri, Jarmo},
  year = {2004},
  month = feb,
  volume = {84},
  pages = {247--254},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2003.10.010},
  abstract = {In blind source separation methods, the sources are typically assumed to be independent. Some methods are also able to separate dependent sources by estimating or assuming a parametric model for their dependencies. Here, we propose a method that separates dependent sources without a parametric model of their dependency structure. This is possible by introducing some general assumptions on the structure of the dependencies: the sources are dependent only through their variances (general activity levels), and the variances of the sources have temporal correlations. The method can be called double-blind because of this additional blind aspect: We do not need to estimate (or assume) a parametric model of the dependencies, which is in stark contrast to most previous methods.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Hyvärinen_Hurri_2004_Blind_separation_of_sources_that_have_spatiotemporal_variance_dependencies.pdf;/Users/ilyes/Zotero/storage/DLZJ9U34/S0165168403002755.html},
  journal = {Signal Processing},
  keywords = {Blind source separation,Dependent component analysis,Higher-order cumulants,Independent component analysis},
  language = {en},
  number = {2},
  series = {Special {{Section}} on  {{Independent Component Analysis}} and {{Beyond}}}
}

@article{hyvarinen2005estimation,
  title = {Estimation of Non-Normalized Statistical Models by Score Matching},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2005},
  volume = {6},
  pages = {695--709},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hyvärinen_2005_Estimation_of_non-normalized_statistical_models_by_score_matching.pdf;/Users/ilyes/Zotero/storage/CZ7F8KR2/hyvarinen05a.html},
  journal = {Journal of Machine Learning Research},
  number = {Apr}
}

@article{hyvarinen2007extensions,
  title = {Some Extensions of Score Matching},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2007},
  volume = {51},
  pages = {2499--2512},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hyvärinen_2007_Some_extensions_of_score_matching.pdf;/Users/ilyes/Zotero/storage/7DMEGN37/S0167947306003264.html},
  journal = {Computational statistics \& data analysis},
  number = {5}
}

@article{hyvarinen2007extensionsa,
  title = {Some Extensions of Score Matching},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2007},
  volume = {51},
  pages = {2499--2512},
  publisher = {{Elsevier}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hyvärinen_2007_Some_extensions_of_score_matching2.pdf;/Users/ilyes/Zotero/storage/KFD6SSC9/S0167947306003264.html},
  journal = {Computational statistics \& data analysis},
  number = {5}
}

@article{hyvarinen2013pairwise,
  title = {Pairwise {{Likelihood Ratios}} for {{Estimation}} of {{Non}}-{{Gaussian Structural Equation Models}}},
  author = {Hyv{\"a}rinen, Aapo and Smith, Stephen M.},
  year = {2013},
  volume = {14},
  pages = {111--152},
  issn = {ISSN 1533-7928},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Causality/Hyvärinen_Smith_2013_Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation.pdf;/Users/ilyes/Zotero/storage/V94VH6FS/hyvarinen13a.html},
  journal = {Journal of Machine Learning Research},
  number = {Jan}
}

@inproceedings{hyvarinen2016unsupervised,
  title = {Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear {{ICA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hyv{\"a}rinen, Aapo and Morioka, Hiroshi},
  year = {2016},
  pages = {3765--3773},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Morioka_2016_Unsupervised_feature_extraction_by_time-contrastive_learning_and_nonlinear_ica.pdf;/Users/ilyes/Zotero/storage/7PR36K9V/6394-unsupervised-feature-extraction-by-time-contrastive-learning-and-nonlinear-ica.html},
  keywords = {ICA,NICA,read}
}

@inproceedings{hyvarinen2017nonlinear,
  title = {Nonlinear {{ICA}} of Temporally Dependent Stationary Sources},
  booktitle = {The 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Hyv{\"a}rinen, Aapo and Morioka, Hiroshi},
  year = {2017},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Hyvärinen_Morioka_2017_Nonlinear_ICA_of_temporally_dependent_stationary_sources.pdf},
  keywords = {ICA,NICA}
}

@inproceedings{hyvarinen2019nonlinear,
  title = {Nonlinear {{ICA Using Auxiliary Variables}} and {{Generalized Contrastive Learning}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Hyv{\"a}rinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  year = {2019},
  month = apr,
  pages = {859--868},
  abstract = {Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Rec...},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvarinen_et_al_2019_Nonlinear_ICA_Using_Auxiliary_Variables_and_Generalized_Contrastive_Learning.pdf;/Users/ilyes/Zotero/storage/5EVMSYK2/hyvarinen19a.html},
  language = {en}
}

@article{izbicki2016nonparametric,
  title = {Nonparametric {{Conditional Density Estimation}} in a {{High}}-{{Dimensional Regression Setting}}},
  author = {Izbicki, Rafael and Lee, Ann B.},
  year = {2016},
  month = oct,
  volume = {25},
  pages = {1297--1316},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2015.1094393},
  abstract = {In some applications (e.g., in cosmology and economics), the regression E[Z|x] is not adequate to represent the association between a predictor x and a response Z because of multi-modality and asymmetry of f(z|x); using the full density instead of a single-point estimate can then lead to less bias in subsequent analysis. As of now, there are no effective ways of estimating f(z|x) when x represents high-dimensional, complex data. In this paper, we propose a new nonparametric estimator of f(z|x) that adapts to sparse (low-dimensional) structure in x. By directly expanding f(z|x) in the eigenfunctions of a kernel-based operator, we avoid tensor products in high dimensions as well as ratios of estimated densities. Our basis functions are orthogonal with respect to the underlying data distribution, allowing fast implementation and tuning of parameters. We derive rates of convergence and show that the method adapts to the intrinsic dimension of the data. We also demonstrate the effectiveness of the series method on images, spectra, and an application to photometric redshift estimation of galaxies.},
  archivePrefix = {arXiv},
  eprint = {1604.00540},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Izbicki_Lee_2016_Nonparametric_Conditional_Density_Estimation_in_a_High-Dimensional_Regression.pdf;/Users/ilyes/Zotero/storage/LD4AQUNI/1604.html},
  journal = {Journal of Computational and Graphical Statistics},
  keywords = {Statistics - Methodology},
  number = {4}
}

@article{jang2016categorical,
  title = {Categorical Reparameterization with Gumbel-Softmax},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2016},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Jang_et_al_2016_Categorical_reparameterization_with_gumbel-softmax.pdf;/Users/ilyes/Zotero/storage/3TX5ATET/1611.html},
  journal = {arXiv preprint arXiv:1611.01144}
}

@misc{khemakhem2018analysis,
  title = {Analysis of {{Fourier MMD}} and a Possible Application to Sampling},
  author = {Khemakhem, Ilyes},
  year = {2018},
  month = jun,
  abstract = {Sampling from a transformation of the MMD from an analytical pdf.},
  copyright = {All rights reserved},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Random/Khemakhem_2018_Analysis_of_Fourier_MMD_and_a_possible_application_to_sampling.pdf},
  language = {English}
}

@misc{khemakhem2019quick,
  title = {Quick Proof of Identifiability for Noisy Nonlinear {{ICA}} Using Auxiliary Variables},
  author = {Khemakhem, Ilyes},
  year = {2019},
  month = may,
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Mine/Khemakhem_2019_quick_proof_of_identifiability_for_noisy_nonlinear_ICA_using_auxiliary_variables.pdf}
}

@article{khemakhem2020icebeem,
  title = {{{ICE}}-{{BeeM}}: {{Identifiable Conditional Energy}}-{{Based Deep Models}}},
  shorttitle = {{{ICE}}-{{BeeM}}},
  author = {Khemakhem, Ilyes and Monti, Ricardo Pio and Kingma, Diederik P. and Hyv{\"a}rinen, Aapo},
  year = {2020},
  month = feb,
  abstract = {Despite the growing popularity of energy-based models, their identifiability properties are not well-understood. In this paper we establish sufficient conditions under which a large family of conditional energy-based models is identifiable in function space, up to a simple transformation. Our results build on recent developments in the theory of nonlinear ICA, showing that the latent representations in certain families of deep latent-variable models are identifiable. We extend these results to a very broad family of conditional energy-based models. In this family, the energy function is simply the dot-product between two feature extractors, one for the dependent variable, and one for the conditioning variable. We show that under mild conditions, the features are unique up to scaling and permutation. Second, we propose the framework of independently modulated component analysis (IMCA), a new form of nonlinear ICA where the indepencence assumption is relaxed. Importantly, we show that our energy-based model can be used for the estimation of the components: the features learned are a simple and often trivial transformation of the latents.},
  archivePrefix = {arXiv},
  eprint = {2002.11537},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Khemakhem_et_al_2020_ICE-BeeM_-_Identifiable_Conditional_Energy-Based_Deep_Models.pdf;/Users/ilyes/Zotero/storage/S27RAMIR/2002.html},
  journal = {arXiv:2002.11537 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{khemakhem2020variational,
  title = {Variational {{Autoencoders}} and {{Nonlinear ICA}}: {{A Unifying Framework}}},
  shorttitle = {Variational {{Autoencoders}} and {{Nonlinear ICA}}},
  booktitle = {The 23rd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Khemakhem, Ilyes and Kingma, Diederik P. and Monti, Ricardo Pio and Hyv{\"a}rinen, Aapo},
  year = {2020},
  month = jun,
  abstract = {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.},
  archivePrefix = {arXiv},
  eprint = {1907.04809},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Khemakhem_et_al_2019_Variational_Autoencoders_and_Nonlinear_ICA_-_A_Unifying_Framework.pdf;/Users/ilyes/Zotero/storage/9IKWJXF4/1907.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kiessediscrete,
  title = {Discrete and Continuous Nonparametric Kernel Estimations for Global Sensitivity Analysis},
  author = {Kiess{\'e}, Tristan Senga and Andrianandraina, Andy and {Eco-construction}, Chaire G{\'e}nie Civil},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Kiessé_et_al_Discrete_and_continuous_nonparametric_kernel_estimations_for_global_sensitivity.pdf}
}

@article{kim2018disentangling,
  title = {Disentangling by {{Factorising}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = {2018},
  month = feb,
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \$\textbackslash{}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  archivePrefix = {arXiv},
  eprint = {1802.05983},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Kim_Mnih_2018_Disentangling_by_Factorising.pdf;/Users/ilyes/Zotero/storage/GTHJGFSD/1802.html},
  journal = {arXiv:1802.05983 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,VAE},
  primaryClass = {cs, stat}
}

@article{kim2018semiamortized,
  title = {Semi-{{Amortized Variational Autoencoders}}},
  author = {Kim, Yoon and Wiseman, Sam and Miller, Andrew C. and Sontag, David and Rush, Alexander M.},
  year = {2018},
  month = feb,
  abstract = {Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.},
  archivePrefix = {arXiv},
  eprint = {1802.02550},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Kim_et_al_2018_Semi-Amortized_Variational_Autoencoders.pdf;/Users/ilyes/Zotero/storage/8APNXGV9/1802.html},
  journal = {arXiv:1802.02550 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kim2019disentangling,
  title = {Disentangling by {{Factorising}}},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = {2019},
  month = jul,
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \$\textbackslash{}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  archivePrefix = {arXiv},
  eprint = {1802.05983},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Kim_Mnih_2019_Disentangling_by_Factorising.pdf;/Users/ilyes/Zotero/storage/PXGEQKAI/1802.html},
  journal = {arXiv:1802.05983 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{kingma2010regularized,
  title = {Regularized Estimation of Image Statistics by {{Score Matching}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Kingma, Diederik P. and LeCun, Yann L.},
  editor = {Lafferty, J. D. and Williams, C. K. I. and {Shawe-Taylor}, J. and Zemel, R. S. and Culotta, A.},
  year = {2010},
  pages = {1126--1134},
  publisher = {{Curran Associates, Inc.}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Kingma_Cun_2010_Regularized_estimation_of_image_statistics_by_Score_Matching.pdf;/Users/ilyes/Zotero/storage/ME5VBSDL/4060-regularized-estimation-of-image-statistics-by-score-matching.html}
}

@inproceedings{kingma2013autoencoding,
  title = {Auto-{{Encoding Variational Bayes}}},
  booktitle = {{{arXiv}}:1312.6114 [Cs, Stat]},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VI/Kingma_Welling_2013_Auto-Encoding_Variational_Bayes.pdf;/Users/ilyes/Zotero/storage/TUSF5A7H/1312.html},
  keywords = {VAE,Variational autoencoders},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kingma2014adam,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  month = dec,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Kingma_Ba_2014_Adam_-_A_Method_for_Stochastic_Optimization.pdf;/Users/ilyes/Zotero/storage/5LZF4VRP/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kingma2014semisupervised,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  year = {2014},
  month = jun,
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1406.5298},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Kingma_et_al_2014_Semi-Supervised_Learning_with_Deep_Generative_Models.pdf;/Users/ilyes/Zotero/storage/BV6AXDTG/1406.html},
  journal = {arXiv:1406.5298 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kingma2016improving,
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2016},
  month = jun,
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  archivePrefix = {arXiv},
  eprint = {1606.04934},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Kingma_et_al_2016_Improving_Variational_Inference_with_Inverse_Autoregressive_Flow.pdf;/Users/ilyes/Zotero/storage/MRQH4N3Y/1606.html},
  journal = {arXiv:1606.04934 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{knight2008sinkhornknopp,
  title = {The {{Sinkhorn}}\textendash{{Knopp Algorithm}}: {{Convergence}} and {{Applications}}},
  shorttitle = {The {{Sinkhorn}}\textendash{{Knopp Algorithm}}},
  author = {Knight, P.},
  year = {2008},
  month = jan,
  volume = {30},
  pages = {261--275},
  issn = {0895-4798},
  doi = {10.1137/060659624},
  abstract = {As long as a square nonnegative matrix A contains sufficient nonzero elements, then the Sinkhorn\textendash{}Knopp algorithm can be used to balance the matrix, that is, to find a diagonal scaling of A that is doubly stochastic. It is known that the convergence is linear, and an upper bound has been given for the rate of convergence for positive matrices. In this paper we give an explicit expression for the rate of convergence for fully indecomposable matrices. We describe how balancing algorithms can be used to give a measure of web page significance. We compare the measure with some well known alternatives, including PageRank. We show that, with an appropriate modification, the Sinkhorn\textendash{}Knopp algorithm is a natural candidate for computing the measure on enormous data sets.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Knight_2008_The_Sinkhorn–Knopp_Algorithm_-_Convergence_and_Applications.pdf;/Users/ilyes/Zotero/storage/HPUAR9NP/060659624.html},
  journal = {SIAM Journal on Matrix Analysis and Applications},
  number = {1}
}

@article{kocev2013tree,
  title = {Tree Ensembles for Predicting Structured Outputs},
  author = {Kocev, Dragi and Vens, Celine and Struyf, Jan and D{\v z}eroski, Sa{\v s}o},
  year = {2013},
  month = mar,
  volume = {46},
  pages = {817--833},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.09.023},
  abstract = {In this paper, we address the task of learning models for predicting structured outputs. We consider both global and local predictions of structured outputs, the former based on a single model that predicts the entire output structure and the latter based on a collection of models, each predicting a component of the output structure. We use ensemble methods and apply them in the context of predicting structured outputs. We propose to build ensemble models consisting of predictive clustering trees, which generalize classification trees: these have been used for predicting different types of structured outputs, both locally and globally. More specifically, we develop methods for learning two types of ensembles (bagging and random forests) of predictive clustering trees for global and local predictions of different types of structured outputs. The types of outputs considered correspond to different predictive modeling tasks: multi-target regression, multi-target classification, and hierarchical multi-label classification. Each of the combinations can be applied both in the context of global prediction (producing a single ensemble) or local prediction (producing a collection of ensembles). We conduct an extensive experimental evaluation across a range of benchmark datasets for each of the three types of structured outputs. We compare ensembles for global and local prediction, as well as single trees for global prediction and tree collections for local prediction, both in terms of predictive performance and in terms of efficiency (running times and model complexity). The results show that both global and local tree ensembles perform better than the single model counterparts in terms of predictive power. Global and local tree ensembles perform equally well, with global ensembles being more efficient and producing smaller models, as well as needing fewer trees in the ensemble to achieve the maximal performance.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Kocev_et_al_2013_Tree_ensembles_for_predicting_structured_outputs.pdf;/Users/ilyes/Zotero/storage/9H9BXVPW/S003132031200430X.html},
  journal = {Pattern Recognition},
  keywords = {Ensemble methods,Hierarchical multi-label classification,Multi-target classification,Multi-target regression,Predictive clustering trees,Structured outputs},
  number = {3}
}

@article{kokonendji2011discrete,
  title = {Discrete Associated Kernels Method and Extensions},
  author = {Kokonendji, C{\'e}lestin C. and Kiesse, Tristan Senga},
  year = {2011},
  volume = {8},
  pages = {497--516},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Kokonendji_Kiesse_2011_Discrete_associated_kernels_method_and_extensions.pdf;/Users/ilyes/Zotero/storage/DK35KZE6/S1572312711000670.html},
  journal = {Statistical Methodology},
  number = {6}
}

@article{korbar2018cooperative,
  title = {Cooperative {{Learning}} of {{Audio}} and {{Video Models}} from {{Self}}-{{Supervised Synchronization}}},
  author = {Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  year = {2018},
  month = jun,
  abstract = {There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9\% in action recognition accuracy on UCF101 and a boost of +17.7\% on HMDB51.},
  archivePrefix = {arXiv},
  eprint = {1807.00230},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Korbar_et_al_2018_Cooperative_Learning_of_Audio_and_Video_Models_from_Self-Supervised.pdf;/Users/ilyes/Zotero/storage/9CI6WA34/1807.html},
  journal = {arXiv:1807.00230 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{kuhn1955hungarian,
  title = {The {{Hungarian}} Method for the Assignment Problem},
  author = {Kuhn, H. W.},
  year = {1955},
  month = mar,
  volume = {2},
  pages = {83--97},
  issn = {1931-9193},
  doi = {10.1002/nav.3800020109},
  abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the ``assignment problem'' is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
  file = {/Users/ilyes/Zotero/storage/7PEIHZ76/abstract.html},
  journal = {Naval Research Logistics Quarterly},
  language = {en},
  number = {1-2}
}

@inproceedings{le2011ica,
  title = {{{ICA}} with {{Reconstruction Cost}} for {{Efficient Overcomplete Feature Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Le, Quoc V. and Karpenko, Alexandre and Ngiam, Jiquan and Ng, Andrew Y.},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {1017--1025},
  publisher = {{Curran Associates, Inc.}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Le_et_al_2011_ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.pdf;/Users/ilyes/Zotero/storage/9R7X9GKB/4467-ica-with-reconstruction-cost-for-efficient-overcomplete-feature-learning.html}
}

@article{lecun2015deep,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  volume = {521},
  pages = {436},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning},
  copyright = {2015 Nature Publishing Group},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/LeCun_et_al_2015_Deep_learning.pdf;/Users/ilyes/Zotero/storage/I5PRJDMM/nature14539.html},
  journal = {Nature},
  language = {En},
  number = {7553}
}

@book{lee2003introduction,
  title = {Introduction to {{Smooth Manifolds}}},
  author = {Lee, John M.},
  year = {2003},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {Manifolds are everywhere. These generalizations of curves and surfaces to arbitrarily many dimensions provide the mathematical context for under\- standing "space" in all of its manifestations. Today, the tools of manifold theory are indispensable in most major subfields of pure mathematics, and outside of pure mathematics they are becoming increasingly important to scientists in such diverse fields as genetics, robotics, econometrics, com\- puter graphics, biomedical imaging, and, of course, the undisputed leader among consumers (and inspirers) of mathematics-theoretical physics. No longer a specialized subject that is studied only by differential geometers, manifold theory is now one of the basic skills that all mathematics students should acquire as early as possible. Over the past few centuries, mathematicians have developed a wondrous collection of conceptual machines designed to enable us to peer ever more deeply into the invisible world of geometry in higher dimensions. Once their operation is mastered, these powerful machines enable us to think geometrically about the 6-dimensional zero set of a polynomial in four complex variables, or the lO-dimensional manifold of 5 x 5 orthogonal ma\- trices, as easily as we think about the familiar 2-dimensional sphere in ]R3.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Lee_2003_Introduction_to_Smooth_Manifolds.pdf;/Users/ilyes/Zotero/storage/T6X5N8XV/9780387217529.html},
  isbn = {978-0-387-21752-9},
  language = {en},
  series = {Graduate {{Texts}} in {{Mathematics}}}
}

@book{lee2011introduction,
  title = {Introduction to {{Topological Manifolds}}},
  author = {Lee, John},
  year = {2011},
  edition = {2},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  abstract = {This book is an introduction to manifolds at the beginning graduate level. It contains the essential topological ideas that are needed for the further study of manifolds, particularly in the context of differential geometry, algebraic topology, and related fields. Its guiding philosophy is to develop these ideas rigorously but economically, with minimal prerequisites and plenty of geometric intuition.Although this second edition has the same basic structure as the first edition, it has been extensively revised and clarified; not a single page has been left untouched. The major changes include a new introduction to CW complexes (replacing most of the material on simplicial complexes in Chapter 5); expanded treatments of manifolds with boundary, local compactness, group actions, and proper maps; and a new section on paracompactness.This text is designed to be used for an introductory graduate course on the geometry and topology of manifolds. It should be accessible to any student who has completed a solid undergraduate degree in mathematics. The author's book Introduction to Smooth Manifolds is meant to act as a sequel to this book.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Lee_2011_Introduction_to_Topological_Manifolds.pdf;/Users/ilyes/Zotero/storage/9S2QN3YZ/9781441979391.html},
  isbn = {978-1-4419-7939-1},
  language = {en},
  series = {Graduate {{Texts}} in {{Mathematics}}}
}

@book{legall2006intégration,
  title = {Int{\'e}gration, Probabilit{\'e}s et Processus Al{\'e}atoires},
  author = {Le Gall, Jean-Fran{\c c}ois},
  year = {2006},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Maths/Le_Gall_2006_Intégration,_probabilités_et_processus_aléatoires.pdf}
}

@article{levy2015numerical,
  title = {A {{Numerical Algorithm}} for {{L2 Semi}}-{{Discrete Optimal Transport}} in {{3D}}},
  author = {L{\'e}vy, Bruno},
  year = {2015},
  month = nov,
  volume = {49},
  pages = {1693--1715},
  issn = {0764-583X, 1290-3841},
  doi = {10.1051/m2an/2015055},
  abstract = {ESAIM: Mathematical Modelling and Numerical Analysis, an international journal on applied mathematics},
  copyright = {\textcopyright{} EDP Sciences, SMAI 2015},
  file = {/Users/ilyes/Zotero/storage/MVHJSKDT/m2an150137.html},
  journal = {ESAIM: Mathematical Modelling and Numerical Analysis},
  language = {en},
  number = {6}
}

@inproceedings{li2015generative,
  title = {Generative Moment Matching Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
  year = {2015},
  pages = {1718--1727},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Li_et_al_2015_Generative_moment_matching_networks.pdf},
  keywords = {ideas}
}

@article{li2017alice,
  title = {{{ALICE}}: {{Towards Understanding Adversarial Learning}} for {{Joint Distribution Matching}}},
  shorttitle = {{{ALICE}}},
  author = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yunchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
  year = {2017},
  month = sep,
  abstract = {We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.},
  archivePrefix = {arXiv},
  eprint = {1709.01215},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Li_et_al_2017_ALICE_-_Towards_Understanding_Adversarial_Learning_for_Joint_Distribution.pdf;/Users/ilyes/Zotero/storage/WQYGXKHU/1709.html},
  journal = {arXiv:1709.01215 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{li2019annealed,
  title = {Annealed {{Denoising Score Matching}}: {{Learning Energy}}-{{Based Models}} in {{High}}-{{Dimensional Spaces}}},
  shorttitle = {Annealed {{Denoising Score Matching}}},
  author = {Li, Zengyi and Chen, Yubei and Sommer, Friedrich T.},
  year = {2019},
  month = oct,
  abstract = {Energy-Based Models (EBMs) outputs unmormalized log-probability values given data samples. Such an estimation is essential in a variety of applications such as sample generation, denoising, sample restoration, outlier detection, Bayesian reasoning, and many more. However, standard maximum likelihood training is computationally expensive due to the requirement of sampling the model distribution. Score matching potentially alleviates this problem, and denoising score matching is a particularly convenient version. However, previous works do not produce models capable of high quality sample synthesis in high dimensional datasets from random initialization. We believe that is because the score is only matched over a single noise scale, which corresponds to a small set in high-dimensional space. To overcome this limitation, here we instead learn an energy function using denoising score matching over all noise scales. When sampled from random initialization using Annealed Langevin Dynamics and single-step denoising jump, our model produced high-quality samples comparable to state-of-the-art techniques such as GANs. The learned model also provide density information and set a new sample quality baseline in energy-based models. We further demonstrate that the proposed method generalizes well with an image inpainting task.},
  archivePrefix = {arXiv},
  eprint = {1910.07762},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Li_et_al_2019_Annealed_Denoising_Score_Matching_-_Learning_Energy-Based_Models_in.pdf;/Users/ilyes/Zotero/storage/K2KGKSTN/1910.html},
  journal = {arXiv:1910.07762 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{liero2015optimal,
  title = {Optimal {{Entropy}}-{{Transport}} Problems and a New {{Hellinger}}-{{Kantorovich}} Distance between Positive Measures},
  author = {Liero, Matthias and Mielke, Alexander and Savar{\'e}, Giuseppe},
  year = {2015},
  month = aug,
  abstract = {We develop a full theory for the new class of Optimal Entropy-Transport problems between nonnegative and finite Radon measures in general topological spaces. They arise quite naturally by relaxing the marginal constraints typical of Optimal Transport problems: given a couple of finite measures (with possibly different total mass), one looks for minimizers of the sum of a linear transport functional and two convex entropy functionals, that quantify in some way the deviation of the marginals of the transport plan from the assigned measures. As a powerful application of this theory, we study the particular case of Logarithmic Entropy-Transport problems and introduce the new Hellinger-Kantorovich distance between measures in metric spaces. The striking connection between these two seemingly far topics allows for a deep analysis of the geometric properties of the new geodesic distance, which lies somehow between the well-known Hellinger-Kakutani and Kantorovich-Wasserstein distances.},
  archivePrefix = {arXiv},
  eprint = {1508.07941},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Liero_et_al_2015_Optimal_Entropy-Transport_problems_and_a_new_Hellinger-Kantorovich_distance.pdf;/Users/ilyes/Zotero/storage/94VGF6IN/1508.html},
  journal = {arXiv:1508.07941 [math]},
  keywords = {Mathematics - Optimization and Control},
  primaryClass = {math}
}

@inproceedings{liu2016stein,
  title = {Stein Variational Gradient Descent: {{A}} General Purpose Bayesian Inference Algorithm},
  shorttitle = {Stein Variational Gradient Descent},
  booktitle = {Advances {{In Neural Information Processing Systems}}},
  author = {Liu, Qiang and Wang, Dilin},
  year = {2016},
  pages = {2378--2386},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Liu_Wang_2016_Stein_variational_gradient_descent_-_A_general_purpose_bayesian_inference.pdf;/Users/ilyes/Zotero/storage/ALD7XAVX/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.html}
}

@inproceedings{liu2017stein,
  title = {Stein Variational Gradient Descent as Gradient Flow},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Liu, Qiang},
  year = {2017},
  pages = {3118--3126},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Liu_2017_Stein_variational_gradient_descent_as_gradient_flow.pdf;/Users/ilyes/Zotero/storage/CDY3EJIN/6904-stein-variational-gradient-descent-as-gradient-flow.html}
}

@article{locatello2018challenging,
  title = {Challenging {{Common Assumptions}} in the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2018},
  month = nov,
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look on recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties `encouraged' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  archivePrefix = {arXiv},
  eprint = {1811.12359},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Locatello_et_al_2018_Challenging_Common_Assumptions_in_the_Unsupervised_Learning_of_Disentangled.pdf;/Users/ilyes/Zotero/storage/RFXIEM2U/1811.html},
  journal = {arXiv:1811.12359 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{louizos2017causal,
  title = {Causal {{Effect Inference}} with {{Deep Latent}}-{{Variable Models}}},
  author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
  year = {2017},
  month = may,
  abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
  archivePrefix = {arXiv},
  eprint = {1705.08821},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Louizos_et_al_2017_Causal_Effect_Inference_with_Deep_Latent-Variable_Models.pdf;/Users/ilyes/Zotero/storage/7KWBIZUA/1705.html},
  journal = {arXiv:1705.08821 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{love2010linux,
  title = {Linux Kernel Development},
  author = {Love, Robert},
  year = {2010},
  publisher = {{Pearson Education}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Random/Love_2010_Linux_kernel_development.pdf;/Users/ilyes/Zotero/storage/AFVADFWJ/books.html}
}

@inproceedings{maaloe2019biva,
  title = {{{BIVA}}: {{A}} Very Deep Hierarchy of Latent Variables for Generative Modeling},
  shorttitle = {{{BIVA}}},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Maal{\o}e, Lars and Fraccaro, Marco and Li{\'e}vin, Valentin and Winther, Ole},
  year = {2019},
  pages = {6548--6558},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Maaløe_et_al_2019_BIVA_-_A_very_deep_hierarchy_of_latent_variables_for_generative_modeling.pdf;/Users/ilyes/Zotero/storage/PAQEGZXS/8882-biva-a-very-deep-hierarchy-of-latent-variables-for-generative-modeling.html}
}

@article{maddison2016concrete,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  shorttitle = {The {{Concrete Distribution}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2016},
  month = nov,
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  archivePrefix = {arXiv},
  eprint = {1611.00712},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Maddison_et_al_2016_The_Concrete_Distribution_-_A_Continuous_Relaxation_of_Discrete_Random_Variables.pdf;/Users/ilyes/Zotero/storage/XW8JUVUU/1611.html},
  journal = {arXiv:1611.00712 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{maddy1993does,
  title = {Does {{V}}. {{Equal L}}?},
  author = {Maddy, Penelope},
  year = {1993},
  volume = {58},
  pages = {15--41},
  issn = {0022-4812},
  doi = {10.2307/2275321},
  file = {/Users/ilyes/Zotero/storage/3JV3HXF5/Maddy_1993_Does_V.pdf},
  journal = {The Journal of Symbolic Logic},
  number = {1}
}

@article{maeda2020causal,
  title = {Causal Discovery of Linear Non-{{Gaussian}} Acyclic Models in the Presence of Latent Confounders},
  author = {Maeda, Takashi Nicholas and Shimizu, Shohei},
  year = {2020},
  month = jan,
  abstract = {Causal discovery from data affected by latent confounders is an important and difficult challenge. Causal functional model-based approaches have not been used to present variables whose relationships are affected by latent confounders, while some constraint-based methods can present them. This paper proposes a causal functional model-based method called repetitive causal discovery (RCD) to discover the causal structure of observed variables affected by latent confounders. RCD repeats inferring the causal directions between a small number of observed variables and determines whether the relationships are affected by latent confounders. RCD finally produces a causal graph where a bi-directed arrow indicates the pair of variables that have the same latent confounders, and a directed arrow indicates the causal direction of a pair of variables that are not affected by the same latent confounder. The results of experimental validation using simulated data and real-world data confirmed that RCD is effective in identifying latent confounders and causal directions between observed variables.},
  archivePrefix = {arXiv},
  eprint = {2001.04197},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Maeda_Shimizu_2020_Causal_discovery_of_linear_non-Gaussian_acyclic_models_in_the_presence_of.pdf;/Users/ilyes/Zotero/storage/SVYA5LLU/2001.html},
  journal = {arXiv:2001.04197 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{mainini2012description,
  title = {A Description of Transport Cost for Signed Measures},
  author = {Mainini, E.},
  year = {2012},
  month = mar,
  volume = {181},
  pages = {837--855},
  issn = {1072-3374, 1573-8795},
  doi = {10.1007/s10958-012-0718-2},
  abstract = {In this paper, we develop the analysis started in a paper by Ambrosio, Mainini, and Serfaty about the extension of the optimal transport framework to the space of real measures. The main motivation comes from the study of nonpositive solutions to some evolution PDEs. Although a canonical optimal transport distance does not seem to be available, we may describe the cost for transporting signed measures in various ways and with interesting properties. Bibliography: 22 titles.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Mainini_2012_A_description_of_transport_cost_for_signed_measures.pdf;/Users/ilyes/Zotero/storage/376BSQ6P/10.html},
  journal = {Journal of Mathematical Sciences},
  language = {en},
  number = {6}
}

@book{mallat1999wavelet,
  title = {A Wavelet Tour of Signal Processing},
  author = {Mallat, St{\'e}phane},
  year = {1999},
  publisher = {{Academic press}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Mallat_1999_A_wavelet_tour_of_signal_processing.pdf;/Users/ilyes/Zotero/storage/2BGJ6LJY/books.html;/Users/ilyes/Zotero/storage/8256AILT/books.html}
}

@book{mallot2013computational,
  title = {Computational {{Neuroscience}}: {{A First Course}}},
  shorttitle = {Computational {{Neuroscience}}},
  author = {Mallot, Hanspeter A.},
  year = {2013},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Computational Neuroscience - A First Course provides an essential introduction to computational neuroscience and equips readers with a fundamental understanding of modeling the nervous system at the membrane, cellular, and network level. The book, which grew out of a lecture series held regularly for more than ten years to graduate students in neuroscience with backgrounds in biology, psychology and medicine, takes its readers on a journey through three fundamental domains of computational neuroscience: membrane biophysics, systems theory and artificial neural networks. The required mathematical concepts are kept as intuitive and simple as possible throughout the book, making it fully accessible to readers who are less familiar with mathematics. Overall, Computational Neuroscience - A First Course represents an essential reference guide for all neuroscientists who use computational methods in their daily work, as well as for any theoretical scientist approaching the field of computational neuroscience.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Mallot_2013_Computational_Neuroscience_-_A_First_Course.pdf},
  googlebooks = {AcZDAAAAQBAJ},
  isbn = {978-3-319-00861-5},
  keywords = {biophysics,Computers / Intelligence (AI) \& Semantics,hodgkin huxley,Language Arts \& Disciplines / Library \& Information Science / General,Medical / Neuroscience,Science / Life Sciences / Neuroscience,sntn,Technology \& Engineering / General},
  language = {en}
}

@article{mathieu2018disentangling,
  title = {Disentangling {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Mathieu, Emile and Rainforth, Tom and Siddharth, N. and Teh, Yee Whye},
  year = {2018},
  month = dec,
  abstract = {We develop a generalisation of disentanglement in VAEs---decomposition of the latent representation---characterising it as the fulfilment of two factors: a) the latent encodings of the data having an appropriate level of overlap, and b) the aggregate encoding of the data conforming to a desired structure, represented through the prior. Decomposition permits disentanglement, i.e. explicit independence between latents, as a special case, but also allows for a much richer class of properties to be imposed on the learnt representation, such as sparsity, clustering, independent subspaces, or even intricate hierarchical dependency relationships. We show that the \$\textbackslash{}beta\$-VAE varies from the standard VAE predominantly in its control of latent overlap and that for the standard choice of an isotropic Gaussian prior, its objective is invariant to rotations of the latent representation. Viewed from the decomposition perspective, breaking this invariance with simple manipulations of the prior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of prior can assist in producing different decompositions and introduce an alternative training objective that allows the control of both decomposition factors in a principled manner.},
  archivePrefix = {arXiv},
  eprint = {1812.02833},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Mathieu_et_al_2018_Disentangling_Disentanglement_in_Variational_Autoencoders.pdf;/Users/ilyes/Zotero/storage/SWAKSH3J/1812.html},
  journal = {arXiv:1812.02833 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{mescheder2017adversarial,
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  shorttitle = {Adversarial {{Variational Bayes}}},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  year = {2017},
  month = jan,
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
  archivePrefix = {arXiv},
  eprint = {1701.04722},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Mescheder_et_al_2017_Adversarial_Variational_Bayes_-_Unifying_Variational_Autoencoders_and_Generative.pdf;/Users/ilyes/Zotero/storage/KEYZXBZD/1701.html},
  journal = {arXiv:1701.04722 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{misra2016shuffle,
  title = {Shuffle and {{Learn}}: {{Unsupervised Learning}} Using {{Temporal Order Verification}}},
  shorttitle = {Shuffle and {{Learn}}},
  author = {Misra, Ishan and Zitnick, C. Lawrence and Hebert, Martial},
  year = {2016},
  month = mar,
  abstract = {In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1603.08561},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Misra_et_al_2016_Shuffle_and_Learn_-_Unsupervised_Learning_using_Temporal_Order_Verification.pdf;/Users/ilyes/Zotero/storage/5ZCTE446/1603.html},
  journal = {arXiv:1603.08561 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{monge1781mémoire,
  title = {M{\'e}moire Sur La Theorie Des D{\'e}blais et Des Remblais},
  author = {MONGE, G.},
  year = {1781},
  file = {/Users/ilyes/Zotero/storage/ACMJKXQH/10018386702.html},
  journal = {Histoire de l'Academie Royale des Sciences de Paris}
}

@article{monti2018unified,
  title = {A {{Unified Probabilistic Model}} for {{Learning Latent Factors}} and {{Their Connectivities}} from {{High}}-{{Dimensional Data}}},
  author = {Monti, Ricardo Pio and Hyv{\"a}rinen, Aapo},
  year = {2018},
  month = may,
  abstract = {Connectivity estimation is challenging in the context of high-dimensional data. A useful preprocessing step is to group variables into clusters, however, it is not always clear how to do so from the perspective of connectivity estimation. Another practical challenge is that we may have data from multiple related classes (e.g., multiple subjects or conditions) and wish to incorporate constraints on the similarities across classes. We propose a probabilistic model which simultaneously performs both a grouping of variables (i.e., detecting community structure) and estimation of connectivities between the groups which correspond to latent variables. The model is essentially a factor analysis model where the factors are allowed to have arbitrary correlations, while the factor loading matrix is constrained to express a community structure. The model can be applied on multiple classes so that the connectivities can be different between the classes, while the community structure is the same for all classes. We propose an efficient estimation algorithm based on score matching, and prove the identifiability of the model. Finally, we present an extension to directed (causal) connectivities over latent variables. Simulations and experiments on fMRI data validate the practical utility of the method.},
  archivePrefix = {arXiv},
  eprint = {1805.09567},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Monti_Hyvärinen_2018_A_Unified_Probabilistic_Model_for_Learning_Latent_Factors_and_Their.pdf;/Users/ilyes/Zotero/storage/GRQR2XCW/1805.html},
  journal = {arXiv:1805.09567 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{monti2019causal,
  title = {Causal Discovery with General Non-Linear Relationships Using Non-Linear {{ICA}}},
  booktitle = {35th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}, {{UAI}} 2019},
  author = {Monti, Ricardo Pio and Zhang, Kun and Hyvarinen, Aapo},
  year = {2019},
  volume = {35},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Monti_et_al_2019_Causal_discovery_with_general_non-linear_relationships_using_non-linear_ICA.pdf;/Users/ilyes/Zotero/storage/WL5RB5AC/1904.html}
}

@article{moreno-bote2014informationlimiting,
  title = {Information-Limiting Correlations},
  author = {{Moreno-Bote}, Rub{\'e}n and Beck, Jeffrey and Kanitscheider, Ingmar and Pitkow, Xaq and Latham, Peter and Pouget, Alexandre},
  year = {2014},
  month = oct,
  volume = {17},
  pages = {1410--1417},
  issn = {1097-6256},
  doi = {10.1038/nn.3807},
  abstract = {Computational strategies used by the brain strongly depend on the amount of information that can be stored in population activity, which in turn strongly depends on the pattern of noise correlations. In vivo, noise correlations tend to be positive and proportional to the similarity in tuning properties. Such correlations are thought to limit information, which has led to the suggestion that decorrelation increases information. In contrast, we found, analytically and numerically, that decorrelation does not imply an increase in information. Instead, the only information-limiting correlations are what we refer to as differential correlations: correlations proportional to the product of the derivatives of the tuning curves. Unfortunately, differential correlations are likely to be very small and buried under correlations that do not limit information, making them particularly difficult to detect. We found, however, that the effect of differential correlations on information can be detected with relatively simple decoders.},
  copyright = {\textcopyright{} 2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Moreno-Bote_et_al_2014_Information-limiting_correlations.pdf;/Users/ilyes/Zotero/storage/MTBQDKD5/nn.3807.html},
  journal = {Nature Neuroscience},
  keywords = {Neural encoding},
  language = {en},
  number = {10}
}

@article{mroueh2018regularized,
  title = {Regularized {{Kernel}} and {{Neural Sobolev Descent}}: {{Dynamic MMD Transport}}},
  shorttitle = {Regularized {{Kernel}} and {{Neural Sobolev Descent}}},
  author = {Mroueh, Youssef and Sercu, Tom and Raj, Anant},
  year = {2018},
  month = may,
  abstract = {We introduce Regularized Kernel and Neural Sobolev Descent for transporting a source distribution to a target distribution along smooth paths of minimum kinetic energy (defined by the Sobolev discrepancy), related to dynamic optimal transport. In the kernel version, we give a simple algorithm to perform the descent along gradients of the Sobolev critic, and show that it converges asymptotically to the target distribution in the MMD sense. In the neural version, we parametrize the Sobolev critic with a neural network with input gradient norm constrained in expectation. We show in theory and experiments that regularization has an important role in favoring smooth transitions between distributions, avoiding large discrete jumps. Our analysis could provide a new perspective on the impact of critic updates (early stopping) on the paths to equilibrium in the GAN setting.},
  archivePrefix = {arXiv},
  eprint = {1805.12062},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Mroueh_et_al_2018_Regularized_Kernel_and_Neural_Sobolev_Descent_-_Dynamic_MMD_Transport.pdf;/Users/ilyes/Zotero/storage/GVPWFYM3/1805.html},
  journal = {arXiv:1805.12062 [cs, stat]},
  keywords = {Computer Science - Learning,flow,read,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{muandet2019dual,
  title = {Dual {{IV}}: {{A Single Stage Instrumental Variable Regression}}},
  shorttitle = {Dual {{IV}}},
  author = {Muandet, Krikamol and Mehrjou, Arash and Lee, Si Kai and Raj, Anant},
  year = {2019},
  month = oct,
  abstract = {We present a novel single-stage procedure for instrumental variable (IV) regression called DualIV which simplifies traditional two-stage regression via a dual formulation. We show that the common two-stage procedure can alternatively be solved via generalized least squares. Our formulation circumvents the first-stage regression which can be a bottleneck in modern two-stage procedures for IV regression. We also show that our framework is closely related to the generalized method of moments (GMM) with specific assumptions. This highlights the fundamental connection between GMM and two-stage procedures in IV literature. Using the proposed framework, we develop a simple kernel-based algorithm with consistency guarantees. Lastly, we give empirical results illustrating the advantages of our method over the existing two-stage algorithms.},
  archivePrefix = {arXiv},
  eprint = {1910.12358},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Muandet_et_al_2019_Dual_IV_-_A_Single_Stage_Instrumental_Variable_Regression.pdf;/Users/ilyes/Zotero/storage/BWXXGVDT/1910.html},
  journal = {arXiv:1910.12358 [cs, econ, stat]},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,Statistics - Machine Learning},
  primaryClass = {cs, econ, stat}
}

@article{munkres1957algorithms,
  title = {Algorithms for the {{Assignment}} and {{Transportation Problems}}},
  author = {Munkres, J.},
  year = {1957},
  month = mar,
  volume = {5},
  pages = {32--38},
  issn = {0368-4245},
  doi = {10.1137/0105003},
  file = {/Users/ilyes/Zotero/storage/CUFGDEAE/0105003.html},
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  number = {1}
}

@article{nalisnick2017variational,
  title = {Variational {{Reference Priors}}},
  author = {Nalisnick, Eric and Smyth, Padhraic},
  year = {2017},
  month = feb,
  abstract = {In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are...},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Nalisnick_Smyth_2017_Variational_Reference_Priors.pdf;/Users/ilyes/Zotero/storage/JPJJG2YC/forum.html}
}

@book{oksendal2003stochastic,
  title = {Stochastic {{Differential Equations}}: {{An Introduction}} with {{Applications}}},
  shorttitle = {Stochastic {{Differential Equations}}},
  author = {{\O}ksendal, Bernt},
  year = {2003},
  edition = {6},
  publisher = {{Springer-Verlag}},
  address = {{Berlin Heidelberg}},
  doi = {10.1007/978-3-642-14394-6},
  abstract = {From the reviews of the fifth edition: "This is a highly readable and refreshingly rigorous introduction to stochastic calculus. \ldots{} This is not a watered-down treatment. It is a serious introduction that starts with fundamental measure-theoretic concepts and ends, coincidentally, with the Black-Scholes formula as one of several examples of applications. This is the best single resource for learning the stochastic calculus \ldots{} ." (riskbook.com, 2002) From the reviews of the sixth edition: "The book \ldots{} has evolved from a 200-page typewritten booklet to a modern classic. Part of its charm and success is the fact that the author does not bother too much with the (for the novice) cumbersome rigorous theory \ldots{} . This does not mean that the book is not rigorous, it is just the timing and dosage of mathematical rigour \ldots{} that is palatable for undergraduates \ldots{} . a highly readable account, suitable for self-study and for use in the classroom." (Ren{\'e} L. Schilling, The Mathematical Gazette, March, 2005) "This is the sixth edition of the classical and excellent book on stochastic differential equations. The main difference with the next to last edition is the addition of detailed solutions of selected exercises \ldots{} . This is certainly an excellent idea in view to test its ability of applications of the concepts \ldots{} . certainly one of the best books on the subject, it will be very helpful to any graduate students and also very valuable for any analysts of financial market." (St{\'e}phane M{\'e}tens, Physicalia, Vol. 26 (1), 2004) "This is now the sixth edition of the excellent book on stochastic differential equations and related topics. \ldots{} the presentation is successfully balanced between being easily accessible for a broad audience and being mathematically rigorous. The book is a first choice for courses at graduate level in applied stochastic differential equations. The inclusion of detailed solutions to many of the exercises in this edition also makes it very useful for self-study." (Evelyn Buckwar, Zentralblatt MATH, Vol. 1025, 2003)},
  file = {/Users/ilyes/Zotero/storage/3B5X2T5H/Øksendal_2003_Stochastic_Differential_Equations_-_An_Introduction_with_Applications.pdf;/Users/ilyes/Zotero/storage/S4H4K7IG/9783540047582.html},
  isbn = {978-3-540-04758-2},
  language = {en},
  series = {Universitext}
}

@article{olshausen1996emergence,
  title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {1996},
  month = jun,
  volume = {381},
  pages = {607},
  issn = {1476-4687},
  doi = {10.1038/381607a0},
  abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\textendash{}4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\textendash{}12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\textendash{}18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  copyright = {1996 Nature Publishing Group},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Olshausen_Field_1996_Emergence_of_simple-cell_receptive_field_properties_by_learning_a_sparse_code.pdf;/Users/ilyes/Zotero/storage/2NFYWLXF/381607a0.html},
  journal = {Nature},
  language = {En},
  number = {6583}
}

@article{olshausen1997sparse,
  title = {Sparse Coding with an Overcomplete Basis Set: {{A}} Strategy Employed by {{V1}}?},
  shorttitle = {Sparse Coding with an Overcomplete Basis Set},
  author = {Olshausen, Bruno A. and Field, David J.},
  year = {1997},
  month = dec,
  volume = {37},
  pages = {3311--3325},
  issn = {0042-6989},
  doi = {10.1016/S0042-6989(97)00169-7},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete\textemdash{}i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Olshausen_Field_1997_Sparse_coding_with_an_overcomplete_basis_set_-_A_strategy_employed_by_V1.pdf;/Users/ilyes/Zotero/storage/N9IWHMNW/S0042698997001697.html},
  journal = {Vision Research},
  keywords = {Coding,Gabor-wavelet,Natural images,V1},
  number = {23}
}

@article{oord2018representation,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  month = jul,
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Oord_et_al_2018_Representation_Learning_with_Contrastive_Predictive_Coding.pdf;/Users/ilyes/Zotero/storage/W2YVNBC6/1807.html},
  journal = {arXiv:1807.03748 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{pajunen1996nonlinear,
  title = {Nonlinear {{Blind Source Separation}} by {{Self}}-{{Organizing Maps}}},
  booktitle = {In {{Proc}}. {{Int}}. {{Conf}}. on {{Neural Information Processing}}},
  author = {Pajunen, Petteri and Hyv{\"a}rinen, Aapo and Karhunen, Juha},
  year = {1996},
  pages = {1207--1210},
  abstract = {In neural blind source separation most approaches have considered the linear source separation problem where the input data consist of unknown linear mixtures of unknown independent source signals. The solution is a linear transformation which makes the output vector components statistically independent. More generally we can consider nonlinear mixtures of sources. Then we can try to separate the sources by constructing mappings that make the components of the output vectors independent. We show that such a mapping can be approximately realized using self-organizing maps with rectangular map topology. We apply these mappings to the separation of nonlinear mixtures of sub-Gaussian sources.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Pajunen_et_al_1996_Nonlinear_Blind_Source_Separation_by_Self-Organizing_Maps.pdf;/Users/ilyes/Zotero/storage/TBAFIC2L/summary.html}
}

@article{papadakis2014optimal,
  title = {Optimal {{Transport}} with {{Proximal Splitting}}},
  author = {Papadakis, N. and Peyr{\'e}, G. and Oudet, E.},
  year = {2014},
  month = jan,
  volume = {7},
  pages = {212--238},
  doi = {10.1137/130920058},
  abstract = {This article reviews the use of first order convex optimization schemes to solve the discretized dynamic optimal transport problem, initially proposed by Benamou and Brenier. We develop a staggered grid discretization that is well adapted to the computation of the \$L\^2\$ optimal transport geodesic between distributions defined on a uniform spatial grid. We show how proximal splitting schemes can be used to solve the resulting large scale convex optimization problem. A specific instantiation of this method on a centered grid corresponds to the initial algorithm developed by Benamou and Brenier. We also show how more general cost functions can be taken into account and how to extend the method to perform optimal transport on a Riemannian manifold.},
  file = {/Users/ilyes/Zotero/storage/54WBIRQQ/130920058.html},
  journal = {SIAM Journal on Imaging Sciences},
  number = {1}
}

@article{papamakarios2018masked,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year = {2018},
  month = jun,
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  archivePrefix = {arXiv},
  eprint = {1705.07057},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Papamakarios_et_al_2018_Masked_Autoregressive_Flow_for_Density_Estimation.pdf;/Users/ilyes/Zotero/storage/V2GK9ALW/1705.html},
  journal = {arXiv:1705.07057 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{paquette2017catalyst,
  title = {Catalyst {{Acceleration}} for {{Gradient}}-{{Based Non}}-{{Convex Optimization}}},
  author = {Paquette, Courtney and Lin, Hongzhou and Drusvyatskiy, Dmitriy and Mairal, Julien and Harchaoui, Zaid},
  year = {2017},
  month = mar,
  abstract = {We introduce a generic scheme to solve nonconvex optimization problems using gradient-based algorithms originally designed for minimizing convex functions. When the objective is convex, the proposed approach enjoys the same properties as the Catalyst approach of Lin et al. [22]. When the objective is nonconvex, it achieves the best known convergence rate to stationary points for first-order methods. Specifically, the proposed algorithm does not require knowledge about the convexity of the objective; yet, it obtains an overall worst-case efficiency of \$\textbackslash{}tilde\{O\}(\textbackslash{}varepsilon\^\{-2\})\$ and, if the function is convex, the complexity reduces to the near-optimal rate \$\textbackslash{}tilde\{O\}(\textbackslash{}varepsilon\^\{-2/3\})\$. We conclude the paper by showing promising experimental results obtained by applying the proposed approach to SVRG and SAGA for sparse matrix factorization and for learning neural networks.},
  archivePrefix = {arXiv},
  eprint = {1703.10993},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Paquette_et_al_2017_Catalyst_Acceleration_for_Gradient-Based_Non-Convex_Optimization.pdf;/Users/ilyes/Zotero/storage/42EERBPB/1703.html},
  journal = {arXiv:1703.10993 [math, stat]},
  keywords = {Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {math, stat}
}

@article{paul2014why,
  title = {Why Does {{Deep Learning}} Work? - {{A}} Perspective from {{Group Theory}}},
  shorttitle = {Why Does {{Deep Learning}} Work?},
  author = {Paul, Arnab and Venkatasubramanian, Suresh},
  year = {2014},
  month = dec,
  abstract = {Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning. One factor behind the recent resurgence of the subject is a key algorithmic step called pre-training: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of \{\textbackslash{}em shadow\} groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the \{\textbackslash{}em simplest\}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.},
  archivePrefix = {arXiv},
  eprint = {1412.6621},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Zotero/storage/CKLNKUJA/Paul_Venkatasubramanian_2014_Why_does_Deep_Learning_work.pdf;/Users/ilyes/Zotero/storage/ZTA3JABN/1412.html},
  journal = {arXiv:1412.6621 [cs, stat]},
  keywords = {Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{pearl2000causality,
  title = {Causality: Models, Reasoning and Inference},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  year = {2000},
  volume = {29},
  publisher = {{Springer}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Maths/Pearl_2000_Causality_-_models,_reasoning_and_inference.pdf;/Users/ilyes/Zotero/storage/9SBLVQB5/10.html}
}

@inproceedings{pele2009fast,
  title = {Fast and Robust {{Earth Mover}}'s {{Distances}}},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Pele, O. and Werman, M.},
  year = {2009},
  month = sep,
  pages = {460--467},
  doi = {10.1109/ICCV.2009.5459199},
  abstract = {We present a new algorithm for a robust family of Earth Mover's Distances - EMDs with thresholded ground distances. The algorithm transforms the flow-network of the EMD so that the number of edges is reduced by an order of magnitude. As a result, we compute the EMD by an order of magnitude faster than the original algorithm, which makes it possible to compute the EMD on large histograms and databases. In addition, we show that EMDs with thresholded ground distances have many desirable properties. First, they correspond to the way humans perceive distances. Second, they are robust to outlier noise and quantization effects. Third, they are metrics. Finally, experimental results on image retrieval show that thresholding the ground distance of the EMD improves both accuracy and speed.},
  file = {/Users/ilyes/Zotero/storage/U93ZH9B6/5459199.html},
  keywords = {computer vision,Costs,Earth,edge detection,flow-network,histograms,Humans,Image databases,Image edge detection,Image retrieval,outlier noise,Quantization,quantization effects,robust earth mover's distances,Robustness,thresholded ground distances}
}

@article{peters2014causal,
  title = {Causal {{Discovery}} with {{Continuous Additive Noise Models}}},
  author = {Peters, Jonas and Mooij, Joris and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2014},
  month = apr,
  abstract = {We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (Regression with Subsequent Independence Test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.},
  archivePrefix = {arXiv},
  eprint = {1309.6779},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Peters_et_al_2014_Causal_Discovery_with_Continuous_Additive_Noise_Models.pdf;/Users/ilyes/Zotero/storage/MFNWA8VG/1309.html},
  journal = {arXiv:1309.6779 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@book{peters2015causality,
  title = {Causality Script},
  author = {Peters, Jonas},
  year = {2015},
  file = {/nfs/ghome/live/ilyesk/google-drive/zotero/Causality/Peters_2015_Causality_script.pdf}
}

@article{peters2016causal,
  title = {Causal Inference by Using Invariant Prediction: Identification and Confidence Intervals},
  shorttitle = {Causal Inference by Using Invariant Prediction},
  author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
  year = {2016},
  volume = {78},
  pages = {947--1012},
  issn = {1467-9868},
  doi = {10.1111/rssb.12167},
  abstract = {What is the difference between a prediction that is made with a causal model and that with a non-causal model? Suppose that we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (e.g. various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
  copyright = {\textcopyright{} 2016 Royal Statistical Society},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Causality/Peters_et_al_2016_Causal_inference_by_using_invariant_prediction_-_identification_and_confidence.pdf;/Users/ilyes/Zotero/storage/8ESL4BRA/rssb.html},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Causal discovery,Causal inference,Confidence intervals,Invariant prediction},
  language = {en},
  number = {5}
}

@book{peters2017elements,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  shorttitle = {Elements of Causal Inference},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  publisher = {{MIT press}},
  file = {/Users/ilyes/Zotero/storage/XENTX43Z/books.html}
}

@book{peters2017elementsa,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  shorttitle = {Elements of Causal Inference},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  publisher = {{MIT press}},
  file = {/Users/ilyes/Zotero/storage/C4ZY2UP3/books.html}
}

@article{peyre2018computational,
  title = {Computational {{Optimal Transport}}},
  author = {Peyr{\'e}, Gabriel and Cuturi, Marco},
  year = {2018},
  month = mar,
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  archivePrefix = {arXiv},
  eprint = {1803.00567},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Peyré_Cuturi_2018_Computational_Optimal_Transport.pdf;/Users/ilyes/Zotero/storage/HG9VRG7W/1803.html},
  journal = {arXiv:1803.00567 [stat]},
  keywords = {Statistics - Machine Learning},
  primaryClass = {stat}
}

@article{pfister2019robustifying,
  title = {Robustifying Independent Component Analysis by Adjusting for Group-Wise Stationary Noise},
  author = {Pfister, Niklas and Weichwald, Sebastian and B{\"u}hlmann, Peter and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  volume = {20},
  pages = {1--50},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Pfister_et_al_2019_Robustifying_independent_component_analysis_by_adjusting_for_group-wise.pdf},
  journal = {Journal of Machine Learning Research},
  number = {147}
}

@article{pfister2019robustifyinga,
  title = {Robustifying {{Independent Component Analysis}} by {{Adjusting}} for {{Group}}-{{Wise Stationary Noise}}},
  author = {Pfister, Niklas and Weichwald, Sebastian and B{\"u}hlmann, Peter and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = oct,
  abstract = {We introduce coroICA, confounding-robust independent component analysis, a novel ICA algorithm which decomposes linearly mixed multivariate observations into independent components that are corrupted (and rendered dependent) by hidden group-wise stationary confounding. It extends the ordinary ICA model in a theoretically sound and explicit way to incorporate group-wise (or environment-wise) confounding. We show that our proposed general noise model allows to perform ICA in settings where other noisy ICA procedures fail. Additionally, it can be used for applications with grouped data by adjusting for different stationary noise within each group. Our proposed noise model has a natural relation to causality and we explain how it can be applied in the context of causal inference. In addition to our theoretical framework, we provide an efficient estimation procedure and prove identifiability of the unmixing matrix under mild assumptions. Finally, we illustrate the performance and robustness of our method on simulated data, provide audible and visual examples, and demonstrate the applicability to real-world scenarios by experiments on publicly available Antarctic ice core data as well as two EEG data sets. We provide a scikit-learn compatible pip-installable Python package coroICA as well as R and Matlab implementations accompanied by a documentation at https://sweichwald.de/coroICA/},
  archivePrefix = {arXiv},
  eprint = {1806.01094},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Pfister_et_al_2019_Robustifying_Independent_Component_Analysis_by_Adjusting_for_Group-Wise2.pdf;/Users/ilyes/Zotero/storage/7GBGT2FQ/1806.html},
  journal = {arXiv:1806.01094 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, q-bio, stat}
}

@article{pham2001blind,
  title = {Blind Separation of Instantaneous Mixtures of Nonstationary Sources},
  author = {Pham, Dinh-Tuan and Cardoso, J.-F.},
  year = {2001},
  month = sep,
  volume = {49},
  pages = {1837--1848},
  issn = {1941-0476},
  doi = {10.1109/78.942614},
  abstract = {Most source separation algorithms are based on a model of stationary sources. However, it is a simple matter to take advantage of possible nonstationarities of the sources to achieve separation. This paper develops novel approaches in this direction based on the principles of maximum likelihood and minimum mutual information. These principles are exploited by efficient algorithms in both the off-line case (via a new joint diagonalization procedure) and in the on-line case (via a Newton-like procedure). Some experiments showing the good performance of our algorithms and evidencing an interesting feature of our methods are presented: their ability to achieve a kind of super-efficiency. The paper concludes with a discussion contrasting separating methods for non-Gaussian and nonstationary models and emphasizing that, as a matter of fact, "what makes the algorithms work" is-strictly speaking-not the nonstationarity itself but rather the property that each realization of the source signals has a time-varying envelope.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Dinh-Tuan_Pham_Cardoso_2001_Blind_separation_of_instantaneous_mixtures_of_nonstationary_sources.pdf;/Users/ilyes/Zotero/storage/C8M34BMT/942614.html},
  journal = {IEEE Transactions on Signal Processing},
  keywords = {blind separation,Blind source separation,Computational modeling,Covariance matrix,Image reconstruction,Independent component analysis,instantaneous mixtures,joint diagonalization procedure,Laboratories,maximum likelihood,maximum likelihood estimation,minimum mutual information,Mutual information,Newton-like procedure,nonGaussian models,nonstationary models,nonstationary sources,off-line case,on-line case,Particle measurements,performance,signal processing,Source separation,source separation algorithms,super-efficiency,time-varying envelope,time-varying systems,Vectors},
  number = {9}
}

@article{pham2001blinda,
  title = {Blind Separation of Instantaneous Mixtures of Nonstationary Sources},
  author = {Pham, Dinh-Tuan and Cardoso, J.-F.},
  year = {2001},
  volume = {49},
  pages = {1837--1848},
  publisher = {{IEEE}},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Pham_Cardoso_2001_Blind_separation_of_instantaneous_mixtures_of_nonstationary_sources.pdf;/Users/ilyes/Zotero/storage/XXL9XAMH/942614.html},
  journal = {IEEE Transactions on signal processing},
  number = {9}
}

@article{plumbley2003algorithms,
  title = {Algorithms for Nonnegative Independent Component Analysis},
  author = {Plumbley, Mark D.},
  year = {2003},
  volume = {14},
  pages = {534--543},
  file = {/Users/ilyes/Zotero/storage/7BFEVMY5/1199651.html},
  journal = {IEEE Transactions on Neural Networks},
  number = {3}
}

@inproceedings{podosinnikova2015rethinking,
  title = {Rethinking {{LDA}}: {{Moment Matching}} for {{Discrete ICA}}},
  shorttitle = {Rethinking {{LDA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Podosinnikova, Anastasia and Bach, Francis and {Lacoste-Julien}, Simon},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {514--522},
  publisher = {{Curran Associates, Inc.}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Podosinnikova_et_al_2015_Rethinking_LDA_-_Moment_Matching_for_Discrete_ICA.pdf;/Users/ilyes/Zotero/storage/75SN7D7J/5671-rethinking-lda-moment-matching-for-discrete-ica.html}
}

@article{podosinnikova2019overcomplete,
  title = {Overcomplete {{Independent Component Analysis}} via {{SDP}}},
  author = {Podosinnikova, Anastasia and Perry, Amelia and Wein, Alexander and Bach, Francis and {d'Aspremont}, Alexandre and Sontag, David},
  year = {2019},
  month = jan,
  abstract = {We present a novel algorithm for overcomplete independent components analysis (ICA), where the number of latent sources k exceeds the dimension p of observed variables. Previous algorithms either suffer from high computational complexity or make strong assumptions about the form of the mixing matrix. Our algorithm does not make any sparsity assumption yet enjoys favorable computational and theoretical properties. Our algorithm consists of two main steps: (a) estimation of the Hessians of the cumulant generating function (as opposed to the fourth and higher order cumulants used by most algorithms) and (b) a novel semi-definite programming (SDP) relaxation for recovering a mixing component. We show that this relaxation can be efficiently solved with a projected accelerated gradient descent method, which makes the whole algorithm computationally practical. Moreover, we conjecture that the proposed program recovers a mixing component at the rate k {$<$} p\^2/4 and prove that a mixing component can be recovered with high probability when k {$<$} (2 - epsilon) p log p when the original components are sampled uniformly at random on the hyper sphere. Experiments are provided on synthetic data and the CIFAR-10 dataset of real images.},
  archivePrefix = {arXiv},
  eprint = {1901.08334},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Podosinnikova_et_al_2019_Overcomplete_Independent_Component_Analysis_via_SDP.pdf;/Users/ilyes/Zotero/storage/NJXL5HRD/1901.html},
  journal = {arXiv:1901.08334 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{poldrack2015longterm,
  title = {Long-Term Neural and Physiological Phenotyping of a Single Human},
  author = {Poldrack, Russell A. and Laumann, Timothy O. and Koyejo, Oluwasanmi and Gregory, Brenda and Hover, Ashleigh and Chen, Mei-Yen and Gorgolewski, Krzysztof J. and Luci, Jeffrey and Joo, Sung Jun and Boyd, Ryan L.},
  year = {2015},
  volume = {6},
  pages = {1--15},
  file = {/Users/ilyes/Zotero/storage/2X95RPZD/ncomms9885.html},
  journal = {Nature communications},
  number = {1}
}

@article{pu2017adversarial,
  title = {Adversarial {{Symmetric Variational Autoencoder}}},
  author = {Pu, Yunchen and Wang, Weiyao and Henao, Ricardo and Chen, Liqun and Gan, Zhe and Li, Chunyuan and Carin, Lawrence},
  year = {2017},
  month = nov,
  abstract = {A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (\$i\$) from observed data fed through the encoder to yield codes, and (\$ii\$) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (\$i\$) and (\$ii\$), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmark datasets.},
  archivePrefix = {arXiv},
  eprint = {1711.04915},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Pu_et_al_2017_Adversarial_Symmetric_Variational_Autoencoder.pdf;/Users/ilyes/Zotero/storage/3FJNHA9L/1711.html},
  journal = {arXiv:1711.04915 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{pu2017vae,
  title = {{{VAE Learning}} via {{Stein Variational Gradient Descent}}},
  author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Li, Chunyuan and Han, Shaobo and Carin, Lawrence},
  year = {2017},
  month = apr,
  abstract = {A new method for learning variational autoencoders (VAEs) is developed, based
on Stein variational gradient descent. A key advantage of this approach is that
one need not make parametric assumptions about the form of the encoder
distribution. Performance is further enhanced by integrating the proposed
encoder with importance sampling. Excellent performance is demonstrated across
multiple unsupervised and semi-supervised problems, including semi-supervised
analysis of the ImageNet data, demonstrating the scalability of the model to
large datasets.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Pu_et_al_2017_VAE_Learning_via_Stein_Variational_Gradient_Descent.pdf;/Users/ilyes/Zotero/storage/7YY7VXDW/1704.html},
  language = {en}
}

@misc{publications,
  title = {Publications by {{Aapo Hyvarinen}}: {{FastICA}}},
  file = {/Users/ilyes/Zotero/storage/3HSRI3IV/fastica.html},
  howpublished = {https://www.cs.helsinki.fi/u/ahyvarin/papers/fastica.shtml}
}

@misc{publicationsa,
  title = {Publications by {{Aapo Hyv{\"a}rinen}}: Nonlinear {{ICA}}},
  file = {/Users/ilyes/Zotero/storage/T3MZS84Q/udl.html},
  howpublished = {https://www.cs.helsinki.fi/u/ahyvarin/papers/udl.shtml},
  keywords = {NICA}
}

@article{radford2015unsupervised,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2015},
  month = nov,
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  archivePrefix = {arXiv},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Radford_et_al_2015_Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative.pdf;/Users/ilyes/Zotero/storage/RN8JF54Q/1511.html},
  journal = {arXiv:1511.06434 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning},
  primaryClass = {cs}
}

@article{ramdas2017wasserstein,
  title = {On {{Wasserstein Two}}-{{Sample Testing}} and {{Related Families}} of {{Nonparametric Tests}}},
  author = {Ramdas, Aaditya and Trillos, Nicol{\'a}s Garc{\'i}a and Cuturi, Marco},
  year = {2017},
  volume = {19},
  pages = {47},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Ramdas_et_al_2017_On_Wasserstein_Two-Sample_Testing_and_Related_Families_of_Nonparametric_Tests.pdf;/Users/ilyes/Zotero/storage/MBTUN2QK/htm.html},
  journal = {Entropy},
  number = {2}
}

@article{raphan2011least,
  title = {Least Squares Estimation without Priors or Supervision},
  author = {Raphan, Martin and Simoncelli, Eero P.},
  year = {2011},
  volume = {23},
  pages = {374--420},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Raphan_Simoncelli_2011_Least_squares_estimation_without_priors_or_supervision.pdf;/Users/ilyes/Zotero/storage/QFLFU6Z9/NECO_a_00076.html;/Users/ilyes/Zotero/storage/UZ7QHU89/NECO_a_00076.html},
  journal = {Neural computation},
  number = {2}
}

@article{rezende2014stochastic,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = jan,
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  archivePrefix = {arXiv},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Rezende_et_al_2014_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models.pdf;/Users/ilyes/Zotero/storage/RMYH2A4P/1401.html},
  journal = {arXiv:1401.4082 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{rezende2015variational,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2015},
  month = may,
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  archivePrefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Rezende_Mohamed_2015_Variational_Inference_with_Normalizing_Flows.pdf;/Users/ilyes/Zotero/storage/T87J5BGC/1505.html},
  journal = {arXiv:1505.05770 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@inproceedings{robbins1956empirical,
  title = {An {{Empirical Bayes Approach}} to {{Statistics}}},
  booktitle = {Proceedings of the {{Third Berkeley Symposium}} on {{Mathematical Statistics}} and {{Probability}}, {{Volume}} 1: {{Contributions}} to the {{Theory}} of {{Statistics}}},
  author = {Robbins, Herbert},
  year = {1956},
  publisher = {{The Regents of the University of California}},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Robbins_1956_An_Empirical_Bayes_Approach_to_Statistics.pdf;/Users/ilyes/Zotero/storage/W2VJ34X3/1200501653.html},
  language = {EN}
}

@article{rolinek2018variational,
  title = {Variational {{Autoencoders Pursue PCA Directions}} (by {{Accident}})},
  author = {Rolinek, Michal and Zietlow, Dominik and Martius, Georg},
  year = {2018},
  month = dec,
  abstract = {The Variational Autoencoder (VAE) is a powerful architecture capable of representation learning and generative modeling. When it comes to learning interpretable (disentangled) representations, VAE and its variants show unparalleled performance. However, the reasons for this are unclear, since a very particular alignment of the latent embedding is needed but the design of the VAE does not encourage it in any explicit way. We address this matter and offer the following explanation: the diagonal approximation in the encoder together with the inherent stochasticity force local orthogonality of the decoder. The local behavior of promoting both reconstruction and orthogonality matches closely how the PCA embedding is chosen. Alongside providing an intuitive understanding, we justify the statement with full theoretical analysis as well as with experiments.},
  archivePrefix = {arXiv},
  eprint = {1812.06775},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Rolinek_et_al_2018_Variational_Autoencoders_Pursue_PCA_Directions_(by_Accident).pdf;/Users/ilyes/Zotero/storage/NP4ANEP2/1812.html},
  journal = {arXiv:1812.06775 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{rothenhausler2018anchor,
  title = {Anchor Regression: Heterogeneous Data Meets Causality},
  shorttitle = {Anchor Regression},
  author = {Rothenh{\"a}usler, Dominik and Meinshausen, Nicolai and B{\"u}hlmann, Peter and Peters, Jonas},
  year = {2018},
  month = jan,
  abstract = {Estimating causal parameters from observational data is notoriously difficult. Popular approaches such as regression adjustment or the instrumental variables approach only work under relatively strong assumptions and are prone to mistakes. Furthermore, causal parameters can exhibit conservative predictive performance which can limit their usefulness in practice. Causal parameters can be written as the solution to a minimax risk problem, where the maximum is taken over a range of interventional (or perturbed) distributions. This motivates anchor regression, a method that makes use of exogeneous variables to solve a relaxation of the "causal" minimax problem. The procedure naturally provides an interpolation between the solution to ordinary least squares and two-stage least squares, but also has predictive guarantees if the instrumental variables assumptions are violated. We derive guarantees of the proposed procedure for predictive performance under perturbations for the population case and for high-dimensional data. An additional characterization of the procedure is given in terms of quantiles: If the data follow a Gaussian distribution, the method minimizes quantiles of the conditional mean squared error. If anchor regression and least squares provide the same answer ("anchor stability"), the relationship between targets and predictors is unconfounded and the coefficients have a causal interpretation. Furthermore, we show under which conditions anchor regression satisfies replicability among different experiments. Anchor regression is shown empirically to improve replicability and protect against distributional shifts},
  archivePrefix = {arXiv},
  eprint = {1801.06229},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Rothenhäusler_et_al_2018_Anchor_regression_-_heterogeneous_data_meets_causality.pdf;/Users/ilyes/Zotero/storage/4YJRI9V7/1801.html},
  journal = {arXiv:1801.06229 [stat]},
  keywords = {Statistics - Methodology},
  primaryClass = {stat}
}

@article{rothfuss2019conditional,
  title = {Conditional {{Density Estimation}} with {{Neural Networks}}: {{Best Practices}} and {{Benchmarks}}},
  shorttitle = {Conditional {{Density Estimation}} with {{Neural Networks}}},
  author = {Rothfuss, Jonas and Ferreira, Fabio and Walther, Simon and Ulrich, Maxim},
  year = {2019},
  month = apr,
  abstract = {Given a set of empirical observations, conditional density estimation aims to capture the statistical relationship between a conditional variable \$\textbackslash{}mathbf\{x\}\$ and a dependent variable \$\textbackslash{}mathbf\{y\}\$ by modeling their conditional probability \$p(\textbackslash{}mathbf\{y\}|\textbackslash{}mathbf\{x\})\$. The paper develops best practices for conditional density estimation for finance applications with neural networks, grounded on mathematical insights and empirical evaluations. In particular, we introduce a noise regularization and data normalization scheme, alleviating problems with over-fitting, initialization and hyper-parameter sensitivity of such estimators. We compare our proposed methodology with popular semi- and non-parametric density estimators, underpin its effectiveness in various benchmarks on simulated and Euro Stoxx 50 data and show its superior performance. Our methodology allows to obtain high-quality estimators for statistical expectations of higher moments, quantiles and non-linear return transformations, with very little assumptions about the return dynamic.},
  archivePrefix = {arXiv},
  eprint = {1903.00954},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Rothfuss_et_al_2019_Conditional_Density_Estimation_with_Neural_Networks_-_Best_Practices_and.pdf;/Users/ilyes/Zotero/storage/MT6NLWTI/1903.html},
  journal = {arXiv:1903.00954 [cs, q-fin, stat]},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - Computational Finance,Quantitative Finance - Statistical Finance,Statistics - Machine Learning},
  primaryClass = {cs, q-fin, stat}
}

@inproceedings{sakoe1971dynamic,
  title = {A Dynamic Programming Approach to Continuous Speech Recognition},
  booktitle = {Proceedings of the Seventh International Congress on Acoustics},
  author = {Sakoe, Hiroaki and Chiba, Seibi},
  year = {1971},
  volume = {3},
  pages = {65--69},
  publisher = {{Budapest, Hungary}},
  file = {/Users/ilyes/Zotero/storage/A8G5CFFU/scholar.html}
}

@article{sakoe1978dynamic,
  title = {Dynamic Programming Algorithm Optimization for Spoken Word Recognition},
  author = {Sakoe, Hiroaki and Chiba, Seibi},
  year = {1978},
  month = feb,
  volume = {26},
  pages = {43--49},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1978.1163055},
  abstract = {This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sakoe_Chiba_1978_Dynamic_programming_algorithm_optimization_for_spoken_word_recognition.pdf;/Users/ilyes/Zotero/storage/9J4HASBA/1163055.html},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  keywords = {Acoustics,Constraint optimization,Dynamic programming,Feature extraction,Fluctuations,Heuristic algorithms,Pattern matching,Signal processing algorithms,Speech processing,Timing},
  number = {1}
}

@book{santambrogio2015optimal,
  title = {Optimal Transport for Applied Mathematicians: Calculus of Variations, Pdes, and Modeling},
  shorttitle = {Optimal Transport for Applied Mathematicians},
  author = {Santambrogio, Filippo},
  year = {2015},
  publisher = {{Birkh{\"a}user}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Santambrogio_2015_Optimal_transport_for_applied_mathematicians_-_calculus_of_variations,_pdes,_and.pdf;/Users/ilyes/Zotero/storage/TVDZEV9Z/books.html},
  keywords = {_read_in_progress,otam}
}

@article{saremi2018deep,
  title = {Deep {{Energy Estimator Networks}}},
  author = {Saremi, Saeed and Mehrjou, Arash and Sch{\"o}lkopf, Bernhard and Hyv{\"a}rinen, Aapo},
  year = {2018},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Saremi_et_al_2018_Deep_Energy_Estimator_Networks.pdf;/Users/ilyes/Zotero/storage/AFYRGKDL/1805.html},
  journal = {arXiv preprint arXiv:1805.08306}
}

@article{saremi2019approximating,
  title = {On Approximating \$\textbackslash{}nabla F\$ with Neural Networks},
  author = {Saremi, Saeed},
  year = {2019},
  month = oct,
  abstract = {Consider a feedforward neural network \$\textbackslash{}psi: \textbackslash{}mathbb\{R\}\^d\textbackslash{}rightarrow \textbackslash{}mathbb\{R\}\^d\$ such that \$\textbackslash{}psi\textbackslash{}approx \textbackslash{}nabla f\$, where \$f:\textbackslash{}mathbb\{R\}\^d \textbackslash{}rightarrow \textbackslash{}mathbb\{R\}\$ is a smooth function, therefore \$\textbackslash{}psi\$ must satisfy \$\textbackslash{}partial\_j \textbackslash{}psi\_i = \textbackslash{}partial\_i \textbackslash{}psi\_j\$ pointwise. We prove a theorem that for any such \$\textbackslash{}psi\$ networks, and for any depth \$L{$>$}2\$, all the input weights must be parallel to each other. In other words, \$\textbackslash{}psi\$ can only represent \$one\$ feature in its first hidden layer. The proof of the theorem is straightforward, where two backward paths (from \$i\$ to \$j\$ and \$j\$ to \$i\$) and a weight-tying matrix (connecting the last and first hidden layers) play the key roles. We thus make a strong theoretical case in favor of the \$implicit\$ parametrization, where the neural network is \$\textbackslash{}phi: \textbackslash{}mathbb\{R\}\^d \textbackslash{}rightarrow \textbackslash{}mathbb\{R\}\$ and \$\textbackslash{}nabla \textbackslash{}phi \textbackslash{}approx \textbackslash{}nabla f\$. Throughout, we revisit two recent unnormalized probabilistic models that are formulated as \$\textbackslash{}psi \textbackslash{}approx \textbackslash{}nabla f\$ and also discuss the denoising autoencoders in the end.},
  archivePrefix = {arXiv},
  eprint = {1910.12744},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Saremi_2019_On_approximating_$-nabla_f$_with_neural_networks.pdf;/Users/ilyes/Zotero/storage/W67V8ZAM/1910.html},
  journal = {arXiv:1910.12744 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{saremi2019neural,
  title = {Neural {{Empirical Bayes}}},
  author = {Saremi, Saeed and Hyv{\"a}rinen, Aapo},
  year = {2019},
  month = mar,
  abstract = {We formulate a novel framework that unifies kernel density estimation and empirical Bayes, where we address a broad set of problems in unsupervised learning with a geometric interpretation rooted in the concentration of measure phenomenon. We start by energy estimation based on a denoising objective which recovers the original/clean data X from its measured/noisy version Y with empirical Bayes least squares estimator. The setup is rooted in kernel density estimation, but the log-pdf in Y is parametrized with a neural network, and crucially, the learning objective is derived for any level of noise/kernel bandwidth. Learning is efficient with double backpropagation and stochastic gradient descent. An elegant physical picture emerges of an interacting system of high-dimensional spheres around each data point, together with a globally-defined probability flow field. The picture is powerful: it leads to a novel sampling algorithm, a new notion of associative memory, and it is instrumental in designing experiments. We start with extreme denoising experiments. Walk-jump sampling is defined by Langevin MCMC walks in Y, along with asynchronous empirical Bayes jumps to X. Robbins associative memory is defined by a deterministic flow to attractors of the learned probability flow field. Finally, we observed the emergence of remarkably rich creative modes in the regime of highly overlapping spheres.},
  archivePrefix = {arXiv},
  eprint = {1903.02334},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Saremi_Hyvarinen_2019_Neural_Empirical_Bayes.pdf;/Users/ilyes/Zotero/storage/5U3L2M76/1903.html},
  journal = {arXiv:1903.02334 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sasaki2018neuralkernelized,
  title = {Neural-{{Kernelized Conditional Density Estimation}}},
  author = {Sasaki, Hiroaki and Hyv{\"a}rinen, Aapo},
  year = {2018},
  month = jun,
  abstract = {Conditional density estimation is a general framework for solving various problems in machine learning. Among existing methods, non-parametric and/or kernel-based methods are often difficult to use on large datasets, while methods based on neural networks usually make restrictive parametric assumptions on the probability densities. Here, we propose a novel method for estimating the conditional density based on score matching. In contrast to existing methods, we employ scalable neural networks, but do not make explicit parametric assumptions on densities. The key challenge in applying score matching to neural networks is computation of the first- and second-order derivatives of a model for the log-density. We tackle this challenge by developing a new neural-kernelized approach, which can be applied on large datasets with stochastic gradient descent, while the reproducing kernels allow for easy computation of the derivatives needed in score matching. We show that the neural-kernelized function approximator has universal approximation capability and that our method is consistent in conditional density estimation. We numerically demonstrate that our method is useful in high-dimensional conditional density estimation, and compares favourably with existing methods. Finally, we prove that the proposed method has interesting connections to two probabilistically principled frameworks of representation learning: Nonlinear sufficient dimension reduction and nonlinear independent component analysis.},
  archivePrefix = {arXiv},
  eprint = {1806.01754},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Saremi_et_al_2018_Deep_Energy_Estimator_Networks.pdf;/Users/ilyes/Zotero/storage/CA25RIJG/1806.html},
  journal = {arXiv:1806.01754 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{schmitzer2016sparse,
  title = {A {{Sparse Multiscale Algorithm}} for {{Dense Optimal Transport}}},
  author = {Schmitzer, Bernhard},
  year = {2016},
  month = oct,
  volume = {56},
  pages = {238--259},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-016-0653-9},
  abstract = {Discrete optimal transport solvers do not scale well on dense large problems since they do not explicitly exploit the geometric structure of the cost function. In analogy to continuous optimal transport, we provide a framework to verify global optimality of a discrete transport plan locally. This allows the construction of an algorithm to solve large dense problems by considering a sequence of sparse problems instead. The algorithm lends itself to being combined with a hierarchical multiscale scheme. Any existing discrete solver can be used as internal black-box. We explicitly describe how to select the sparse sub-problems for several cost functions, including the noisy squared Euclidean distance. Significant reductions in run-time and memory requirements have been observed.},
  file = {/Users/ilyes/Zotero/storage/EBQ52WIZ/s10851-016-0653-9.html},
  journal = {Journal of Mathematical Imaging and Vision},
  language = {en},
  number = {2}
}

@article{schmitzer2016stabilized,
  title = {Stabilized {{Sparse Scaling Algorithms}} for {{Entropy Regularized Transport Problems}}},
  author = {Schmitzer, Bernhard},
  year = {2016},
  month = oct,
  abstract = {Scaling algorithms for entropic transport-type problems have become a very popular numerical method, encompassing Wasserstein barycenters, multi-marginal problems, gradient flows and unbalanced transport. However, a standard implementation of the scaling algorithm has several numerical limitations: the scaling factors diverge and convergence becomes impractically slow as the entropy regularization approaches zero. Moreover, handling the dense kernel matrix becomes unfeasible for large problems. To address this, we propose several modifications: A log-domain stabilized formulation, the well-known epsilon-scaling heuristic, an adaptive truncation of the kernel and a coarse-to-fine scheme. This allows to solve larger problems with smaller regularization and negligible truncation error. A new convergence analysis of the Sinkhorn algorithm is developed, working towards a better understanding of epsilon-scaling. Numerical examples illustrate efficiency and versatility of the modified algorithm.},
  archivePrefix = {arXiv},
  eprint = {1610.06519},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Zotero/storage/GM895QZ7/1610.html},
  journal = {arXiv:1610.06519 [cs, math]},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@article{shadlen1996computational,
  title = {A Computational Analysis of the Relationship between Neuronal and Behavioral Responses to Visual Motion},
  author = {Shadlen, M. N. and Britten, K. H. and Newsome, W. T. and Movshon, J. A.},
  year = {1996},
  month = feb,
  volume = {16},
  pages = {1486--1510},
  issn = {0270-6474, 1529-2401},
  abstract = {We have documented previously a close relationship between neuronal activity in the middle temporal visual area (MT or V5) and behavioral judgments of motion (Newsome et al., 1989; Salzman et al., 1990; Britten et al., 1992; Britten et al., 1996). We have now used numerical simulations to try to understand how neural signals in area MT support psychophysical decisions. We developed a model that pools neuronal responses drawn from our physiological data set and compares average responses in different pools to produce psychophysical decisions. The structure of the model allows us to assess the relationship between ``neuronal'' input signals and simulated psychophysical performance using the same methods we have applied to real experimental data. We sought to reconcile three experimental observations: psychophysical performance (threshold sensitivity to motion stimuli embedded in noise), a trial-by-trial covariation between the neural response and the monkey's choices, and a modest correlation between pairs of MT neurons in their variable responses to identical visual stimuli. Our results can be most accurately simulated if psychophysical decisions are based on pools of at least 100 weakly correlated sensory neurons. The neurons composing the pools must include a broader range of sensitivities than we encountered in our MT recordings, presumably because of the inclusion of neurons whose optimal stimulus is different from the one being discriminated. Central sources of noise degrade the signal-to-noise ratio of the pooled signal, but this degradation is relatively small compared with the noise typically carried by single cortical neurons. This suggests that our monkeys base near-threshold psychophysical judgments on signals carried by populations of weakly interacting neurons; these populations include many neurons that are not tuned optimally for the particular stimuli being discriminated.},
  copyright = {\textcopyright{} 1996 by Society for Neuroscience},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Shadlen_et_al_1996_A_computational_analysis_of_the_relationship_between_neuronal_and_behavioral.pdf;/Users/ilyes/Zotero/storage/8F3JITDW/1486.html;/Users/ilyes/Zotero/storage/J5W4HVQQ/1486.html},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {4},
  pmid = {8778300}
}

@article{shimizu2006linear,
  title = {A {{Linear Non}}-{{Gaussian Acyclic Model}} for {{Causal Discovery}}},
  author = {Shimizu, Shohei and Hoyer, Patrik O. and Hyv{\"a}rinen, Aapo and Kerminen, Antti},
  year = {2006},
  volume = {7},
  pages = {2003--2030},
  issn = {ISSN 1533-7928},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Shimizu_et_al_2006_A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.pdf;/Users/ilyes/Zotero/storage/KEIYBHTJ/shimizu06a.html},
  journal = {Journal of Machine Learning Research},
  number = {Oct}
}

@article{shimizu2006lineara,
  title = {A Linear Non-{{Gaussian}} Acyclic Model for Causal Discovery},
  author = {Shimizu, Shohei and Hoyer, Patrik O. and Hyv{\"a}rinen, Aapo and Kerminen, Antti},
  year = {2006},
  volume = {7},
  pages = {2003--2030},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Shimizu_et_al_2006_A_linear_non-Gaussian_acyclic_model_for_causal_discovery.pdf;/Users/ilyes/Zotero/storage/W6PSBAFN/shimizu06a.html},
  journal = {Journal of Machine Learning Research},
  number = {Oct}
}

@article{shimizu2011directlingam,
  title = {{{DirectLiNGAM}}: {{A}} Direct Method for Learning a Linear Non-{{Gaussian}} Structural Equation Model},
  shorttitle = {{{DirectLiNGAM}}},
  author = {Shimizu, Shohei and Inazumi, Takanori and Sogawa, Yasuhiro and Hyv{\"a}rinen, Aapo and Kawahara, Yoshinobu and Washio, Takashi and Hoyer, Patrik O. and Bollen, Kenneth},
  year = {2011},
  volume = {12},
  pages = {1225--1248},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Shimizu_et_al_2011_DirectLiNGAM_-_A_direct_method_for_learning_a_linear_non-Gaussian_structural.pdf;/Users/ilyes/Zotero/storage/SJXGIH3I/shimizu11a.pdf;/Users/ilyes/Zotero/storage/D3UN6ZHI/shimizu11a.html},
  journal = {Journal of Machine Learning Research},
  number = {Apr}
}

@article{silverman1978weak,
  title = {Weak and {{Strong Uniform Consistency}} of the {{Kernel Estimate}} of a {{Density}} and Its {{Derivatives}}},
  author = {Silverman, Bernard W.},
  year = {1978},
  month = jan,
  volume = {6},
  pages = {177--184},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176344076},
  abstract = {The estimation of a density and its derivatives by the kernel method is considered. Uniform consistency properties over the whole real line are studied. For suitable kernels and uniformly continuous densities it is shown that the conditions h\textrightarrow{}0h\textrightarrow{}0h \textbackslash{}rightarrow 0 and (nh)-1logn\textrightarrow{}0(nh)-1log⁡n\textrightarrow{}0(nh)\^\{-1\} \textbackslash{}log n \textbackslash{}rightarrow 0 are sufficient for strong uniform consistency of the density estimate, where nnn is the sample size and hhh is the "window width." Under certain conditions on the kernel, conditions are found on the density and on the behavior of the window width which are necessary and sufficient for weak and strong uniform consistency of the estimate of the density derivatives. Theorems on the rate of strong and weak consistency are also proved.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Silverman_1978_Weak_and_Strong_Uniform_Consistency_of_the_Kernel_Estimate_of_a_Density_and_its.pdf;/Users/ilyes/Zotero/storage/K6N7LLBI/1176344076.html},
  journal = {The Annals of Statistics},
  keywords = {density derivative estimate,Density estimate,Gaussian process,global consistency,kernel,modulus of continuity,rates of convergence,supremum over real line},
  language = {EN},
  mrnumber = {MR471166},
  number = {1},
  zmnumber = {0376.62024}
}

@article{singh2019kernel,
  title = {Kernel {{Instrumental Variable Regression}}},
  author = {Singh, Rahul and Sahani, Maneesh and Gretton, Arthur},
  year = {2019},
  month = jun,
  abstract = {Instrumental variable regression is a strategy for learning causal relationships in observational data. If measurements of input X and output Y are confounded, the causal relationship can nonetheless be identified if an instrumental variable Z is available that influences X directly, but is conditionally independent of Y given X. The classic two-stage least squares algorithm (2SLS) simplifies the estimation problem by modeling all relationships as linear functions. We propose kernel instrumental variable regression (KIV), a nonparametric generalization of 2SLS, modeling relations among X, Y, and Z as nonlinear functions in reproducing kernel Hilbert spaces (RKHSs). We prove the consistency of KIV under mild assumptions, and derive conditions under which the convergence rate achieves the minimax optimal rate for unconfounded, one-stage RKHS regression. In doing so, we obtain an efficient ratio between training sample sizes used in the algorithm's first and second stages. In experiments, KIV outperforms state of the art alternatives for nonparametric instrumental variable regression.},
  archivePrefix = {arXiv},
  eprint = {1906.00232},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Singh_et_al_2019_Kernel_Instrumental_Variable_Regression.pdf;/Users/ilyes/Zotero/storage/894D3RGH/1906.html},
  journal = {arXiv:1906.00232 [cs, econ, math, stat]},
  keywords = {62,Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Functional Analysis,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, econ, math, stat}
}

@article{singh2019kernela,
  title = {Kernel {{Instrumental Variable Regression}}},
  author = {Singh, Rahul and Sahani, Maneesh and Gretton, Arthur},
  year = {2019},
  month = jun,
  abstract = {Instrumental variable regression is a strategy for learning causal relationships in observational data. If measurements of input X and output Y are confounded, the causal relationship can nonetheless be identified if an instrumental variable Z is available that influences X directly, but is conditionally independent of Y given X. The classic two-stage least squares algorithm (2SLS) simplifies the estimation problem by modeling all relationships as linear functions. We propose kernel instrumental variable regression (KIV), a nonparametric generalization of 2SLS, modeling relations among X, Y, and Z as nonlinear functions in reproducing kernel Hilbert spaces (RKHSs). We prove the consistency of KIV under mild assumptions, and derive conditions under which the convergence rate achieves the minimax optimal rate for unconfounded, one-stage RKHS regression. In doing so, we obtain an efficient ratio between training sample sizes used in the algorithm's first and second stages. In experiments, KIV outperforms state of the art alternatives for nonparametric instrumental variable regression.},
  archivePrefix = {arXiv},
  eprint = {1906.00232},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Singh_et_al_2019_Kernel_Instrumental_Variable_Regression.pdf;/Users/ilyes/Zotero/storage/NLVINB5Q/1906.html},
  journal = {arXiv:1906.00232 [cs, econ, math, stat]},
  keywords = {62,Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Functional Analysis,Mathematics - Statistics Theory,Statistics - Machine Learning},
  primaryClass = {cs, econ, math, stat}
}

@article{sinkhorn1964relationship,
  title = {A {{Relationship Between Arbitrary Positive Matrices}} and {{Doubly Stochastic Matrices}}},
  author = {Sinkhorn, Richard},
  year = {1964},
  volume = {35},
  pages = {876--879},
  issn = {0003-4851},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sinkhorn_1964_A_Relationship_Between_Arbitrary_Positive_Matrices_and_Doubly_Stochastic.pdf},
  journal = {The Annals of Mathematical Statistics},
  number = {2}
}

@article{sinkhorn1967diagonal,
  title = {Diagonal {{Equivalence}} to {{Matrices}} with {{Prescribed Row}} and {{Column Sums}}},
  author = {Sinkhorn, Richard},
  year = {1967},
  volume = {74},
  pages = {402--405},
  issn = {0002-9890},
  doi = {10.2307/2314570},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sinkhorn_1967_Diagonal_Equivalence_to_Matrices_with_Prescribed_Row_and_Column_Sums.pdf},
  journal = {The American Mathematical Monthly},
  number = {4}
}

@article{sinz2009characterization,
  title = {Characterization of the P-Generalized Normal Distribution},
  author = {Sinz, Fabian and Gerwinn, Sebastian and Bethge, Matthias},
  year = {2009},
  month = may,
  volume = {100},
  pages = {817--820},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2008.07.006},
  abstract = {It is a well known fact that invariance under the orthogonal group and marginal independence uniquely characterizes the isotropic normal distribution. Here, a similar characterization is provided for the more general class of differentiable bounded Lp-spherically symmetric distributions: Every factorial distribution in this class is necessarily p-generalized normal.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Sinz_et_al_2009_Characterization_of_the_p-generalized_normal_distribution.pdf;/Users/ilyes/Zotero/storage/SG42WLSM/S0047259X08001681.html},
  journal = {Journal of Multivariate Analysis},
  keywords = {-spherically symmetric distributions,62E10,Characterization,Exponential power distribution,Generalized normal distribution,mljc,teatalk},
  number = {5}
}

@article{smith2014grouppca,
  title = {Group-{{PCA}} for Very Large {{fMRI}} Datasets},
  author = {Smith, Stephen M. and Hyv{\"a}rinen, Aapo and Varoquaux, Ga{\"e}l and Miller, Karla L. and Beckmann, Christian F.},
  year = {2014},
  month = nov,
  volume = {101},
  pages = {738--749},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2014.07.051},
  abstract = {Increasingly-large datasets (for example, the resting-state fMRI data from the Human Connectome Project) are demanding analyses that are problematic because of the sheer scale of the aggregate data. We present two approaches for applying group-level PCA; both give a close approximation to the output of PCA applied to full concatenation of all individual datasets, while having very low memory requirements regardless of the number of datasets being combined. Across a range of realistic simulations, we find that in most situations, both methods are more accurate than current popular approaches for analysis of multi-subject resting-state fMRI studies. The group-PCA output can be used to feed into a range of further analyses that are then rendered practical, such as the estimation of group-averaged voxelwise connectivity, group-level parcellation, and group-ICA.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Smith_et_al_2014_Group-PCA_for_very_large_fMRI_datasets.pdf;/Users/ilyes/Zotero/storage/MENMRIJ9/S105381191400634X.html},
  journal = {NeuroImage},
  keywords = {Big data,fMRI,ICA,PCA}
}

@article{solomon2015convolutional,
  title = {Convolutional {{Wasserstein Distances}}: {{Efficient Optimal Transportation}} on {{Geometric Domains}}},
  shorttitle = {Convolutional {{Wasserstein Distances}}},
  author = {Solomon, Justin and {de Goes}, Fernando and Peyr{\'e}, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
  year = {2015},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Solomon_et_al_2015_Convolutional_Wasserstein_Distances_-_Efficient_Optimal_Transportation_on.pdf;/Users/ilyes/Zotero/storage/H724RA73/citation.html},
  journal = {ACM Transactions on Graphics (TOG) SIGGRAPH, 2015}
}

@article{song2019generative,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  author = {Song, Yang and Ermon, Stefano},
  year = {2019},
  month = jul,
  abstract = {We introduce a new generative model where samples are produced via Langevin
dynamics using gradients of the data distribution estimated with score
matching. Because gradients can be ill-defined and hard to estimate when the
data resides on low-dimensional manifolds, we perturb the data with different
levels of Gaussian noise, and jointly estimate the corresponding scores, i.e.,
the vector fields of gradients of the perturbed data distribution for all noise
levels. For sampling, we propose an annealed Langevin dynamics where we use
gradients corresponding to gradually decreasing noise levels as the sampling
process gets closer to the data manifold. Our framework allows flexible model
architectures, requires no sampling during training or the use of adversarial
methods, and provides a learning objective that can be used for principled
model comparisons. Our models produce samples comparable to GANs on MNIST,
CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score
of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn
effective representations via image inpainting experiments.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Song_Ermon_2019_Generative_Modeling_by_Estimating_Gradients_of_the_Data_Distribution.pdf;/Users/ilyes/Zotero/storage/R32R6P5K/1907.html},
  language = {en}
}

@article{song2019sliced,
  title = {Sliced {{Score Matching}}: {{A Scalable Approach}} to {{Density}} and {{Score Estimation}}},
  shorttitle = {Sliced {{Score Matching}}},
  author = {Song, Yang and Garg, Sahaj and Shi, Jiaxin and Ermon, Stefano},
  year = {2019},
  month = jun,
  abstract = {Score matching is a popular method for estimating unnormalized statistical models. However, it has been so far limited to simple, shallow models or low-dimensional data, due to the difficulty of computing the Hessian of log-density functions. We show this difficulty can be mitigated by projecting the scores onto random vectors before comparing them. This objective, called sliced score matching, only involves Hessian-vector products, which can be easily implemented using reverse-mode automatic differentiation. Therefore, sliced score matching is amenable to more complex models and higher dimensional data compared to score matching. Theoretically, we prove the consistency and asymptotic normality of sliced score matching estimators. Moreover, we demonstrate that sliced score matching can be used to learn deep score estimators for implicit distributions. In our experiments, we show sliced score matching can learn deep energy-based models effectively, and can produce accurate score estimates for applications such as variational inference with implicit distributions and training Wasserstein Auto-Encoders.},
  archivePrefix = {arXiv},
  eprint = {1905.07088},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Song_et_al_2019_Sliced_Score_Matching_-_A_Scalable_Approach_to_Density_and_Score_Estimation.pdf;/Users/ilyes/Zotero/storage/AGINKQ5M/1905.html},
  journal = {arXiv:1905.07088 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sorrenson2020disentanglement,
  title = {Disentanglement by {{Nonlinear ICA}} with {{General Incompressible}}-Flow {{Networks}} ({{GIN}})},
  author = {Sorrenson, Peter and Rother, Carsten and K{\"o}the, Ullrich},
  year = {2020},
  month = jan,
  abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
  archivePrefix = {arXiv},
  eprint = {2001.04872},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Sorrenson_et_al_2020_Disentanglement_by_Nonlinear_ICA_with_General_Incompressible-flow_Networks_(GIN).pdf;/Users/ilyes/Zotero/storage/5336X9LJ/2001.html},
  journal = {arXiv:2001.04872 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sriperumbudur2011universality,
  title = {Universality, Characteristic Kernels and {{RKHS}} Embedding of Measures},
  author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Lanckriet, Gert RG},
  year = {2011},
  volume = {12},
  pages = {2389--2410},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sriperumbudur_et_al_2011_Universality,_characteristic_kernels_and_RKHS_embedding_of_measures.pdf;/Users/ilyes/Zotero/storage/4596P7VL/sriperumbudur11a.html},
  journal = {Journal of Machine Learning Research},
  number = {Jul}
}

@article{sriperumbudur2017density,
  title = {Density {{Estimation}} in {{Infinite Dimensional Exponential Families}}},
  author = {Sriperumbudur, Bharath and Fukumizu, Kenji and Gretton, Arthur and Hyv{\"a}rinen, Aapo and Kumar, Revant},
  year = {2017},
  volume = {18},
  pages = {1--59},
  issn = {1533-7928},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sriperumbudur_et_al_2017_Density_Estimation_in_Infinite_Dimensional_Exponential_Families.pdf;/Users/ilyes/Zotero/storage/MFZNZ2KF/16-011.html},
  journal = {Journal of Machine Learning Research},
  number = {57}
}

@book{strichartz2000way,
  title = {The {{Way}} of {{Analysis}}},
  author = {Strichartz, Robert S.},
  year = {2000},
  publisher = {{Jones \& Bartlett Learning}},
  abstract = {The Way of Analysis gives a thorough account of real analysis in one or several variables, from the construction of the real number system to an introduction of the Lebesgue integral. The text provides proofs of all main results, as well as motivations, examples, applications, exercises, and formal chapter summaries. Additionally, there are three chapters on application of analysis, ordinary differential equations, Fourier series, and curves and surfaces to show how the techniques of analysis are used in concrete settings.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Strichartz_2000_The_Way_of_Analysis.pdf},
  googlebooks = {Yix09oVvI1IC},
  isbn = {978-0-7637-1497-0},
  keywords = {Mathematics / Mathematical Analysis},
  language = {en}
}

@book{strichartz2003guide,
  title = {A Guide to Distribution Theory and {{Fourier}} Transforms},
  author = {Strichartz, Robert S.},
  year = {2003},
  publisher = {{World Scientific Publishing Company}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Strichartz_2003_A_guide_to_distribution_theory_and_Fourier_transforms.pdf;/Users/ilyes/Zotero/storage/FUDRZ8QB/books.html}
}

@article{stuhmer2018isavae,
  title = {{{ISA}}-{{VAE}}: {{Independent Subspace Analysis}} with {{Variational Autoencoders}}},
  shorttitle = {{{ISA}}-{{VAE}}},
  author = {St{\"u}hmer, Jan and Turner, Richard and Nowozin, Sebastian},
  year = {2018},
  month = sep,
  abstract = {Recent work has shown increased interest in using the Variational Autoencoder (VAE) framework to discover interpretable representations of data in an unsupervised way. These methods have focussed...},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Stühmer_et_al_2018_ISA-VAE_-_Independent_Subspace_Analysis_with_Variational_Autoencoders.pdf;/Users/ilyes/Zotero/storage/5DHD7EKX/forum.html}
}

@article{sugiyama2010leastsquares,
  title = {Least-Squares Conditional Density Estimation},
  author = {Sugiyama, Masashi and Takeuchi, Ichiro and Suzuki, Taiji and Kanamori, Takafumi and Hachiya, Hirotaka and Okanohara, Daisuke},
  year = {2010},
  volume = {93},
  pages = {583--594},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sugiyama_et_al_2010_Least-squares_conditional_density_estimation.pdf;/Users/ilyes/Zotero/storage/LGY8ALSM/summary.html},
  journal = {IEICE Transactions on Information and Systems},
  number = {3}
}

@article{suorderpreserving,
  title = {Order-Preserving {{Wasserstein Distance}} for {{Sequence Matching}}},
  author = {Su, Bing and Hua, Gang},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Su_Hua_Order-preserving_Wasserstein_Distance_for_Sequence_Matching.pdf}
}

@book{sutton2018reinforcement,
  title = {Reinforcement Learning: {{An}} Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  volume = {1},
  publisher = {{MIT press Cambridge}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Sutton_Barto_2018_Reinforcement_learning_-_An_introduction.pdf},
  note = {DRAFT},
  number = {1}
}

@article{teshima2020fewshot,
  title = {Few-Shot {{Domain Adaptation}} by {{Causal Mechanism Transfer}}},
  author = {Teshima, Takeshi and Sato, Issei and Sugiyama, Masashi},
  year = {2020},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Teshima_et_al_2020_Few-shot_Domain_Adaptation_by_Causal_Mechanism_Transfer.pdf;/Users/ilyes/Zotero/storage/SV6XWUEU/2002.html},
  journal = {arXiv preprint arXiv:2002.03497}
}

@article{thetheanodevelopmentteam2016theano,
  title = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  shorttitle = {Theano},
  author = {The Theano Development Team and {Al-Rfou}, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'e}d{\'e}ric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and {Boulanger-Lewandowski}, Nicolas and Bouthillier, Xavier and {de Br{\'e}bisson}, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and C{\^o}t{\'e}, Marc-Alexandre and C{\^o}t{\'e}, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, M{\'e}lanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Bal{\'a}zs and Honari, Sina and Jain, Arjun and Jean, S{\'e}bastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, C{\'e}sar and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and L{\'e}onard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and {van Merri{\"e}nboer}, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, Fran{\c c}ois and Schl{\"u}ter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, {\'E}tienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, J{\'e}r{\'e}mie and {van Tulder}, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and {de Vries}, Harm and {Warde-Farley}, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
  year = {2016},
  month = may,
  abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
  archivePrefix = {arXiv},
  eprint = {1605.02688},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Zotero/storage/BCUXES3C/The_Theano_Development_Team_et_al_2016_Theano.pdf;/Users/ilyes/Zotero/storage/33ARWVJX/1605.html},
  journal = {arXiv:1605.02688 [cs]},
  keywords = {Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation},
  primaryClass = {cs}
}

@article{tishby2000information,
  title = {The Information Bottleneck Method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {2000},
  month = apr,
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Tishby_et_al_2000_The_information_bottleneck_method.pdf;/Users/ilyes/Zotero/storage/UIZK7RTX/0004057.html},
  keywords = {IB,Information Bottleneck},
  language = {en}
}

@article{tolstikhin2018wasserstein,
  title = {Wasserstein {{Auto}}-{{Encoders}}},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  year = {2018},
  month = feb,
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the...},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Tolstikhin_et_al_2018_Wasserstein_Auto-Encoders.pdf;/Users/ilyes/Zotero/storage/NBX3QPWD/forum.html}
}

@inproceedings{trouve2000diffeomorphic,
  title = {Diffeomorphic {{Matching Problems}} in {{One Dimension}}: {{Designing}} and {{Minimizing Matching Functionals}}},
  shorttitle = {Diffeomorphic {{Matching Problems}} in {{One Dimension}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2000},
  author = {Trouv{\'e}, Alain and Younes, Laurent},
  year = {2000},
  month = jun,
  pages = {573--587},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-45054-8_37},
  abstract = {This paper focuses on matching 1D structures by variational methods. We provide rigorous rules for the construction of the cost function, on the basis of an analysis of properties which should be satisfied by the optimal matching. A new, exact, dynamic programming algorithm is then designed for the minimization. We conclude with experimental results on shape comparison.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Trouvé_Younes_2000_Diffeomorphic_Matching_Problems_in_One_Dimension_-_Designing_and_Minimizing.pdf;/Users/ilyes/Zotero/storage/PPIEVGRX/10.html},
  isbn = {978-3-540-67685-0 978-3-540-45054-2},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{tsochantaridis2005large,
  title = {Large {{Margin Methods}} for {{Structured}} and {{Interdependent Output Variables}}},
  author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
  year = {2005},
  volume = {6},
  pages = {1453--1484},
  issn = {ISSN 1533-7928},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Tsochantaridis_et_al_2005_Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.pdf;/Users/ilyes/Zotero/storage/DWTSAA7Q/tsochantaridis05a.html},
  journal = {Journal of Machine Learning Research},
  number = {Sep}
}

@article{tucker2018doubly,
  title = {Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},
  author = {Tucker, George and Lawson, Dieterich and Gu, Shixiang and Maddison, Chris J.},
  year = {2018},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Tucker_et_al_2018_Doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives.pdf;/Users/ilyes/Zotero/storage/LSSIPIYB/1810.html},
  journal = {arXiv preprint arXiv:1810.04152}
}

@book{tuckwell1988introduction,
  title = {Introduction to {{Theoretical Neurobiology}}: {{Volume}} 1, {{Linear Cable Theory}} and {{Dendritic Structure}}},
  shorttitle = {Introduction to {{Theoretical Neurobiology}}},
  author = {Tuckwell, Henry C.},
  year = {1988},
  month = apr,
  publisher = {{Cambridge University Press}},
  abstract = {The human brain contains billions of nerve cells whose activity plays a critical role in the way we behave, feel, perceive, and think. This two-volume set explains the basic properties of a neuron--an electrically active nerve cell--and develops mathematical theories for the way neurons respond to the various stimuli they receive. Volume 1 contains descriptions and analyses of the principle mathematical models that have been developed for neurons in the past thirty years. It provides a brief review of the basic neuroanatomical and neurophysiological facts that will form the focus of the mathematical treatment. Tuckwell discusses the mathematical theories, beginning with the theory of membrane potentials. He then goes on to treat the Lapicque model, linear cable theory, and time-dependent solutions of the cable equations. He concludes with a description of Rall's model nerve cell. Because the level of mathematics increases steadily upward from Chapter Two some familiarity with differential equations and linear algebra is desirable.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Tuckwell_1988_Introduction_to_Theoretical_Neurobiology_-_Volume_1,_Linear_Cable_Theory_and.pdf},
  googlebooks = {XawuXjH14\_UC},
  isbn = {978-0-521-35096-9},
  keywords = {Mathematics / Applied,Medical / Physiology,Science / Life Sciences / Neuroscience},
  language = {en}
}

@book{tuckwell2005introduction,
  title = {Introduction to {{Theoretical Neurobiology}}: {{Volume}} 2, {{Nonlinear}} and {{Stochastic Theories}}},
  shorttitle = {Introduction to {{Theoretical Neurobiology}}},
  author = {Tuckwell, Henry C.},
  year = {2005},
  month = sep,
  publisher = {{Cambridge University Press}},
  abstract = {The second part of this two-volume set contains advanced aspects of the quantitative theory of the dynamics of neurons. It begins with an introduction to the effects of reversal potentials on response to synaptic input. It then develops the theory of action potential generation based on the seminal Hodgkin-Huxley equations and gives methods for their solution in the space-clamped and nonspaceclamped cases. The remainder of the book discusses stochastic models of neural activity and ends with a statistical analysis of neuronal data with emphasis on spike trains. The mathematics is more complex in this volume than in the first volume and involves numerical methods of solution of partial differential equations and the statistical analysis of point processes.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Tuckwell_2005_Introduction_to_Theoretical_Neurobiology_-_Volume_2,_Nonlinear_and_Stochastic.pdf},
  googlebooks = {CO\_SM44p5NkC},
  isbn = {978-0-521-01932-3},
  keywords = {Mathematics / Applied,Science / Life Sciences / Biology,Science / Life Sciences / Neuroscience},
  language = {en}
}

@article{turkin2016benchmarking,
  title = {Benchmarking {{Python Tools}} for {{Automatic Differentiation}}},
  author = {Turkin, Andrei and Thu, Aung},
  year = {2016},
  volume = {4},
  issn = {2307-8162},
  abstract = {In this paper, we compare several Python tools for automatic differentiation. In order to assess the difference in performance and their precision, the problem of finding the optimal geometrical structure of a cluster of identical atoms is used as follows. First, we compare the performance of calculating gradients for the objective function. We showed that the PyADOL-C and PyCppAD tools have much better performance for big clusters than the other ones. Second, we assess the precision of these two tools by calculating the difference between the obtained at the optimal configuration gradient norms. We conclude that PyCppAD has the best performance among others, while having almost the same precision as the second-best performing tool PyADOL-C.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Turkin_Thu_2016_Benchmarking_Python_Tools_for_Automatic_Differentiation.pdf;/Users/ilyes/Zotero/storage/UCVUX2V9/benchmarking-python-tools-for-automatic-differentiation.html},
  journal = {International Journal of Open Information Technologies},
  number = {9}
}

@article{unterthiner2017coulomb,
  title = {Coulomb {{GANs}}: {{Provably Optimal Nash Equilibria}} via {{Potential Fields}}},
  shorttitle = {Coulomb {{GANs}}},
  author = {Unterthiner, Thomas and Nessler, Bernhard and Seward, Calvin and Klambauer, G{\"u}nter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
  year = {2017},
  month = aug,
  abstract = {Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.},
  archivePrefix = {arXiv},
  eprint = {1708.08819},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Zotero/storage/66N9PPT4/Unterthiner_et_al_2017_Coulomb_GANs.pdf;/Users/ilyes/Zotero/storage/2WZRAVJI/1708.html},
  journal = {arXiv:1708.08819 [cs, stat]},
  keywords = {altmmd,Computer Science - Computer Science and Game Theory,Computer Science - Learning,ideas,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{uria2013rnade,
  title = {{{RNADE}}: {{The}} Real-Valued Neural Autoregressive Density-Estimator},
  shorttitle = {{{RNADE}}},
  author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  year = {2013},
  month = jun,
  abstract = {We introduce RNADE, a new model for joint density estimation of real-valued vectors. Our model calculates the density of a datapoint as the product of one-dimensional conditionals modeled using mixture density networks with shared parameters. RNADE learns a distributed representation of the data, while having a tractable expression for the calculation of densities. A tractable likelihood allows direct comparison with other methods and training by standard gradient-based optimizers. We compare the performance of RNADE on several datasets of heterogeneous and perceptual data, finding it outperforms mixture models in all but one case.},
  archivePrefix = {arXiv},
  eprint = {1306.0186},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Uria_et_al_2013_RNADE_-_The_real-valued_neural_autoregressive_density-estimator.pdf;/Users/ilyes/Zotero/storage/F6G4AHL3/1306.html},
  journal = {arXiv:1306.0186 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{uria2016neural,
  title = {Neural {{Autoregressive Distribution Estimation}}},
  author = {Uria, Benigno and C{\^o}t{\'e}, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2016},
  month = may,
  abstract = {We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.},
  archivePrefix = {arXiv},
  eprint = {1605.02226},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/VI/Uria_et_al_2016_Neural_Autoregressive_Distribution_Estimation.pdf;/Users/ilyes/Zotero/storage/LIEYGJSA/1605.html},
  journal = {arXiv:1605.02226 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@book{vandervaart1998asymptotic,
  title = {Asymptotic Statistics},
  author = {{Van der Vaart}, Aad W.},
  year = {1998},
  volume = {3},
  publisher = {{Cambridge university press}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Van_der_Vaart_1998_Asymptotic_statistics.pdf;/Users/ilyes/Zotero/storage/VCL36BD5/books.html}
}

@article{vansteenkiste2019are,
  title = {Are {{Disentangled Representations Helpful}} for {{Abstract Visual Reasoning}}?},
  author = {{van Steenkiste}, Sjoerd and Locatello, Francesco and Schmidhuber, J{\"u}rgen and Bachem, Olivier},
  year = {2019},
  month = oct,
  abstract = {A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.},
  archivePrefix = {arXiv},
  eprint = {1905.12506},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/van_Steenkiste_et_al_2019_Are_Disentangled_Representations_Helpful_for_Abstract_Visual_Reasoning.pdf;/Users/ilyes/Zotero/storage/HN8DJRAL/1905.html},
  journal = {arXiv:1905.12506 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{villani2008optimal,
  title = {Optimal {{Transport}}: {{Old}} and {{New}}},
  shorttitle = {Optimal {{Transport}}},
  author = {Villani, C{\'e}dric},
  year = {2008},
  month = oct,
  publisher = {{Springer Science \& Business Media}},
  abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results.   PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book's value as a most welcome reference text on this subject.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Villani_2008_Optimal_Transport_-_Old_and_New.pdf},
  googlebooks = {hV8o5R7\_5tkC},
  isbn = {978-3-540-71050-9},
  keywords = {Mathematics / Calculus,Mathematics / Differential Equations / General,Mathematics / Functional Analysis,Mathematics / Geometry / Differential,Mathematics / Mathematical Analysis},
  language = {en}
}

@article{vincent2011connection,
  title = {A Connection between Score Matching and Denoising Autoencoders},
  author = {Vincent, Pascal},
  year = {2011},
  volume = {23},
  pages = {1661--1674},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Vincent_2011_A_connection_between_score_matching_and_denoising_autoencoders.pdf;/Users/ilyes/Zotero/storage/7E3UYA6R/NECO_a_00142.html;/Users/ilyes/Zotero/storage/QYDUYDHA/NECO_a_00142.html},
  journal = {Neural computation},
  number = {7}
}

@article{vinyals2015order,
  title = {Order {{Matters}}: {{Sequence}} to Sequence for Sets},
  shorttitle = {Order {{Matters}}},
  author = {Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
  year = {2015},
  month = nov,
  abstract = {Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.},
  archivePrefix = {arXiv},
  eprint = {1511.06391},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Vinyals_et_al_2015_Order_Matters_-_Sequence_to_sequence_for_sets.pdf;/Users/ilyes/Zotero/storage/GCQ72MMU/1511.html},
  journal = {arXiv:1511.06391 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{waegeman2012kernelbased,
  title = {A Kernel-Based Framework for Learning Graded Relations from Data},
  author = {Waegeman, Willem and Pahikkala, Tapio and Airola, Antti and Salakoski, Tapio and Stock, Michiel and De Baets, Bernard},
  year = {2012},
  month = dec,
  volume = {20},
  pages = {1090--1101},
  issn = {1063-6706, 1941-0034},
  doi = {10.1109/TFUZZ.2012.2194151},
  abstract = {Driven by a large number of potential applications in areas like bioinformatics, information retrieval and social network analysis, the problem setting of inferring relations between pairs of data objects has recently been investigated quite intensively in the machine learning community. To this end, current approaches typically consider datasets containing crisp relations, so that standard classification methods can be adopted. However, relations between objects like similarities and preferences are often expressed in a graded manner in real-world applications. A general kernel-based framework for learning relations from data is introduced here. It extends existing approaches because both crisp and graded relations are considered, and it unifies existing approaches because different types of graded relations can be modeled, including symmetric and reciprocal relations. This framework establishes important links between recent developments in fuzzy set theory and machine learning. Its usefulness is demonstrated through various experiments on synthetic and real-world data.},
  archivePrefix = {arXiv},
  eprint = {1111.6473},
  eprinttype = {arxiv},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Waegeman_et_al_2012_A_kernel-based_framework_for_learning_graded_relations_from_data.pdf;/Users/ilyes/Zotero/storage/WKZ3ANUN/1111.html},
  journal = {IEEE Transactions on Fuzzy Systems},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  number = {6}
}

@article{wang1997alignment,
  title = {Alignment of Curves by Dynamic Time Warping},
  author = {Wang, Kongming and Gasser, Theo},
  year = {1997},
  month = jun,
  volume = {25},
  pages = {1251--1276},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1069362747},
  abstract = {When studying some process or development in different subjects or units--be it biological, chemical or physical--we usually see a typical pattern, common to all curves. Yet there is variation both in amplitude and dynamics between curves. Following some ideas of structural analysis introduced by Kneip and Gasser, we study a method--dynamic time warping with a proper cost function--for estimating the shift or warping function from one curve to another to align the two functions. For some models this method can identify the true shift functions if the data are noise free. Noisy data are smoothed by a nonparametric function estimate such as a kernel estimate. It is shown that the proposed estimator is asymptotically normal and converges to the true shift function as the sample size per subject goes to infinity. Some simulation results are presented to illustrate the performance of this method.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Wang_Gasser_1997_Alignment_of_curves_by_dynamic_time_warping.pdf;/Users/ilyes/Zotero/storage/XGRPVAK3/1069362747.html},
  journal = {The Annals of Statistics},
  keywords = {Curves,dynamic time warping,kernel estimation,shift functions,structural analysis},
  mrnumber = {MR1447750},
  number = {3},
  zmnumber = {0898.62051}
}

@article{wang2016learning,
  title = {Learning to {{Draw Samples}}: {{With Application}} to {{Amortized MLE}} for {{Generative Adversarial Learning}}},
  shorttitle = {Learning to {{Draw Samples}}},
  author = {Wang, Dilin and Liu, Qiang},
  year = {2016},
  month = nov,
  abstract = {We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results.},
  archivePrefix = {arXiv},
  eprint = {1611.01722},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Dropbox (Personal)/Zotero/ML/Wang_Liu_2016_Learning_to_Draw_Samples.pdf;/Users/ilyes/Zotero/storage/8VM28P5S/1611.html},
  journal = {arXiv:1611.01722 [cs, stat]},
  keywords = {Computer Science - Learning,ideas,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{wasserman2013all,
  title = {All of Statistics: A Concise Course in Statistical Inference},
  shorttitle = {All of Statistics},
  author = {Wasserman, Larry},
  year = {2013},
  publisher = {{Springer Science \& Business Media}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Wasserman_2013_All_of_statistics_-_a_concise_course_in_statistical_inference.pdf;/Users/ilyes/Zotero/storage/9GWU46PQ/books.html}
}

@article{wied2012consistency,
  title = {Consistency of the Kernel Density Estimator: A Survey},
  shorttitle = {Consistency of the Kernel Density Estimator},
  author = {Wied, Dominik and Wei{\ss}bach, Rafael},
  year = {2012},
  month = feb,
  volume = {53},
  pages = {1--21},
  issn = {1613-9798},
  doi = {10.1007/s00362-010-0338-1},
  abstract = {Various consistency proofs for the kernel density estimator have been developed over the last few decades. Important milestones are the pointwise consistency and almost sure uniform convergence with a fixed bandwidth on the one hand and the rate of convergence with a fixed or even a variable bandwidth on the other hand. While considering global properties of the empirical distribution functions is sufficient for strong consistency, proofs of exact convergence rates use deeper information about the underlying empirical processes. A unifying character, however, is that earlier and more recent proofs use bounds on the probability that a sum of random variables deviates from its mean.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Probastats/Wied_Weißbach_2012_Consistency_of_the_kernel_density_estimator_-_a_survey.pdf},
  journal = {Statistical Papers},
  keywords = {60-02,62-02,Empirical process,Kernel estimation,Pointwise consistency,Rate of convergence,Strong uniform consistency,Variable bandwidth},
  language = {en},
  number = {1}
}

@book{wikibooks2016latex,
  title = {{{LaTeX}}},
  author = {Wikibooks},
  year = {2016},
  publisher = {{Wikibooks}},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Random/Wikibooks_2016_LaTeX.pdf}
}

@article{wolpert2000computational,
  title = {Computational Principles of Movement Neuroscience},
  author = {Wolpert, Daniel M. and Ghahramani, Zoubin},
  year = {2000},
  month = nov,
  volume = {3},
  pages = {1212},
  issn = {1546-1726},
  doi = {10.1038/81497},
  abstract = {Computational principles of movement neuroscience},
  copyright = {2000 Nature Publishing Group},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Wolpert_Ghahramani_2000_Computational_principles_of_movement_neuroscience.pdf;/Users/ilyes/Zotero/storage/982UK8Z5/nn1100_1212.html},
  journal = {Nature Neuroscience},
  language = {En},
  number = {11s}
}

@article{wu2020causal,
  title = {Causal {{Mosaic}}: {{Cause}}-{{Effect Inference}} via {{Nonlinear ICA}} and {{Ensemble Method}}},
  shorttitle = {Causal {{Mosaic}}},
  author = {Wu, Pengzhou and Fukumizu, Kenji},
  year = {2020},
  month = jan,
  abstract = {We address the problem of distinguishing cause from effect in bivariate setting. Based on recent developments in nonlinear independent component analysis (ICA), we train nonparametrically general nonlinear causal models that allow non-additive noise. Further, we build an ensemble framework, namely Causal Mosaic, which models a causal pair by a mixture of nonlinear models. We compare this method with other recent methods on artificial and real world benchmark datasets, and our method shows state-of-the-art performance.},
  archivePrefix = {arXiv},
  eprint = {2001.01894},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Wu_Fukumizu_2020_Causal_Mosaic_-_Cause-Effect_Inference_via_Nonlinear_ICA_and_Ensemble_Method.pdf;/Users/ilyes/Zotero/storage/IET224AS/2001.html},
  journal = {arXiv:2001.01894 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{yang1998informationtheoretic,
  title = {Information\textendash{}Theoretic Approach to Blind Separation of Sources in Non-Linear Mixture},
  author = {Yang, Howard Hua and Amari, Shun-ichi and Cichocki, Andrzej},
  year = {1998},
  month = feb,
  volume = {64},
  pages = {291--300},
  issn = {0165-1684},
  doi = {10.1016/S0165-1684(97)00196-5},
  abstract = {The linear mixture model is assumed in most of the papers devoted to blind separation. A more realistic model for mixture should be non-linear. In this paper, a two-layer perceptron is used as a de-mixing system to separate sources in non-linear mixture. The learning algorithms for the de-mixing system are derived by two approaches: maximum entropy and minimum mutual information. The algorithms derived from the two approaches have a common structure. The new learning equations for the hidden layer are different from the learning equations for the output layer. The natural gradient descent method is applied in maximizing entropy and minimizing mutual information. The information (entropy or mutual information) back-propagation method is proposed to derive the learning equations for the hidden layer.
Zusammenfassung
In den meisten Arbeiten zur blinden Separation von Quellen wird von einem linearen Mischmodell ausgegangen. In dieser Arbeit hingegen wird ein Zweischicht-Perzeptron als System zur Trennung von Quellen bei nichtlinearer Mischung verwendet. Die Lernalgorithmen f{\"u}r das trennende System werden ausgehend von zwei Ans{\"a}tzen abgeleitet: Maximale Entropie und minimale wechselseitige Information. Die Algorithmen, die von den beiden Ans{\"a}tzen abgeleitet werden, haben eine gemeinsame Struktur. Die resultierenden Gleichungen f{\"u}r die versteckte Schicht weichen von denen f{\"u}r die Ausgangsschicht ab. Das herk{\"o}mmliche Gradientenabstiegsverfahren wird dazu verwendet, die Entropie zu maximieren und die wechselseitige Information zu minimieren. F{\"u}r die versteckte Schicht wird die Methode des Informations-Backpropagation (Entropie oder wechselseitige Information) zur Ableitung der Gleichungen des Lernverfahrens vorgeschlagen.
R{\'e}sum{\'e}
Un mod{\`e}le lin{\'e}aire de m{\'e}lange est suppos{\'e} dans la plupart des articles d{\'e}volus {\`a} la s{\'e}paration aveugle. Un mod{\`e}le plus r{\'e}aliste de m{\'e}lange devrait {\^e}tre non-lin{\'e}aire. Dans cet article, un perceptron {\`a} deux couches est utilis{\'e} comme syst{\`e}me de s{\'e}paration de sources dans un m{\'e}lange non-lin{\'e}aire. Les algorithmes pour le syst{\`e}me de s{\'e}paration sont d{\'e}riv{\'e}s {\`a} l'aide de deux approches: entropie maximum et information mutuelle minimum. Les algorithmes d{\'e}riv{\'e}s dans ces deux approches ont une structure commune. Les nouvelles {\'e}quations d'apprentissage pour la couche cach{\'e}e sont diff{\'e}rentes de celles pour la couche de sortie. La m{\'e}thode du gradient naturelle est appliqu{\'e}e pour la maximisation et la minimisation de l'information mutuelle. La m{\'e}thode de r{\'e}tro-propagation de l'information (entropie ou information mutuelle) est propos{\'e}e pour d{\'e}river les {\'e}quations d'apprentissage de la couche cach{\'e}e.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Yang_et_al_1998_Information–theoretic_approach_to_blind_separation_of_sources_in_non-linear.pdf;/Users/ilyes/Zotero/storage/AA6ELCRD/S0165168497001965.html},
  journal = {Signal Processing},
  keywords = {Blind separation,BSS,ICA,Information back propagation,Maximum entropy,Minimum mutual information,NICA,Non-linear mixture,Old NICA},
  number = {3}
}

@article{zhang1999neuronal,
  title = {Neuronal Tuning: {{To}} Sharpen or Broaden?},
  shorttitle = {Neuronal Tuning},
  author = {Zhang, K. and Sejnowski, T. J.},
  year = {1999},
  month = jan,
  volume = {11},
  pages = {75--84},
  issn = {0899-7667},
  abstract = {Sensory and motor variables are typically represented by a population of broadly tuned neurons. A coarser representation with broader tuning can often improve coding accuracy, but sometimes the accuracy may also improve with sharper tuning. The theoretical analysis here shows that the relationship between tuning width and accuracy depends crucially on the dimension of the encoded variable. A general rule is derived for how the Fisher information scales with the tuning width, regardless of the exact shape of the tuning function, the probability distribution of spikes, and allowing some correlated noise between neurons. These results demonstrate a universal dimensionality effect in neural population coding.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Zhang_Sejnowski_1999_Neuronal_tuning_-_To_sharpen_or_broaden.pdf},
  journal = {Neural Computation},
  keywords = {Artifacts,Electrophysiology,Models; Neurological,Neurons},
  language = {eng},
  number = {1},
  pmid = {9950722}
}

@inproceedings{zhang2009identifiability,
  title = {On the Identifiability of the Post-Nonlinear Causal Model},
  booktitle = {25th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}, {{UAI}} 2009},
  author = {Zhang, Kun and Hyvarinen, Aapo},
  year = {2009},
  volume = {35},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Zhang_Hyvarinen_2012_On_the_identifiability_of_the_post-nonlinear_causal_model.pdf;/Users/ilyes/Zotero/storage/Q7YPL78A/1205.html}
}

@article{zhang2015distinguishing,
  title = {Distinguishing {{Cause}} from {{Effect Based}} on {{Exogeneity}}},
  author = {Zhang, Kun and Zhang, Jiji and Sch{\"o}lkopf, Bernhard},
  year = {2015},
  month = apr,
  abstract = {Recent developments in structural equation modeling have produced several methods that can usually distinguish cause from effect in the two-variable case. For that purpose, however, one has to impose substantial structural constraints or smoothness assumptions on the functional causal models. In this paper, we consider the problem of determining the causal direction from a related but different point of view, and propose a new framework for causal direction determination. We show that it is possible to perform causal inference based on the condition that the cause is "exogenous" for the parameters involved in the generating process from the cause to the effect. In this way, we avoid the structural constraints required by the SEM-based approaches. In particular, we exploit nonparametric methods to estimate marginal and conditional distributions, and propose a bootstrap-based approach to test for the exogeneity condition; the testing results indicate the causal direction between two variables. The proposed method is validated on both synthetic and real data.},
  archivePrefix = {arXiv},
  eprint = {1504.05651},
  eprinttype = {arxiv},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Zhang_et_al_2015_Distinguishing_Cause_from_Effect_Based_on_Exogeneity.pdf;/Users/ilyes/Zotero/storage/ERMKKQU2/1504.html},
  journal = {arXiv:1504.05651 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{zhang2015learning,
  title = {Learning {{Multiple Linear Mappings}} for {{Efficient Single Image Super}}-{{Resolution}}},
  author = {Zhang, K. and Tao, D. and Gao, X. and Li, X. and Xiong, Z.},
  year = {2015},
  month = mar,
  volume = {24},
  pages = {846--861},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2389629},
  abstract = {Example learning-based superresolution (SR) algorithms show promise for restoring a high-resolution (HR) image from a single low-resolution (LR) input. The most popular approaches, however, are either time- or space-intensive, which limits their practical applications in many resource-limited settings. In this paper, we propose a novel computationally efficient single image SR method that learns multiple linear mappings (MLM) to directly transform LR feature subspaces into HR subspaces. In particular, we first partition the large nonlinear feature space of LR images into a cluster of linear subspaces. Multiple LR subdictionaries are then learned, followed by inferring the corresponding HR subdictionaries based on the assumption that the LR-HR features share the same representation coefficients. We establish MLM from the input LR features to the desired HR outputs in order to achieve fast yet stable SR recovery. Furthermore, in order to suppress displeasing artifacts generated by the MLM-based method, we apply a fast nonlocal means algorithm to construct a simple yet effective similarity-based regularization term for SR enhancement. Experimental results indicate that our approach is both quantitatively and qualitatively superior to other application-oriented SR methods, while maintaining relatively low time and space complexity.},
  file = {/Users/ilyes/Zotero/storage/HAJQXUHG/7003985.html},
  journal = {IEEE Transactions on Image Processing},
  keywords = {Dictionaries,example learning-based superresolution algorithm,Fast non-local means,Feature extraction,feature subspace,high-resolution image restoration,HR subspace,Image reconstruction,image resolution,image restoration,learning (artificial intelligence),LR feature subspace,LR subdictionaries learning,MLM learning,multiple linear mapping learning,multiple linear mappings (MLMs),Principal component analysis,similarity-based regularization term,single image super-resolution,single image super-resolution (SR),single image super-resolution (SR).,SR algorithm,SR enhancement,Training,Transforms,Vectors},
  number = {3}
}

@inproceedings{zhang2017causal,
  title = {Causal Discovery from Nonstationary/Heterogeneous Data: {{Skeleton}} Estimation and Orientation Determination},
  shorttitle = {Causal Discovery from Nonstationary/Heterogeneous Data},
  booktitle = {{{IJCAI}}: {{Proceedings}} of the {{Conference}}},
  author = {Zhang, Kun and Huang, Biwei and Zhang, Jiji and Glymour, Clark and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  volume = {2017},
  pages = {1347},
  publisher = {{NIH Public Access}},
  file = {/Users/ilyes/Zotero/storage/AWNVYCNR/PMC5617646.html}
}

@article{zylberberg2017robust,
  title = {Robust Information Propagation through Noisy Neural Circuits},
  author = {Zylberberg, Joel and Pouget, Alexandre and Latham, Peter E. and {Shea-Brown}, Eric},
  year = {2017},
  month = apr,
  volume = {13},
  pages = {e1005497},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005497},
  abstract = {Author summary Information about the outside world, which originates in sensory neurons, propagates through multiple stages of processing before reaching the neural structures that control behavior. While much work in neuroscience has investigated the factors that affect the amount of information contained in peripheral sensory areas, very little work has asked how much of that information makes it through subsequent processing stages. That's the focus of this paper, and it's an important issue because information that fails to propagate cannot be used to affect decision-making. We find a tradeoff between information content and information transmission: neural codes which contain a large amount of information can transmit that information poorly to subsequent processing stages. Thus, the problem of robust information propagation\textemdash{}which has largely been overlooked in previous research\textemdash{}may be critical for determining how our sensory organs communicate with our brains. We identify the conditions under which information propagates well\textemdash{}or poorly\textemdash{}through multiple stages of neural processing.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Zylberberg_et_al_2017_Robust_information_propagation_through_noisy_neural_circuits.pdf;/Users/ilyes/Zotero/storage/JXZ7V3RZ/article.html},
  journal = {PLOS Computational Biology},
  keywords = {Coding mechanisms,Covariance,Eigenvalues,Ellipses,Gaussian noise,Neural pathways,Neuronal tuning,Neurons},
  number = {4}
}


