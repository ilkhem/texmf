
@article{benamou2015iterative,
  title = {Iterative {{Bregman Projections}} for {{Regularized Transportation Problems}}},
  volume = {37},
  issn = {1064-8275},
  url = {http://epubs.siam.org/doi/abs/10.1137/141000439},
  doi = {10.1137/141000439},
  abstract = {This paper details a general numerical framework to approximate solutions to linear programs related to optimal transport. The general idea is to introduce an entropic regularization of the initial linear program. This regularized problem corresponds to a  Kullback--Leibler Bregman divergence projection of a vector (representing some initial joint distribution) on the polytope of constraints. We show that for many problems related to optimal transport, the set of linear constraints can be split in an intersection of a few simple constraints, for which the projections can be computed in closed form. This allows us to make use of iterative Bregman projections (when there are only equality constraints) or, more generally, Bregman--Dykstra iterations (when  inequality constraints are involved). We illustrate the usefulness of this approach for several variational problems related to optimal transport: barycenters for the optimal transport metric, tomographic reconstruction, multimarginal optimal transport, and in particular its application to Brenier's relaxed solutions of incompressible Euler equations, partial unbalanced optimal transport, and optimal transport with capacity constraints.},
  number = {2},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  date = {2015-01-01},
  pages = {A1111-A1138},
  author = {Benamou, J. and Carlier, G. and Cuturi, M. and Nenna, L. and Peyré, G.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/MHZSR6D7/141000439.html}
}

@article{agueh2011barycenters,
  title = {Barycenters in the {{Wasserstein Space}}},
  volume = {43},
  issn = {0036-1410},
  url = {http://epubs.siam.org/doi/abs/10.1137/100805741},
  doi = {10.1137/100805741},
  abstract = {In this paper, we introduce a notion of barycenter in the Wasserstein space which generalizes McCann's interpolation to the case of more than two measures. We provide existence, uniqueness, characterizations, and regularity of the barycenter and relate it to the multimarginal optimal transport problem considered by Gangbo and Święch in [Comm. Pure Appl. Math., 51 (1998), pp. 23–45]. We also consider some examples and, in particular, rigorously solve the Gaussian case. We finally discuss convexity of functionals in the Wasserstein space.},
  number = {2},
  journaltitle = {SIAM Journal on Mathematical Analysis},
  shortjournal = {SIAM J. Math. Anal.},
  date = {2011-01-01},
  pages = {904-924},
  author = {Agueh, M. and Carlier, G.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/NBCZ98NJ/100805741.html}
}

@book{santambrogio2015optimal,
  title = {Optimal Transport for Applied Mathematicians: Calculus of Variations, Pdes, and Modeling},
  url = {http://books.google.com/books?hl=en&lr=&id=UOHHCgAAQBAJ&oi=fnd&pg=PR7&dq=info:eNsUYsJclrsJ:scholar.google.com&ots=WvUJ5K4B14&sig=QQ2V_ElonnAUPw1WiLf_fN-YH8M},
  shorttitle = {Optimal Transport for Applied Mathematicians},
  publisher = {{Birkhäuser}},
  date = {2015},
  keywords = {_read_in_progress,otam},
  author = {Santambrogio, Filippo},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Santambrogio_2015_Optimal_transport_for_applied_mathematicians_-_calculus_of_variations,_pdes,_and.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/TVDZEV9Z/books.html}
}

@article{cuturi2016smoothed,
  title = {A Smoothed Dual Approach for Variational Wasserstein Problems},
  volume = {9},
  url = {http://epubs.siam.org/doi/abs/10.1137/15M1032600},
  number = {1},
  journaltitle = {SIAM J. Imaging Sciences},
  date = {2016},
  pages = {320--343},
  author = {Cuturi, Marco and Peyré, Gabriel},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Peyré_2016_A_smoothed_dual_approach_for_variational_wasserstein_problems.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/RNE5NT6R/15M1032600.html}
}

@inproceedings{cuturi2005semigroup,
  title = {Semigroup Kernels on Finite Sets},
  volume = {17},
  url = {http://papers.nips.cc/paper/2709-semigroup-kernels-on-finite-sets.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 17: {{Proceedings}} of the 2004 {{Conference}}},
  publisher = {{MIT Press}},
  date = {2005},
  pages = {329},
  author = {Cuturi, Marco and Vert, Jean-Philippe},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Vert_2005_Semigroup_kernels_on_finite_sets.pdf}
}

@article{cuturi2005contexttree,
  title = {The Context-Tree Kernel for Strings},
  volume = {18},
  url = {http://www.sciencedirect.com/science/article/pii/S089360800500170X},
  number = {8},
  journaltitle = {Neural Networks},
  date = {2005},
  pages = {1111--1123},
  author = {Cuturi, Marco and Vert, Jean-Philippe},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Vert_2005_The_context-tree_kernel_for_strings.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2ZCA37F2/S089360800500170X.html}
}

@article{solomon2015convolutional,
  title = {Convolutional {{Wasserstein Distances}}: {{Efficient Optimal Transportation}} on {{Geometric Domains}}},
  url = {http://dl.acm.org/citation.cfm?id=2766963},
  shorttitle = {Convolutional {{Wasserstein Distances}}},
  journaltitle = {ACM Transactions on Graphics (TOG) SIGGRAPH, 2015},
  date = {2015},
  author = {Solomon, Justin and de Goes, Fernando and Peyré, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
  options = {useprefix=true},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Solomon_et_al_2015_Convolutional_Wasserstein_Distances_-_Efficient_Optimal_Transportation_on.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/H724RA73/citation.html}
}

@inproceedings{cuturi2014fast,
  title = {Fast Computation of Wasserstein Barycenters},
  volume = {32},
  url = {http://www.jmlr.org/proceedings/papers/v32/cuturi14.pdf},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}} 2014, {{JMLR W}}\&{{CP}}},
  date = {2014},
  pages = {685--693},
  author = {Cuturi, Marco and Doucet, Arnaud},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Doucet_2014_Fast_computation_of_wasserstein_barycenters.pdf}
}

@article{cuturi2005semigroupa,
  title = {Semigroup Kernels on Measures},
  volume = {6},
  url = {http://www.jmlr.org/papers/v6/cuturi05a.html},
  issue = {Jul},
  journaltitle = {Journal of Machine Learning Research},
  date = {2005},
  pages = {1169--1198},
  author = {Cuturi, Marco and Fukumizu, Kenji and Vert, Jean-Philippe},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_et_al_2005_Semigroup_kernels_on_measures.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/WKAN63C3/cuturi05a.html}
}

@inproceedings{cuturi2011fast,
  title = {Fast Global Alignment Kernels},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Cuturi_489.pdf},
  booktitle = {International {{Conference}} in {{Machine Learning}} 2011},
  date = {2011},
  author = {Cuturi, Marco},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_2011_Fast_global_alignment_kernels.pdf}
}

@inproceedings{cuturi2007kernel,
  title = {A Kernel for Time Series Based on Global Alignments},
  volume = {2},
  url = {http://ieeexplore.ieee.org/abstract/document/4217433/},
  booktitle = {Acoustics, {{Speech}} and {{Signal Processing}}, 2007. {{ICASSP}} 2007. {{IEEE International Conference}} On},
  publisher = {{IEEE}},
  date = {2007},
  pages = {II--413},
  author = {Cuturi, Marco and Vert, Jean-Philippe and Birkenes, Oystein and Matsui, Tomoko},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_et_al_2007_A_kernel_for_time_series_based_on_global_alignments.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/3599QFCW/4217433.html}
}

@inproceedings{cuturi2013sinkhorn,
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transport}}},
  url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport},
  shorttitle = {Sinkhorn {{Distances}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  date = {2013},
  pages = {2292--2300},
  keywords = {_read},
  author = {Cuturi, Marco},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_2013_Sinkhorn_Distances_-_Lightspeed_Computation_of_Optimal_Transport.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/GSDA4WN7/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.html}
}

@online{centerforhistoryandnewmediazotero,
  title = {Zotero {{Quick Start Guide}}},
  url = {http://zotero.org/support/quick_start_guide},
  author = {{Center for History and New Media}}
}

@book{wasserman2013all,
  title = {All of Statistics: A Concise Course in Statistical Inference},
  shorttitle = {All of Statistics},
  publisher = {{Springer Science \& Business Media}},
  date = {2013},
  author = {Wasserman, Larry},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Wasserman_2013_All_of_statistics_-_a_concise_course_in_statistical_inference.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/9GWU46PQ/books.html}
}

@article{hyvarinen2005estimation,
  title = {Estimation of Non-Normalized Statistical Models by Score Matching},
  volume = {6},
  issue = {Apr},
  journaltitle = {Journal of Machine Learning Research},
  date = {2005},
  pages = {695--709},
  author = {Hyvärinen, Aapo},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hyvärinen_2005_Estimation_of_non-normalized_statistical_models_by_score_matching.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/CZ7F8KR2/hyvarinen05a.html}
}

@article{mroueh2018regularized,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.12062},
  primaryClass = {cs, stat},
  title = {Regularized {{Kernel}} and {{Neural Sobolev Descent}}: {{Dynamic MMD Transport}}},
  url = {http://arxiv.org/abs/1805.12062},
  shorttitle = {Regularized {{Kernel}} and {{Neural Sobolev Descent}}},
  abstract = {We introduce Regularized Kernel and Neural Sobolev Descent for transporting a source distribution to a target distribution along smooth paths of minimum kinetic energy (defined by the Sobolev discrepancy), related to dynamic optimal transport. In the kernel version, we give a simple algorithm to perform the descent along gradients of the Sobolev critic, and show that it converges asymptotically to the target distribution in the MMD sense. In the neural version, we parametrize the Sobolev critic with a neural network with input gradient norm constrained in expectation. We show in theory and experiments that regularization has an important role in favoring smooth transitions between distributions, avoiding large discrete jumps. Our analysis could provide a new perspective on the impact of critic updates (early stopping) on the paths to equilibrium in the GAN setting.},
  urldate = {2018-06-12},
  date = {2018-05-30},
  keywords = {Computer Science - Learning,flow,read,Statistics - Machine Learning},
  author = {Mroueh, Youssef and Sercu, Tom and Raj, Anant},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Mroueh_et_al_2018_Regularized_Kernel_and_Neural_Sobolev_Descent_-_Dynamic_MMD_Transport.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/GVPWFYM3/1805.html}
}

@inproceedings{gutmann2010noisecontrastive,
  title = {Noise-Contrastive Estimation: {{A}} New Estimation Principle for Unnormalized Statistical Models},
  shorttitle = {Noise-Contrastive Estimation},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  date = {2010},
  pages = {297--304},
  keywords = {read,logistic regression,noise,non linear logistic regression},
  author = {Gutmann, Michael and Hyvärinen, Aapo},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gutmann_Hyvärinen_2010_Noise-contrastive_estimation_-_A_new_estimation_principle_for_unnormalized.pdf}
}

@article{saremi2018deep,
  title = {Deep {{Energy Estimator Networks}}},
  journaltitle = {arXiv preprint arXiv:1805.08306},
  date = {2018},
  author = {Saremi, Saeed and Mehrjou, Arash and Schölkopf, Bernhard and Hyvärinen, Aapo},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Saremi_et_al_2018_Deep_Energy_Estimator_Networks.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/AFYRGKDL/1805.html}
}

@book{dudley2002real,
  langid = {english},
  title = {Real {{Analysis}} and {{Probability}}},
  url = {/core/books/real-analysis-and-probability/26DDF2D09E526185F2347AA5658B96F6},
  abstract = {Cambridge Core - Real and Complex Analysis - Real Analysis and Probability -  by R. M. Dudley},
  urldate = {2018-07-17},
  date = {2002-10},
  author = {Dudley, R. M.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Dudley_2002_Real_Analysis_and_Probability.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/3APF9BIK/26DDF2D09E526185F2347AA5658B96F6.html},
  doi = {10.1017/CBO9780511755347}
}

@article{gretton2012kernel,
  title = {A {{Kernel Two}}-{{Sample Test}}},
  volume = {13},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v13/gretton12a.html},
  issue = {Mar},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-07-16},
  date = {2012},
  pages = {723-773},
  author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gretton_et_al_2012_A_Kernel_Two-Sample_Test.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2HFEFKTC/gretton12a.html}
}

@article{chwialkowski2015fast,
  langid = {english},
  title = {Fast {{Two}}-{{Sample Testing}} with {{Analytic Representations}} of {{Probability Measures}}},
  url = {https://arxiv.org/abs/1506.04725},
  urldate = {2018-07-10},
  date = {2015-06-15},
  author = {Chwialkowski, Kacper and Ramdas, Aaditya and Sejdinovic, Dino and Gretton, Arthur},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chwialkowski_et_al_2015_Fast_Two-Sample_Testing_with_Analytic_Representations_of_Probability_Measures.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/672JCJTI/1506.html}
}

@article{bach2012equivalence,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1203.4523},
  primaryClass = {cs, math, stat},
  title = {On the {{Equivalence}} between {{Herding}} and {{Conditional Gradient Algorithms}}},
  url = {http://arxiv.org/abs/1203.4523},
  abstract = {We show that the herding procedure of Welling (2009) takes exactly the form of a standard convex optimization algorithm--namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. The experiments indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm tends to approach more often the maximum entropy distribution, shedding more light on the learning bias behind herding.},
  urldate = {2018-06-15},
  date = {2012-03-20},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Mathematics - Optimization and Control},
  author = {Bach, Francis and Lacoste-Julien, Simon and Obozinski, Guillaume},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Bach_et_al_2012_On_the_Equivalence_between_Herding_and_Conditional_Gradient_Algorithms.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/L3SCTUZL/1203.html}
}

@article{chizat2018global,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.09545},
  primaryClass = {cs, math, stat},
  title = {On the {{Global Convergence}} of {{Gradient Descent}} for {{Over}}-Parameterized {{Models}} Using {{Optimal Transport}}},
  url = {http://arxiv.org/abs/1805.09545},
  abstract = {Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.},
  urldate = {2018-06-15},
  date = {2018-05-24},
  keywords = {Statistics - Machine Learning,Mathematics - Optimization and Control,Computer Science - Neural and Evolutionary Computing},
  author = {Chizat, Lenaic and Bach, Francis},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chizat_Bach_2018_On_the_Global_Convergence_of_Gradient_Descent_for_Over-parameterized_Models.pdf;/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chizat_Bach_2018_On_the_Global_Convergence_of_Gradient_Descent_for_Over-parameterized_Models2.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/R8VSAF8I/1805.html}
}

@inproceedings{liu2017stein,
  title = {Stein Variational Gradient Descent as Gradient Flow},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {2017},
  pages = {3118--3126},
  author = {Liu, Qiang},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Liu_2017_Stein_variational_gradient_descent_as_gradient_flow.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/CDY3EJIN/6904-stein-variational-gradient-descent-as-gradient-flow.html}
}

@inproceedings{liu2016stein,
  title = {Stein Variational Gradient Descent: {{A}} General Purpose Bayesian Inference Algorithm},
  shorttitle = {Stein Variational Gradient Descent},
  booktitle = {Advances {{In Neural Information Processing Systems}}},
  date = {2016},
  pages = {2378--2386},
  author = {Liu, Qiang and Wang, Dilin},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Liu_Wang_2016_Stein_variational_gradient_descent_-_A_general_purpose_bayesian_inference.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/ALD7XAVX/6338-stein-variational-gradient-descent-a-general-purpose-bayesian-inference-algorithm.html}
}

@article{sriperumbudur2011universality,
  title = {Universality, Characteristic Kernels and {{RKHS}} Embedding of Measures},
  volume = {12},
  issue = {Jul},
  journaltitle = {Journal of Machine Learning Research},
  date = {2011},
  pages = {2389--2410},
  author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Lanckriet, Gert RG},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sriperumbudur_et_al_2011_Universality,_characteristic_kernels_and_RKHS_embedding_of_measures.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/4596P7VL/sriperumbudur11a.html}
}

@inproceedings{li2015generative,
  title = {Generative Moment Matching Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  date = {2015},
  pages = {1718--1727},
  keywords = {ideas},
  author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Li_et_al_2015_Generative_moment_matching_networks.pdf}
}

@article{unterthiner2017coulomb,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1708.08819},
  primaryClass = {cs, stat},
  title = {Coulomb {{GANs}}: {{Provably Optimal Nash Equilibria}} via {{Potential Fields}}},
  url = {http://arxiv.org/abs/1708.08819},
  shorttitle = {Coulomb {{GANs}}},
  abstract = {Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples.},
  urldate = {2018-05-09},
  date = {2017-08-29},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,ideas,Computer Science - Computer Science and Game Theory,altmmd},
  author = {Unterthiner, Thomas and Nessler, Bernhard and Seward, Calvin and Klambauer, Günter and Heusel, Martin and Ramsauer, Hubert and Hochreiter, Sepp},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/66N9PPT4/Unterthiner_et_al_2017_Coulomb_GANs.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2WZRAVJI/1708.html}
}

@inproceedings{hochreiter2001maximum,
  title = {Beyond Maximum Likelihood and Density Estimation: {{A}} Sample-Based Criterion for Unsupervised Learning of Complex Models},
  shorttitle = {Beyond Maximum Likelihood and Density Estimation},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {2001},
  pages = {535--541},
  keywords = {ideas},
  author = {Hochreiter, Sepp and Mozer, Michael C.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hochreiter_Mozer_2001_Beyond_maximum_likelihood_and_density_estimation_-_A_sample-based_criterion_for.pdf}
}

@article{wang2016learning,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.01722},
  primaryClass = {cs, stat},
  title = {Learning to {{Draw Samples}}: {{With Application}} to {{Amortized MLE}} for {{Generative Adversarial Learning}}},
  url = {http://arxiv.org/abs/1611.01722},
  shorttitle = {Learning to {{Draw Samples}}},
  abstract = {We propose a simple algorithm to train stochastic neural networks to draw samples from given target distributions for probabilistic inference. Our method is based on iteratively adjusting the neural network parameters so that the output changes along a Stein variational gradient that maximumly decreases the KL divergence with the target distribution. Our method works for any target distribution specified by their unnormalized density function, and can train any black-box architectures that are differentiable in terms of the parameters we want to adapt. As an application of our method, we propose an amortized MLE algorithm for training deep energy model, where a neural sampler is adaptively trained to approximate the likelihood function. Our method mimics an adversarial game between the deep energy model and the neural sampler, and obtains realistic-looking images competitive with the state-of-the-art results.},
  urldate = {2018-05-09},
  date = {2016-11-05},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,ideas},
  author = {Wang, Dilin and Liu, Qiang},
  file = {/Users/ilyes/Dropbox (Personal)/Zotero/ML/Wang_Liu_2016_Learning_to_Draw_Samples.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/8VM28P5S/1611.html}
}

@article{barp2017geometry,
  title = {Geometry and {{Dynamics}} for {{Markov Chain Monte Carlo}}},
  number = {0},
  journaltitle = {Annual Review of Statistics and Its Application},
  date = {2017},
  keywords = {ideas,projet1},
  author = {Barp, Alessandro and Briol, François-Xavier and Kennedy, Anthony D. and Girolami, Mark},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Barp_et_al_2017_Geometry_and_Dynamics_for_Markov_Chain_Monte_Carlo.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/5ZT4EDPV/annurev-statistics-031017-100141.html}
}

@inproceedings{hyvarinen2016unsupervised,
  title = {Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear {{ICA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  date = {2016},
  pages = {3765--3773},
  keywords = {ICA,NICA},
  author = {Hyvärinen, Aapo and Morioka, Hiroshi},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Morioka_2016_Unsupervised_feature_extraction_by_time-contrastive_learning_and_nonlinear_ica.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/7PR36K9V/6394-unsupervised-feature-extraction-by-time-contrastive-learning-and-nonlinear-ica.html}
}

@book{boucheron2013concentration,
  title = {Concentration Inequalities: {{A}} Nonasymptotic Theory of Independence},
  shorttitle = {Concentration Inequalities},
  publisher = {{Oxford university press}},
  date = {2013},
  author = {Boucheron, Stéphane and Lugosi, Gábor and Massart, Pascal},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Boucheron_et_al_2013_Concentration_inequalities_-_A_nonasymptotic_theory_of_independence.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/MDJSZV7V/books.html}
}

@book{friedman2001elements,
  title = {The Elements of Statistical Learning},
  volume = {1},
  publisher = {{Springer series in statistics New York}},
  date = {2001},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Friedman_et_al_2001_The_elements_of_statistical_learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/8NSQJPWT/Friedman et al. - 2001 - The elements of statistical learning.ps;/nfs/nhome/live/ilyesk/Zotero/storage/FND7MQNT/Friedman et al. - 2001 - The elements of statistical learning.ps}
}

@book{eagleton2011literary,
  title = {Literary Theory: {{An}} Introduction},
  shorttitle = {Literary Theory},
  publisher = {{John Wiley \& Sons}},
  date = {2011},
  author = {Eagleton, Terry},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Random/Eagleton_2011_Literary_theory_-_An_introduction.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/7VWY5VZE/books.html}
}

@book{love2010linux,
  title = {Linux Kernel Development},
  publisher = {{Pearson Education}},
  date = {2010},
  author = {Love, Robert},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Random/Love_2010_Linux_kernel_development.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/AFVADFWJ/books.html}
}

@article{smith2014grouppca,
  title = {Group-{{PCA}} for Very Large {{fMRI}} Datasets},
  volume = {101},
  issn = {1053-8119},
  url = {http://www.sciencedirect.com/science/article/pii/S105381191400634X},
  doi = {10.1016/j.neuroimage.2014.07.051},
  abstract = {Increasingly-large datasets (for example, the resting-state fMRI data from the Human Connectome Project) are demanding analyses that are problematic because of the sheer scale of the aggregate data. We present two approaches for applying group-level PCA; both give a close approximation to the output of PCA applied to full concatenation of all individual datasets, while having very low memory requirements regardless of the number of datasets being combined. Across a range of realistic simulations, we find that in most situations, both methods are more accurate than current popular approaches for analysis of multi-subject resting-state fMRI studies. The group-PCA output can be used to feed into a range of further analyses that are then rendered practical, such as the estimation of group-averaged voxelwise connectivity, group-level parcellation, and group-ICA.},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  urldate = {2018-03-27},
  date = {2014-11-01},
  pages = {738-749},
  keywords = {Big data,fMRI,ICA,PCA},
  author = {Smith, Stephen M. and Hyvärinen, Aapo and Varoquaux, Gaël and Miller, Karla L. and Beckmann, Christian F.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Smith_et_al_2014_Group-PCA_for_very_large_fMRI_datasets.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/MENMRIJ9/S105381191400634X.html}
}

@book{sutton2018reinforcement,
  title = {Reinforcement Learning: {{An}} Introduction},
  volume = {1},
  shorttitle = {Reinforcement Learning},
  number = {1},
  publisher = {{MIT press Cambridge}},
  date = {2018},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Sutton_Barto_2018_Reinforcement_learning_-_An_introduction.pdf},
  note = {DRAFT}
}

@book{legall2006intégration,
  title = {Intégration, Probabilités et Processus Aléatoires},
  date = {2006},
  author = {Le Gall, Jean-François},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Le_Gall_2006_Intégration,_probabilités_et_processus_aléatoires.pdf}
}

@article{graupner2012calciumbased,
  langid = {english},
  title = {Calcium-Based Plasticity Model Explains Sensitivity of Synaptic Changes to Spike Pattern, Rate, and Dendritic Location},
  volume = {109},
  issn = {1091-6490},
  doi = {10.1073/pnas.1109359109},
  abstract = {Multiple stimulation protocols have been found to be effective in changing synaptic efficacy by inducing long-term potentiation or depression. In many of those protocols, increases in postsynaptic calcium concentration have been shown to play a crucial role. However, it is still unclear whether and how the dynamics of the postsynaptic calcium alone determine the outcome of synaptic plasticity. Here, we propose a calcium-based model of a synapse in which potentiation and depression are activated above calcium thresholds. We show that this model gives rise to a large diversity of spike timing-dependent plasticity curves, most of which have been observed experimentally in different systems. It accounts quantitatively for plasticity outcomes evoked by protocols involving patterns with variable spike timing and firing rate in hippocampus and neocortex. Furthermore, it allows us to predict that differences in plasticity outcomes in different studies are due to differences in parameters defining the calcium dynamics. The model provides a mechanistic understanding of how various stimulation protocols provoke specific synaptic changes through the dynamics of calcium concentration and thresholds implementing in simplified fashion protein signaling cascades, leading to long-term potentiation and long-term depression. The combination of biophysical realism and analytical tractability makes it the ideal candidate to study plasticity at the synapse, neuron, and network levels.},
  number = {10},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  date = {2012-03-06},
  pages = {3991-3996},
  keywords = {Calcium,Calcium Signaling,Computer Simulation,Dendrites,Hippocampus,Humans,Long-Term Potentiation,Long-Term Synaptic Depression,Models; Biological,Models; Statistical,Neocortex,Neuronal Plasticity,Synapses,Synaptic Transmission},
  author = {Graupner, Michael and Brunel, Nicolas},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Graupner_Brunel_2012_Calcium-based_plasticity_model_explains_sensitivity_of_synaptic_changes_to.pdf},
  eprinttype = {pmid},
  eprint = {22357758},
  pmcid = {PMC3309784}
}

@article{cuturi1987independent,
  langid = {english},
  title = {Independent Regulation of Tumor Necrosis Factor and Lymphotoxin Production by Human Peripheral Blood Lymphocytes.},
  volume = {165},
  issn = {0022-1007, 1540-9538},
  url = {http://jem.rupress.org/content/165/6/1581},
  doi = {10.1084/jem.165.6.1581},
  abstract = {We present evidence that human peripheral blood lymphocytes, free of contaminating monocytes, rapidly produce high levels of tumor necrosis factor (TNF) when stimulated with phorbol diester and calcium ionophore, and lower but significant levels of TNF when stimulated with mitogens. These two types of inducers act preferentially on T cells, both CD4+ and CD8+. NK cells produce TNF only when stimulated with phorbol diester and calcium ionophore, and they do so at a much lower level than T cells. The procedures used in the purification of lymphocytes and the differential ability to respond to various inducers allow us to exclude that monocytes or basophils contaminating the lymphocyte preparation participate in the production of TNF. In particular, LPS, a potent inducer of TNF production from monocytes, is unable to induce significant levels of TNF in the lymphocyte preparations. The TNF produced by lymphocytes has antigenic, physicochemical, and biochemical characteristics identical to those of the TNF produced by myeloid cell lines or monocytes upon stimulation with LPS. LT is also produced by lymphocyte preparations. Production of TNF and LT proteins in response to the different inducers is paralleled by accumulation of cytoplasmic TNF and LT mRNA. Both at mRNA and at protein levels, stimulation of T lymphocytes with phorbol diester and calcium ionophore preferentially induces TNF, whereas mitogen stimulation preferentially induces LT. Our data suggest that the TNF and LT genes, two closely linked genes encoding two partially homologous proteins with almost identical biological functions, are independently regulated in lymphocytes.},
  number = {6},
  journaltitle = {Journal of Experimental Medicine},
  urldate = {2018-02-14},
  date = {1987-06-01},
  pages = {1581-1594},
  author = {Cuturi, M. C. and Murphy, M. and Costa-Giomi, M. P. and Weinmann, R. and Perussia, B. and Trinchieri, G.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/5VUDV93Z/1581.html},
  eprinttype = {pmid},
  eprint = {3108447}
}

@book{strichartz2003guide,
  title = {A Guide to Distribution Theory and {{Fourier}} Transforms},
  publisher = {{World Scientific Publishing Company}},
  date = {2003},
  author = {Strichartz, Robert S.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Strichartz_2003_A_guide_to_distribution_theory_and_Fourier_transforms.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/FUDRZ8QB/books.html}
}

@book{tuckwell2005introduction,
  langid = {english},
  title = {Introduction to {{Theoretical Neurobiology}}: {{Volume}} 2, {{Nonlinear}} and {{Stochastic Theories}}},
  isbn = {978-0-521-01932-3},
  shorttitle = {Introduction to {{Theoretical Neurobiology}}},
  abstract = {The second part of this two-volume set contains advanced aspects of the quantitative theory of the dynamics of neurons. It begins with an introduction to the effects of reversal potentials on response to synaptic input. It then develops the theory of action potential generation based on the seminal Hodgkin-Huxley equations and gives methods for their solution in the space-clamped and nonspaceclamped cases. The remainder of the book discusses stochastic models of neural activity and ends with a statistical analysis of neuronal data with emphasis on spike trains. The mathematics is more complex in this volume than in the first volume and involves numerical methods of solution of partial differential equations and the statistical analysis of point processes.},
  pagetotal = {279},
  publisher = {{Cambridge University Press}},
  date = {2005-09-08},
  keywords = {Mathematics / Applied,Science / Life Sciences / Biology,Science / Life Sciences / Neuroscience},
  author = {Tuckwell, Henry C.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Tuckwell_2005_Introduction_to_Theoretical_Neurobiology_-_Volume_2,_Nonlinear_and_Stochastic.pdf},
  eprinttype = {googlebooks}
}

@book{tuckwell1988introduction,
  langid = {english},
  title = {Introduction to {{Theoretical Neurobiology}}: {{Volume}} 1, {{Linear Cable Theory}} and {{Dendritic Structure}}},
  isbn = {978-0-521-35096-9},
  shorttitle = {Introduction to {{Theoretical Neurobiology}}},
  abstract = {The human brain contains billions of nerve cells whose activity plays a critical role in the way we behave, feel, perceive, and think. This two-volume set explains the basic properties of a neuron--an electrically active nerve cell--and develops mathematical theories for the way neurons respond to the various stimuli they receive. Volume 1 contains descriptions and analyses of the principle mathematical models that have been developed for neurons in the past thirty years. It provides a brief review of the basic neuroanatomical and neurophysiological facts that will form the focus of the mathematical treatment. Tuckwell discusses the mathematical theories, beginning with the theory of membrane potentials. He then goes on to treat the Lapicque model, linear cable theory, and time-dependent solutions of the cable equations. He concludes with a description of Rall's model nerve cell. Because the level of mathematics increases steadily upward from Chapter Two some familiarity with differential equations and linear algebra is desirable.},
  pagetotal = {307},
  publisher = {{Cambridge University Press}},
  date = {1988-04-29},
  keywords = {Mathematics / Applied,Science / Life Sciences / Neuroscience,Medical / Physiology},
  author = {Tuckwell, Henry C.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Tuckwell_1988_Introduction_to_Theoretical_Neurobiology_-_Volume_1,_Linear_Cable_Theory_and.pdf},
  eprinttype = {googlebooks}
}

@book{mallat1999wavelet,
  title = {A Wavelet Tour of Signal Processing},
  publisher = {{Academic press}},
  date = {1999},
  author = {Mallat, Stéphane},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Mallat_1999_A_wavelet_tour_of_signal_processing.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2BGJ6LJY/books.html;/nfs/nhome/live/ilyesk/Zotero/storage/8256AILT/books.html}
}

@book{giraud2014introduction,
  langid = {english},
  title = {Introduction to {{High}}-{{Dimensional Statistics}}},
  isbn = {978-1-4822-3794-8},
  abstract = {Ever-greater computing technologies have given rise to an exponentially growing volume of data. Today massive data sets (with potentially thousands of variables) play an important role in almost every branch of modern human activity, including networks, finance, and genetics. However, analyzing such data has presented a challenge for statisticians and data analysts and has required the development of new statistical methods capable of separating the signal from the noise. Introduction to High-Dimensional Statistics is a concise guide to state-of-the-art models, techniques, and approaches for handling high-dimensional data. The book is intended to expose the reader to the key concepts and ideas in the most simple settings possible while avoiding unnecessary technicalities.  Offering a succinct presentation of the mathematical foundations of high-dimensional statistics, this highly accessible text:  Describes the challenges related to the analysis of high-dimensional data Covers cutting-edge statistical methods including model selection, sparsity and the lasso, aggregation, and learning theory Provides detailed exercises at the end of every chapter with collaborative solutions on a wikisite Illustrates concepts with simple but clear practical examples Introduction to High-Dimensional Statistics is suitable for graduate students and researchers interested in discovering modern statistics for massive data. It can be used as a graduate text or for self-study.},
  pagetotal = {270},
  publisher = {{Taylor \& Francis}},
  date = {2014-12-17},
  keywords = {Business & Economics / Statistics,Computers / Machine Theory,Mathematics / Probability & Statistics / General,Technology & Engineering / Automation},
  author = {Giraud, Christophe},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Giraud_2014_Introduction_to_High-Dimensional_Statistics.pdf},
  eprinttype = {googlebooks}
}

@article{figalli2010optimal,
  langid = {english},
  title = {The {{Optimal Partial Transport Problem}}},
  volume = {195},
  issn = {0003-9527, 1432-0673},
  url = {https://link.springer.com/article/10.1007/s00205-008-0212-7},
  doi = {10.1007/s00205-008-0212-7},
  abstract = {Given two densities f and g, we consider the problem of transporting a fraction \{m \textbackslash{}in [0,\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\}m∈[0,min\{‖f‖L1,‖g‖L1\}]\{m \textbackslash{}in [0,\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\} of the mass of f onto g minimizing a transportation cost. If the cost per unit of mass is given by |x − y|2, we will see that uniqueness of solutions holds for \{m \textbackslash{}in [\textbackslash{}|f\textbackslash{}wedge g\textbackslash{}|\_\{L\^1\},\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\}\{m \textbackslash{}in [\textbackslash{}|f\textbackslash{}wedge g\textbackslash{}|\_\{L\^1\},\textbackslash{}min\textbackslash\{\textbackslash{}|f\textbackslash{}|\_\{L\^1\},\textbackslash{}|g\textbackslash{}|\_\{L\^1\}\textbackslash\}]\} . This extends the result of Caffarelli and McCann in Ann Math (in print), where the authors consider two densities with disjoint supports. The free boundaries of the active regions are shown to be (n − 1)-rectifiable (provided the supports of f and g have Lipschitz boundaries), and under some weak regularity assumptions on the geometry of the supports they are also locally semiconvex. Moreover, assuming f and g supported on two bounded strictly convex sets \{\{\textbackslash{}Omega,\textbackslash{}Lambda \textbackslash{}subset \textbackslash{}mathbb \{R\}\^n\}\}\{\{\textbackslash{}Omega,\textbackslash{}Lambda \textbackslash{}subset \textbackslash{}mathbb \{R\}\^n\}\} , and bounded away from zero and infinity on their respective supports, \{C\^\{0,\textbackslash{}alpha\}\_\{\textbackslash{}rm loc\}\}\{C\^\{0,\textbackslash{}alpha\}\_\{\textbackslash{}rm loc\}\} regularity of the optimal transport map and local C1 regularity of the free boundaries away from \{\{\textbackslash{}Omega\textbackslash{}cap \textbackslash{}Lambda\}\}\{\{\textbackslash{}Omega\textbackslash{}cap \textbackslash{}Lambda\}\} are shown. Finally, the optimal transport map extends to a global homeomorphism between the active regions.},
  number = {2},
  journaltitle = {Archive for Rational Mechanics and Analysis},
  shortjournal = {Arch Rational Mech Anal},
  urldate = {2017-07-03},
  date = {2010-02-01},
  pages = {533-560},
  author = {Figalli, Alessio},
  file = {/Users/ilyes/Dropbox/Zotero/OT/Figalli_2010_The_Optimal_Partial_Transport_Problem.pdf;/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Figalli_2010_The_Optimal_Partial_Transport_Problem.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/DFI5M3DH/10.html;/nfs/nhome/live/ilyesk/Zotero/storage/TH2AQRES/s00205-008-0212-7.html}
}

@article{shadlen1996computational,
  langid = {english},
  title = {A Computational Analysis of the Relationship between Neuronal and Behavioral Responses to Visual Motion},
  volume = {16},
  issn = {0270-6474, 1529-2401},
  url = {http://www.jneurosci.org/content/16/4/1486},
  abstract = {We have documented previously a close relationship between neuronal activity in the middle temporal visual area (MT or V5) and behavioral judgments of motion (Newsome et al., 1989; Salzman et al., 1990; Britten et al., 1992; Britten et al., 1996). We have now used numerical simulations to try to understand how neural signals in area MT support psychophysical decisions. We developed a model that pools neuronal responses drawn from our physiological data set and compares average responses in different pools to produce psychophysical decisions. The structure of the model allows us to assess the relationship between “neuronal” input signals and simulated psychophysical performance using the same methods we have applied to real experimental data. We sought to reconcile three experimental observations: psychophysical performance (threshold sensitivity to motion stimuli embedded in noise), a trial-by-trial covariation between the neural response and the monkey's choices, and a modest correlation between pairs of MT neurons in their variable responses to identical visual stimuli. Our results can be most accurately simulated if psychophysical decisions are based on pools of at least 100 weakly correlated sensory neurons. The neurons composing the pools must include a broader range of sensitivities than we encountered in our MT recordings, presumably because of the inclusion of neurons whose optimal stimulus is different from the one being discriminated. Central sources of noise degrade the signal-to-noise ratio of the pooled signal, but this degradation is relatively small compared with the noise typically carried by single cortical neurons. This suggests that our monkeys base near-threshold psychophysical judgments on signals carried by populations of weakly interacting neurons; these populations include many neurons that are not tuned optimally for the particular stimuli being discriminated.},
  number = {4},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  urldate = {2017-11-07},
  date = {1996-02-15},
  pages = {1486-1510},
  author = {Shadlen, M. N. and Britten, K. H. and Newsome, W. T. and Movshon, J. A.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Shadlen_et_al_1996_A_computational_analysis_of_the_relationship_between_neuronal_and_behavioral.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/8F3JITDW/1486.html;/nfs/nhome/live/ilyesk/Zotero/storage/J5W4HVQQ/1486.html},
  eprinttype = {pmid},
  eprint = {8778300}
}

@book{mallot2013computational,
  langid = {english},
  title = {Computational {{Neuroscience}}: {{A First Course}}},
  isbn = {978-3-319-00861-5},
  shorttitle = {Computational {{Neuroscience}}},
  abstract = {Computational Neuroscience - A First Course provides an essential introduction to computational neuroscience and equips readers with a fundamental understanding of modeling the nervous system at the membrane, cellular, and network level. The book, which grew out of a lecture series held regularly for more than ten years to graduate students in neuroscience with backgrounds in biology, psychology and medicine, takes its readers on a journey through three fundamental domains of computational neuroscience: membrane biophysics, systems theory and artificial neural networks. The required mathematical concepts are kept as intuitive and simple as possible throughout the book, making it fully accessible to readers who are less familiar with mathematics. Overall, Computational Neuroscience - A First Course represents an essential reference guide for all neuroscientists who use computational methods in their daily work, as well as for any theoretical scientist approaching the field of computational neuroscience.},
  pagetotal = {142},
  publisher = {{Springer Science \& Business Media}},
  date = {2013-05-23},
  keywords = {Science / Life Sciences / Neuroscience,biophysics,Computers / Intelligence (AI) & Semantics,hodgkin huxley,Language Arts & Disciplines / Library & Information Science / General,Medical / Neuroscience,sntn,Technology & Engineering / General},
  author = {Mallot, Hanspeter A.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Mallot_2013_Computational_Neuroscience_-_A_First_Course.pdf},
  eprinttype = {googlebooks}
}

@article{emde1999active,
  langid = {english},
  title = {Active Electrolocation of Objects in Weakly Electric Fish},
  volume = {202},
  issn = {0022-0949, 1477-9145},
  url = {http://jeb.biologists.org/content/202/10/1205},
  abstract = {Skip to Next Section
Weakly electric fish produce electric signals (electric organ discharges, EODs) with a specialised electric organ creating an electric field around their body. Objects within this field alter the EOD-induced current at epidermal electroreceptor organs, which are distributed over almost the entire body surface. The detection, localisation and analysis of objects performed by monitoring self-produced electric signals is called active electrolocation. Electric fish employ active electrolocation to detect objects that are less than 12 cm away and have electric properties that are different from those of the surrounding water. Within this range, the mormyrid Gnathonemus petersii can also perceive the distance of objects. Depth perception is independent of object parameters such as size, shape and material. The mechanism for distance determination through electrolocation involves calculating the ratio between two parameters of the electric image that the object projects onto the fish's skin. Electric fish can not only locate objects but can also analyse their electrical properties. Fish are informed about object impedance by measuring local amplitude changes at their receptor organs evoked by an object. In addition, all electric fish studied so far can independently determine the capacitative and resistive components of objects that possess complex impedances. This ability allows the fish to discriminate between living and non-living matter, because capacitance is a property of living organisms. African mormyrids and South American gymnotiforms use different mechanisms for capacitance detection. Mormyrids detect capacitance-evoked EOD waveform distortions, whereas gymnotiforms perform time measurements. Gymnotiforms measure the temporal phase shift of their EODs induced at body parts close to the object relative to unaffected body parts further away.},
  number = {10},
  journaltitle = {Journal of Experimental Biology},
  urldate = {2017-12-07},
  date = {1999-05-15},
  pages = {1205-1215},
  author = {von der Emde, G.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Emde_1999_Active_electrolocation_of_objects_in_weakly_electric_fish.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/DIPFEDCE/1205.html},
  eprinttype = {pmid},
  eprint = {10210662}
}

@article{hinton2014where,
  langid = {english},
  title = {Where {{Do Features Come From}}?},
  volume = {38},
  issn = {1551-6709},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12049/abstract},
  doi = {10.1111/cogs.12049},
  abstract = {It is possible to learn multiple layers of non-linear features by backpropagating error derivatives through a feedforward neural network. This is a very effective learning procedure when there is a huge amount of labeled training data, but for many learning tasks very few labeled examples are available. In an effort to overcome the need for labeled data, several different generative models were developed that learned interesting features by modeling the higher order statistical structure of a set of input vectors. One of these generative models, the restricted Boltzmann machine (RBM), has no connections between its hidden units and this makes perceptual inference and learning much simpler. More significantly, after a layer of hidden features has been learned, the activities of these features can be used as training data for another RBM. By applying this idea recursively, it is possible to learn a deep hierarchy of progressively more complicated features without requiring any labeled data. This deep hierarchy can then be treated as a feedforward neural network which can be discriminatively fine-tuned using backpropagation. Using a stack of RBMs to initialize the weights of a feedforward neural network allows backpropagation to work effectively in much deeper networks and it leads to much better generalization. A stack of RBMs can also be used to initialize a deep Boltzmann machine that has many hidden layers. Combining this initialization method with a new method for fine-tuning the weights finally leads to the first efficient way of training Boltzmann machines with many hidden layers and millions of weights.},
  number = {6},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  urldate = {2017-12-05},
  date = {2014-08-01},
  pages = {1078-1101},
  keywords = {Backpropagation,Boltzmann machines,Contrastive divergence,Deep learning,Distributed representations,Learning features,Learning graphical models,Variational learning},
  author = {Hinton, Geoffrey},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hinton_2014_Where_Do_Features_Come_From.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/NHKDT3U8/abstract.html}
}

@article{hinton2007learning,
  title = {Learning Multiple Layers of Representation},
  volume = {11},
  issn = {1364-6613},
  url = {http://www.sciencedirect.com/science/article/pii/S1364661307002173},
  doi = {10.1016/j.tics.2007.09.004},
  abstract = {To achieve its impressive performance in tasks such as speech perception or object recognition, the brain extracts multiple levels of representation from the sensory input. Backpropagation was the first computationally efficient model of how neural networks could learn multiple layers of representation, but it required labeled training data and it did not work well in deep networks. The limitations of backpropagation learning can now be overcome by using multilayer neural networks that contain top-down connections and training them to generate sensory data rather than to classify it. Learning multilayer generative models might seem difficult, but a recent discovery makes it easy to learn nonlinear distributed representations one layer at a time.},
  number = {10},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  urldate = {2017-12-05},
  date = {2007-10-01},
  pages = {428-434},
  author = {Hinton, Geoffrey E.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hinton_2007_Learning_multiple_layers_of_representation.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/WM8HE9LD/S1364661307002173.html}
}

@article{hinton2006reducing,
  langid = {english},
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  volume = {313},
  issn = {0036-8075, 1095-9203},
  url = {http://science.sciencemag.org/content/313/5786/504},
  doi = {10.1126/science.1127647},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
  number = {5786},
  journaltitle = {Science},
  urldate = {2017-12-05},
  date = {2006-07-28},
  pages = {504-507},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hinton_Salakhutdinov_2006_Reducing_the_Dimensionality_of_Data_with_Neural_Networks.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/ER4RMP2M/504.html},
  eprinttype = {pmid},
  eprint = {16873662}
}

@article{lecun2015deep,
  langid = {english},
  title = {Deep Learning},
  volume = {521},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/nature14539},
  doi = {10.1038/nature14539},
  abstract = {Deep learning},
  number = {7553},
  journaltitle = {Nature},
  urldate = {2017-12-05},
  date = {2015-05},
  pages = {436},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/LeCun_et_al_2015_Deep_learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/I5PRJDMM/nature14539.html}
}

@article{wolpert2000computational,
  langid = {english},
  title = {Computational Principles of Movement Neuroscience},
  volume = {3},
  issn = {1546-1726},
  url = {https://www.nature.com/articles/nn1100_1212},
  doi = {10.1038/81497},
  abstract = {Computational principles of movement neuroscience},
  issue = {11s},
  journaltitle = {Nature Neuroscience},
  urldate = {2017-12-05},
  date = {2000-11-01},
  pages = {1212},
  author = {Wolpert, Daniel M. and Ghahramani, Zoubin},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Wolpert_Ghahramani_2000_Computational_principles_of_movement_neuroscience.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/982UK8Z5/nn1100_1212.html}
}

@article{paul2014why,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6621},
  primaryClass = {cs, stat},
  title = {Why Does {{Deep Learning}} Work? - {{A}} Perspective from {{Group Theory}}},
  url = {http://arxiv.org/abs/1412.6621},
  shorttitle = {Why Does {{Deep Learning}} Work?},
  abstract = {Why does Deep Learning work? What representations does it capture? How do higher-order representations emerge? We study these questions from the perspective of group theory, thereby opening a new approach towards a theory of Deep learning. One factor behind the recent resurgence of the subject is a key algorithmic step called pre-training: first search for a good generative model for the input samples, and repeat the process one layer at a time. We show deeper implications of this simple principle, by establishing a connection with the interplay of orbits and stabilizers of group actions. Although the neural networks themselves may not form groups, we show the existence of \{\textbackslash{}em shadow\} groups whose elements serve as close approximations. Over the shadow groups, the pre-training step, originally introduced as a mechanism to better initialize a network, becomes equivalent to a search for features with minimal orbits. Intuitively, these features are in a way the \{\textbackslash{}em simplest\}. Which explains why a deep learning network learns simple features first. Next, we show how the same principle, when repeated in the deeper layers, can capture higher order representations, and why representation complexity increases as the layers get deeper.},
  urldate = {2017-11-20},
  date = {2014-12-20},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Paul, Arnab and Venkatasubramanian, Suresh},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/CKLNKUJA/Paul_Venkatasubramanian_2014_Why_does_Deep_Learning_work.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/ZTA3JABN/1412.html}
}

@article{altschuler2017nearlinear,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.09634},
  primaryClass = {cs, stat},
  title = {Near-Linear Time Approximation Algorithms for Optimal Transport via {{Sinkhorn}} Iteration},
  url = {http://arxiv.org/abs/1705.09634},
  abstract = {Computing optimal transport distances such as the earth mover's distance is a fundamental problem in machine learning, statistics, and computer vision. Despite the recent introduction of several algorithms with good empirical performance, it is unknown whether general optimal transport distances can be approximated in near-linear time. This paper demonstrates that this ambitious goal is in fact achieved by Cuturi's Sinkhorn Distances, and provides guidance towards parameter tuning for this algorithm. This result relies on a new analysis of Sinkhorn iterations that also directly suggests a new algorithm Greenkhorn with the same theoretical guarantees. Numerical simulations illustrate that Greenkhorn significantly outperforms the classical Sinkhorn algorithm in practice.},
  urldate = {2017-11-20},
  date = {2017-05-26},
  keywords = {Statistics - Machine Learning,Computer Science - Data Structures and Algorithms},
  author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Altschuler_et_al_2017_Near-linear_time_approximation_algorithms_for_optimal_transport_via_Sinkhorn.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/6JNNSIYU/1705.html}
}

@article{zhang1999neuronal,
  langid = {english},
  title = {Neuronal Tuning: {{To}} Sharpen or Broaden?},
  volume = {11},
  issn = {0899-7667},
  shorttitle = {Neuronal Tuning},
  abstract = {Sensory and motor variables are typically represented by a population of broadly tuned neurons. A coarser representation with broader tuning can often improve coding accuracy, but sometimes the accuracy may also improve with sharper tuning. The theoretical analysis here shows that the relationship between tuning width and accuracy depends crucially on the dimension of the encoded variable. A general rule is derived for how the Fisher information scales with the tuning width, regardless of the exact shape of the tuning function, the probability distribution of spikes, and allowing some correlated noise between neurons. These results demonstrate a universal dimensionality effect in neural population coding.},
  number = {1},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Comput},
  date = {1999-01-01},
  pages = {75-84},
  keywords = {Artifacts,Electrophysiology,Models; Neurological,Neurons},
  author = {Zhang, K. and Sejnowski, T. J.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Zhang_Sejnowski_1999_Neuronal_tuning_-_To_sharpen_or_broaden.pdf},
  eprinttype = {pmid},
  eprint = {9950722}
}

@article{zylberberg2017robust,
  title = {Robust Information Propagation through Noisy Neural Circuits},
  volume = {13},
  issn = {1553-7358},
  url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005497},
  doi = {10.1371/journal.pcbi.1005497},
  abstract = {Author summary Information about the outside world, which originates in sensory neurons, propagates through multiple stages of processing before reaching the neural structures that control behavior. While much work in neuroscience has investigated the factors that affect the amount of information contained in peripheral sensory areas, very little work has asked how much of that information makes it through subsequent processing stages. That’s the focus of this paper, and it’s an important issue because information that fails to propagate cannot be used to affect decision-making. We find a tradeoff between information content and information transmission: neural codes which contain a large amount of information can transmit that information poorly to subsequent processing stages. Thus, the problem of robust information propagation—which has largely been overlooked in previous research—may be critical for determining how our sensory organs communicate with our brains. We identify the conditions under which information propagates well—or poorly—through multiple stages of neural processing.},
  number = {4},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  urldate = {2017-11-07},
  date = {2017-04-18},
  pages = {e1005497},
  keywords = {Neurons,Coding mechanisms,Covariance,Eigenvalues,Ellipses,Gaussian noise,Neural pathways,Neuronal tuning},
  author = {Zylberberg, Joel and Pouget, Alexandre and Latham, Peter E. and Shea-Brown, Eric},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Zylberberg_et_al_2017_Robust_information_propagation_through_noisy_neural_circuits.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/JXZ7V3RZ/article.html}
}

@article{ecker2011effect,
  langid = {english},
  title = {The Effect of Noise Correlations in Populations of Diversely Tuned Neurons},
  volume = {31},
  issn = {1529-2401},
  doi = {10.1523/JNEUROSCI.2539-11.2011},
  abstract = {The amount of information encoded by networks of neurons critically depends on the correlation structure of their activity. Neurons with similar stimulus preferences tend to have higher noise correlations than others. In homogeneous populations of neurons, this limited range correlation structure is highly detrimental to the accuracy of a population code. Therefore, reduced spike count correlations under attention, after adaptation, or after learning have been interpreted as evidence for a more efficient population code. Here, we analyze the role of limited range correlations in more realistic, heterogeneous population models. We use Fisher information and maximum-likelihood decoding to show that reduced correlations do not necessarily improve encoding accuracy. In fact, in populations with more than a few hundred neurons, increasing the level of limited range correlations can substantially improve encoding accuracy. We found that this improvement results from a decrease in noise entropy that is associated with increasing correlations if the marginal distributions are unchanged. Surprisingly, for constant noise entropy and in the limit of large populations, the encoding accuracy is independent of both structure and magnitude of noise correlations.},
  number = {40},
  journaltitle = {The Journal of Neuroscience: The Official Journal of the Society for Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2011-10-05},
  pages = {14272-14283},
  keywords = {Models; Neurological,Neurons,Action Potentials,Animals,Electricity,Random Allocation},
  author = {Ecker, Alexander S. and Berens, Philipp and Tolias, Andreas S. and Bethge, Matthias},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Ecker_et_al_2011_The_effect_of_noise_correlations_in_populations_of_diversely_tuned_neurons.pdf},
  eprinttype = {pmid},
  eprint = {21976512},
  pmcid = {PMC3221941}
}

@article{moreno-bote2014informationlimiting,
  langid = {english},
  title = {Information-Limiting Correlations},
  volume = {17},
  issn = {1097-6256},
  url = {https://www.nature.com/neuro/journal/v17/n10/full/nn.3807.html},
  doi = {10.1038/nn.3807},
  abstract = {Computational strategies used by the brain strongly depend on the amount of information that can be stored in population activity, which in turn strongly depends on the pattern of noise correlations. In vivo, noise correlations tend to be positive and proportional to the similarity in tuning properties. Such correlations are thought to limit information, which has led to the suggestion that decorrelation increases information. In contrast, we found, analytically and numerically, that decorrelation does not imply an increase in information. Instead, the only information-limiting correlations are what we refer to as differential correlations: correlations proportional to the product of the derivatives of the tuning curves. Unfortunately, differential correlations are likely to be very small and buried under correlations that do not limit information, making them particularly difficult to detect. We found, however, that the effect of differential correlations on information can be detected with relatively simple decoders.},
  number = {10},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  urldate = {2017-11-07},
  date = {2014-10},
  pages = {1410-1417},
  keywords = {Neural encoding},
  author = {Moreno-Bote, Rubén and Beck, Jeffrey and Kanitscheider, Ingmar and Pitkow, Xaq and Latham, Peter and Pouget, Alexandre},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Neurosciences/Moreno-Bote_et_al_2014_Information-limiting_correlations.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/MTBQDKD5/nn.3807.html}
}

@article{thetheanodevelopmentteam2016theano,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.02688},
  primaryClass = {cs},
  title = {Theano: {{A Python}} Framework for Fast Computation of Mathematical Expressions},
  url = {http://arxiv.org/abs/1605.02688},
  shorttitle = {Theano},
  abstract = {Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.},
  date = {2016-05-09},
  keywords = {Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation},
  author = {The Theano Development Team and Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Frédéric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and Snyder, Josh Bleecher and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Brébisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and Côté, Marc-Alexandre and Côté, Myriam and Courville, Aaron and Dauphin, Yann N. and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, Mélanie and Dumoulin, Vincent and Kahou, Samira Ebrahimi and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Balázs and Honari, Sina and Jain, Arjun and Jean, Sébastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, César and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and Léonard, Nicholas and Lin, Zhouhan and Livezey, Jesse A. and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T. and Memisevic, Roland and van Merriënboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, François and Schlüter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, Étienne and Spieckermann, Sigurd and Subramanyam, S. Ramana and Sygnowski, Jakub and Tanguay, Jérémie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J. and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
  options = {useprefix=true},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/BCUXES3C/The_Theano_Development_Team_et_al_2016_Theano.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/33ARWVJX/1605.html}
}

@article{arjovsky2017wasserstein,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07875},
  primaryClass = {cs, stat},
  title = {Wasserstein {{GAN}}},
  url = {http://arxiv.org/abs/1701.07875},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  date = {2017-01-26},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Arjovsky_et_al_2017_Wasserstein_GAN.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/82VXM5JT/1701.html}
}

@article{kuhn1955hungarian,
  langid = {english},
  title = {The {{Hungarian}} Method for the Assignment Problem},
  volume = {2},
  issn = {1931-9193},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/nav.3800020109/abstract},
  doi = {10.1002/nav.3800020109},
  abstract = {Assuming that numerical scores are available for the performance of each of n persons on each of n jobs, the “assignment problem” is the quest for an assignment of persons to jobs so that the sum of the n scores so obtained is as large as possible. It is shown that ideas latent in the work of two Hungarian mathematicians may be exploited to yield a new method of solving this problem.},
  number = {1-2},
  journaltitle = {Naval Research Logistics Quarterly},
  shortjournal = {Naval Research Logistics},
  date = {1955-03-01},
  pages = {83-97},
  author = {Kuhn, H. W.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/7PEIHZ76/abstract.html}
}

@article{schmitzer2016stabilized,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.06519},
  primaryClass = {cs, math},
  title = {Stabilized {{Sparse Scaling Algorithms}} for {{Entropy Regularized Transport Problems}}},
  url = {http://arxiv.org/abs/1610.06519},
  abstract = {Scaling algorithms for entropic transport-type problems have become a very popular numerical method, encompassing Wasserstein barycenters, multi-marginal problems, gradient flows and unbalanced transport. However, a standard implementation of the scaling algorithm has several numerical limitations: the scaling factors diverge and convergence becomes impractically slow as the entropy regularization approaches zero. Moreover, handling the dense kernel matrix becomes unfeasible for large problems. To address this, we propose several modifications: A log-domain stabilized formulation, the well-known epsilon-scaling heuristic, an adaptive truncation of the kernel and a coarse-to-fine scheme. This allows to solve larger problems with smaller regularization and negligible truncation error. A new convergence analysis of the Sinkhorn algorithm is developed, working towards a better understanding of epsilon-scaling. Numerical examples illustrate efficiency and versatility of the modified algorithm.},
  date = {2016-10-20},
  keywords = {Mathematics - Optimization and Control,Computer Science - Computational Engineering; Finance; and Science,Mathematics - Numerical Analysis},
  author = {Schmitzer, Bernhard},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/GM895QZ7/1610.html}
}

@article{paquette2017catalyst,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.10993},
  primaryClass = {math, stat},
  title = {Catalyst {{Acceleration}} for {{Gradient}}-{{Based Non}}-{{Convex Optimization}}},
  url = {http://arxiv.org/abs/1703.10993},
  abstract = {We introduce a generic scheme to solve nonconvex optimization problems using gradient-based algorithms originally designed for minimizing convex functions. When the objective is convex, the proposed approach enjoys the same properties as the Catalyst approach of Lin et al. [22]. When the objective is nonconvex, it achieves the best known convergence rate to stationary points for first-order methods. Specifically, the proposed algorithm does not require knowledge about the convexity of the objective; yet, it obtains an overall worst-case efficiency of \$\textbackslash{}tilde\{O\}(\textbackslash{}varepsilon\^\{-2\})\$ and, if the function is convex, the complexity reduces to the near-optimal rate \$\textbackslash{}tilde\{O\}(\textbackslash{}varepsilon\^\{-2/3\})\$. We conclude the paper by showing promising experimental results obtained by applying the proposed approach to SVRG and SAGA for sparse matrix factorization and for learning neural networks.},
  date = {2017-03-31},
  keywords = {Statistics - Machine Learning,Mathematics - Optimization and Control},
  author = {Paquette, Courtney and Lin, Hongzhou and Drusvyatskiy, Dmitriy and Mairal, Julien and Harchaoui, Zaid},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Paquette_et_al_2017_Catalyst_Acceleration_for_Gradient-Based_Non-Convex_Optimization.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/42EERBPB/1703.html}
}

@article{sinkhorn1967diagonal,
  eprinttype = {jstor},
  eprint = {2314570},
  title = {Diagonal {{Equivalence}} to {{Matrices}} with {{Prescribed Row}} and {{Column Sums}}},
  volume = {74},
  issn = {0002-9890},
  doi = {10.2307/2314570},
  number = {4},
  journaltitle = {The American Mathematical Monthly},
  date = {1967},
  pages = {402-405},
  author = {Sinkhorn, Richard},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sinkhorn_1967_Diagonal_Equivalence_to_Matrices_with_Prescribed_Row_and_Column_Sums.pdf}
}

@inproceedings{sakoe1971dynamic,
  title = {A Dynamic Programming Approach to Continuous Speech Recognition},
  volume = {3},
  url = {http://scholar.google.com/scholar?cluster=8386804209821554915&hl=en&oi=scholarr},
  booktitle = {Proceedings of the Seventh International Congress on Acoustics},
  publisher = {{Budapest, Hungary}},
  date = {1971},
  pages = {65--69},
  author = {Sakoe, Hiroaki and Chiba, Seibi},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/A8G5CFFU/scholar.html}
}

@article{sakoe1978dynamic,
  title = {Dynamic Programming Algorithm Optimization for Spoken Word Recognition},
  volume = {26},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1978.1163055},
  abstract = {This paper reports on an optimum dynamic progxamming (DP) based time-normalization algorithm for spoken word recognition. First, a general principle of time-normalization is given using time-warping function. Then, two time-normalized distance definitions, called symmetric and asymmetric forms, are derived from the principle. These two forms are compared with each other through theoretical discussions and experimental studies. The symmetric form algorithm superiority is established. A new technique, called slope constraint, is successfully introduced, in which the warping function slope is restricted so as to improve discrimination between words in different categories. The effective slope constraint characteristic is qualitatively analyzed, and the optimum slope constraint condition is determined through experiments. The optimized algorithm is then extensively subjected to experimental comparison with various DP-algorithms, previously applied to spoken word recognition by different research groups. The experiment shows that the present algorithm gives no more than about two-thirds errors, even compared to the best conventional algorithm.},
  number = {1},
  journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  date = {1978-02},
  pages = {43-49},
  keywords = {Acoustics,Constraint optimization,Dynamic programming,Feature extraction,Fluctuations,Heuristic algorithms,Pattern matching,Signal processing algorithms,Speech processing,Timing},
  author = {Sakoe, Hiroaki and Chiba, Seibi},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sakoe_Chiba_1978_Dynamic_programming_algorithm_optimization_for_spoken_word_recognition.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/9J4HASBA/1163055.html}
}

@article{monge1781mémoire,
  title = {Mémoire Sur La Theorie Des Déblais et Des Remblais},
  url = {http://ci.nii.ac.jp/naid/10018386702/},
  journaltitle = {Histoire de l'Academie Royale des Sciences de Paris},
  date = {1781},
  author = {MONGE, G.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/ACMJKXQH/10018386702.html}
}

@book{hyvarinen2001independent,
  title = {Independent Component Analysis},
  url = {http://books.google.com/books?hl=en&lr=&id=96D0ypDwAkkC&oi=fnd&pg=PR5&dq=info:z5Vp898d6QIJ:scholar.google.com&ots=yOxgxyPnpI&sig=G-qJnwAdRwpSvBQQmm4w1JiUFvU},
  publisher = {{John Wiley \& Sons}},
  date = {2001},
  keywords = {ICA},
  author = {Hyvärinen, Aapo and Karhunen, Juha and Oja, Erkki},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Hyvärinen_et_al_2001_Independent_component_analysis.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/Q8APBQQH/books.html}
}

@article{cover1994elements,
  title = {Elements of Information Theory},
  volume = {36},
  url = {http://scholar.google.com/scholar?cluster=10887039674309886683&hl=en&oi=scholarr},
  number = {3},
  journaltitle = {SIAM Review},
  date = {1994},
  pages = {509--510},
  author = {Cover, Thomas M. and Thomas, Joy A. and Kieffer, John},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/IN5ZK9N6/scholar.html}
}

@article{sinkhorn1964relationship,
  eprinttype = {jstor},
  eprint = {2238545},
  title = {A {{Relationship Between Arbitrary Positive Matrices}} and {{Doubly Stochastic Matrices}}},
  volume = {35},
  issn = {0003-4851},
  number = {2},
  journaltitle = {The Annals of Mathematical Statistics},
  date = {1964},
  pages = {876-879},
  author = {Sinkhorn, Richard},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Sinkhorn_1964_A_Relationship_Between_Arbitrary_Positive_Matrices_and_Doubly_Stochastic.pdf}
}

@article{levy2015numerical,
  langid = {english},
  title = {A {{Numerical Algorithm}} for {{L2 Semi}}-{{Discrete Optimal Transport}} in {{3D}}},
  volume = {49},
  issn = {0764-583X, 1290-3841},
  url = {http://dx.doi.org/10.1051/m2an/2015055},
  doi = {10.1051/m2an/2015055},
  abstract = {ESAIM: Mathematical Modelling and Numerical Analysis, an international journal on applied mathematics},
  number = {6},
  journaltitle = {ESAIM: Mathematical Modelling and Numerical Analysis},
  shortjournal = {ESAIM: M2AN},
  urldate = {2017-07-18},
  date = {2015-11-01},
  pages = {1693-1715},
  author = {Lévy, Bruno},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/MVHJSKDT/m2an150137.html}
}

@article{schmitzer2016sparse,
  langid = {english},
  title = {A {{Sparse Multiscale Algorithm}} for {{Dense Optimal Transport}}},
  volume = {56},
  issn = {0924-9907, 1573-7683},
  url = {https://link.springer.com/article/10.1007/s10851-016-0653-9},
  doi = {10.1007/s10851-016-0653-9},
  abstract = {Discrete optimal transport solvers do not scale well on dense large problems since they do not explicitly exploit the geometric structure of the cost function. In analogy to continuous optimal transport, we provide a framework to verify global optimality of a discrete transport plan locally. This allows the construction of an algorithm to solve large dense problems by considering a sequence of sparse problems instead. The algorithm lends itself to being combined with a hierarchical multiscale scheme. Any existing discrete solver can be used as internal black-box. We explicitly describe how to select the sparse sub-problems for several cost functions, including the noisy squared Euclidean distance. Significant reductions in run-time and memory requirements have been observed.},
  number = {2},
  journaltitle = {Journal of Mathematical Imaging and Vision},
  shortjournal = {J Math Imaging Vis},
  urldate = {2017-07-18},
  date = {2016-10-01},
  pages = {238-259},
  author = {Schmitzer, Bernhard},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/EBQ52WIZ/s10851-016-0653-9.html}
}

@article{papadakis2014optimal,
  title = {Optimal {{Transport}} with {{Proximal Splitting}}},
  volume = {7},
  url = {http://epubs.siam.org/doi/abs/10.1137/130920058},
  doi = {10.1137/130920058},
  abstract = {This article reviews the use of first order convex optimization schemes to solve the discretized dynamic optimal transport problem, initially proposed by Benamou and Brenier. We develop a staggered grid discretization that is well adapted to the computation of the \$L\^2\$ optimal transport geodesic between distributions defined on a uniform spatial grid. We show how proximal splitting schemes can be used to solve the resulting large scale convex optimization problem. A specific instantiation of this method on a centered grid corresponds to the initial algorithm developed by Benamou and Brenier. We also show how more general cost functions can be taken into account and how to extend the method to perform optimal transport on a Riemannian manifold.},
  number = {1},
  journaltitle = {SIAM Journal on Imaging Sciences},
  shortjournal = {SIAM J. Imaging Sci.},
  date = {2014-01-01},
  pages = {212-238},
  author = {Papadakis, N. and Peyré, G. and Oudet, E.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/54WBIRQQ/130920058.html}
}

@article{benamou2000computational,
  langid = {english},
  title = {A Computational Fluid Mechanics Solution to the {{Monge}}-{{Kantorovich}} Mass Transfer Problem},
  volume = {84},
  issn = {0029-599X, 0945-3245},
  url = {https://link.springer.com/article/10.1007/s002110050002},
  doi = {10.1007/s002110050002},
  abstract = {Summary. The L2L2L\^2 Monge-Kantorovich mass transfer problem [31] is reset in a fluid mechanics framework and numerically solved by an augmented Lagrangian method.},
  number = {3},
  journaltitle = {Numerische Mathematik},
  shortjournal = {Numer. Math.},
  urldate = {2017-07-18},
  date = {2000-01-01},
  pages = {375-393},
  author = {Benamou, Jean-David and Brenier, Yann},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/TG9I9HID/10.html}
}

@article{aurenhammer1998minkowskitype,
  langid = {english},
  title = {Minkowski-{{Type Theorems}} and {{Least}}-{{Squares Clustering}}},
  volume = {20},
  issn = {0178-4617, 1432-0541},
  url = {https://link.springer.com/article/10.1007/PL00009187},
  doi = {10.1007/PL00009187},
  abstract = {.Dissecting Euclidean d -space with the power diagram of n weighted point sites partitions a given m -point set into clusters, one cluster for each region of the diagram. In this manner, an assignment of points to sites is induced. We show the equivalence of such assignments to constrained Euclidean least-squares assignments. As a corollary, there always exists a power diagram whose regions partition a given d -dimensional m -point set into clusters of prescribed sizes, no matter where the sites are placed. Another consequence is that constrained least-squares assignments can be computed by finding suitable weights for the sites. In the plane, this takes roughly O(n2m) time and optimal space O(m) , which improves on previous methods. We further show that a constrained least-squares assignment can be computed by solving a specially structured linear program in n+1 dimensions. This leads to an algorithm for iteratively improving the weights, based on the gradient-descent method. Besides having the obvious optimization property, least-squares assignments are shown to be useful in solving a certain transportation problem, and in finding a least-squares fitting of two point sets where translation and scaling are allowed. Finally, we extend the concept of a constrained least-squares assignment to continuous distributions of points, thereby obtaining existence results for power diagrams with prescribed region volumes. These results are related to Minkowski's theorem for convex polytopes. The aforementioned iterative method for approximating the desired power diagram applies to continuous distributions as well.},
  number = {1},
  journaltitle = {Algorithmica},
  shortjournal = {Algorithmica},
  urldate = {2017-07-18},
  date = {1998-01-01},
  pages = {61-76},
  author = {Aurenhammer, F. and Hoffmann, F. and Aronov, B.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/ZBQZV37J/10.html}
}

@inproceedings{pele2009fast,
  title = {Fast and Robust {{Earth Mover}}'s {{Distances}}},
  doi = {10.1109/ICCV.2009.5459199},
  abstract = {We present a new algorithm for a robust family of Earth Mover's Distances - EMDs with thresholded ground distances. The algorithm transforms the flow-network of the EMD so that the number of edges is reduced by an order of magnitude. As a result, we compute the EMD by an order of magnitude faster than the original algorithm, which makes it possible to compute the EMD on large histograms and databases. In addition, we show that EMDs with thresholded ground distances have many desirable properties. First, they correspond to the way humans perceive distances. Second, they are robust to outlier noise and quantization effects. Third, they are metrics. Finally, experimental results on image retrieval show that thresholding the ground distance of the EMD improves both accuracy and speed.},
  eventtitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  date = {2009-09},
  pages = {460-467},
  keywords = {Humans,computer vision,Costs,Earth,edge detection,flow-network,histograms,Image databases,Image edge detection,Image retrieval,outlier noise,Quantization,quantization effects,robust earth mover's distances,Robustness,thresholded ground distances},
  author = {Pele, O. and Werman, M.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/U93ZH9B6/5459199.html}
}

@article{cuturi2011ground,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1110.2306},
  primaryClass = {cs, stat},
  title = {Ground {{Metric Learning}}},
  url = {http://arxiv.org/abs/1110.2306},
  abstract = {Transportation distances have been used for more than a decade now in machine learning to compare histograms of features. They have one parameter: the ground metric, which can be any metric between the features themselves. As is the case for all parameterized distances, transportation distances can only prove useful in practice when this parameter is carefully chosen. To date, the only option available to practitioners to set the ground metric parameter was to rely on a priori knowledge of the features, which limited considerably the scope of application of transportation distances. We propose to lift this limitation and consider instead algorithms that can learn the ground metric using only a training set of labeled histograms. We call this approach ground metric learning. We formulate the problem of learning the ground metric as the minimization of the difference of two polyhedral convex functions over a convex set of distance matrices. We follow the presentation of our algorithms with promising experimental results on binary classification tasks using GIST descriptors of images taken in the Caltech-256 set.},
  date = {2011-10-11},
  keywords = {Computer Science - Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  author = {Cuturi, Marco and Avis, David},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Cuturi_Avis_2011_Ground_Metric_Learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/7PFMPSST/1110.html}
}

@article{hanin1992kantorovichrubinstein,
  eprinttype = {jstor},
  eprint = {2159251},
  title = {Kantorovich-{{Rubinstein Norm}} and {{Its Application}} in the {{Theory}} of {{Lipschitz Spaces}}},
  volume = {115},
  issn = {0002-9939},
  doi = {10.2307/2159251},
  abstract = {We obtain necessary and sufficient conditions on a compact metric space (K, ρ) that provide a natural isometric isomorphism between completion of the space of Borel measures on K with the Kantorovich-Rubinstein norm and the space \$(\textbackslash{}operatorname\{lip\}(K, \textbackslash{}rho))\^\textbackslash{}ast\$ or equivalently between the spaces \$\textbackslash{}operatorname\{Lip\}(K, \textbackslash{}rho)\$ and \$(\textbackslash{}operatorname\{lip\}(K, \textbackslash{}rho))\^\{\textbackslash{}ast\textbackslash{}ast\}\$. Such metric spaces are studied and related properties of Lipschitz spaces are established.},
  number = {2},
  journaltitle = {Proceedings of the American Mathematical Society},
  date = {1992},
  pages = {345-352},
  author = {Hanin, Leonid G.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Hanin_1992_Kantorovich-Rubinstein_Norm_and_Its_Application_in_the_Theory_of_Lipschitz.pdf}
}

@article{liero2015optimal,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1508.07941},
  primaryClass = {math},
  title = {Optimal {{Entropy}}-{{Transport}} Problems and a New {{Hellinger}}-{{Kantorovich}} Distance between Positive Measures},
  url = {http://arxiv.org/abs/1508.07941},
  abstract = {We develop a full theory for the new class of Optimal Entropy-Transport problems between nonnegative and finite Radon measures in general topological spaces. They arise quite naturally by relaxing the marginal constraints typical of Optimal Transport problems: given a couple of finite measures (with possibly different total mass), one looks for minimizers of the sum of a linear transport functional and two convex entropy functionals, that quantify in some way the deviation of the marginals of the transport plan from the assigned measures. As a powerful application of this theory, we study the particular case of Logarithmic Entropy-Transport problems and introduce the new Hellinger-Kantorovich distance between measures in metric spaces. The striking connection between these two seemingly far topics allows for a deep analysis of the geometric properties of the new geodesic distance, which lies somehow between the well-known Hellinger-Kakutani and Kantorovich-Wasserstein distances.},
  date = {2015-08-31},
  keywords = {Mathematics - Optimization and Control},
  author = {Liero, Matthias and Mielke, Alexander and Savaré, Giuseppe},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Liero_et_al_2015_Optimal_Entropy-Transport_problems_and_a_new_Hellinger-Kantorovich_distance.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/94VGF6IN/1508.html}
}

@inproceedings{cuturi2004mutual,
  title = {A Mutual Information Kernel for Sequences},
  volume = {3},
  url = {http://ieeexplore.ieee.org/abstract/document/1380902/},
  booktitle = {Neural {{Networks}}, 2004. {{Proceedings}}. 2004 {{IEEE International Joint Conference}} On},
  publisher = {{IEEE}},
  date = {2004},
  pages = {1905--1910},
  author = {Cuturi, Marco and Vert, J.-P.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Cuturi_Vert_2004_A_mutual_information_kernel_for_sequences.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/IFVVXS25/1380902.html}
}

@book{gowers2002mathematics,
  langid = {Anglais},
  location = {{Oxford ; New York}},
  title = {Mathematics: {{A Very Short Introduction}}},
  isbn = {978-0-19-285361-5},
  shorttitle = {Mathematics},
  abstract = {The aim of this book is to explain, carefully but not technically, the differences between advanced, research-level mathematics, and the sort of mathematics we learn at school. The most fundamental differences are philosophical, and readers of this book will emerge with a clearer understanding of paradoxical-sounding concepts such as infinity, curved space, and imaginary numbers. The first few chapters are about general aspects of mathematical thought. These are followed by discussions of more specific topics, and the book closes with a chapter answering common sociological questions about the mathematical community (such as "Is it true that mathematicians burn out at the age of 25?")  ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
  pagetotal = {160},
  publisher = {{OUP Oxford}},
  date = {2002-08-22},
  author = {Gowers, Timothy},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Gowers_2002_Mathematics_-_A_Very_Short_Introduction.pdf}
}

@article{maddy1993does,
  eprinttype = {jstor},
  eprint = {2275321},
  title = {Does {{V}}. {{Equal L}}?},
  volume = {58},
  issn = {0022-4812},
  doi = {10.2307/2275321},
  number = {1},
  journaltitle = {The Journal of Symbolic Logic},
  date = {1993},
  pages = {15-41},
  author = {Maddy, Penelope},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/3JV3HXF5/Maddy_1993_Does_V.pdf}
}

@article{knight2008sinkhornknopp,
  title = {The {{Sinkhorn}}–{{Knopp Algorithm}}: {{Convergence}} and {{Applications}}},
  volume = {30},
  issn = {0895-4798},
  url = {http://epubs.siam.org/doi/abs/10.1137/060659624},
  doi = {10.1137/060659624},
  shorttitle = {The {{Sinkhorn}}–{{Knopp Algorithm}}},
  abstract = {As long as a square nonnegative matrix A contains sufficient nonzero elements, then the Sinkhorn–Knopp algorithm can be used to balance the matrix, that is, to find a diagonal scaling of A that is doubly stochastic. It is known that the convergence is linear, and an upper bound has been given for the rate of convergence for positive matrices. In this paper we give an explicit expression for the rate of convergence for fully indecomposable matrices. We describe how balancing algorithms can be used to give a measure of web page significance. We compare the measure with some well known alternatives, including PageRank. We show that, with an appropriate modification, the Sinkhorn–Knopp algorithm is a natural candidate for computing the measure on enormous data sets.},
  number = {1},
  journaltitle = {SIAM Journal on Matrix Analysis and Applications},
  shortjournal = {SIAM. J. Matrix Anal. \& Appl.},
  date = {2008-01-01},
  pages = {261-275},
  author = {Knight, P.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Knight_2008_The_Sinkhorn–Knopp_Algorithm_-_Convergence_and_Applications.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/HPUAR9NP/060659624.html}
}

@article{hare2016struck,
  title = {Struck: {{Structured Output Tracking}} with {{Kernels}}},
  volume = {38},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2015.2509974},
  shorttitle = {Struck},
  abstract = {Adaptive tracking-by-detection methods are widely used in computer vision for tracking arbitrary objects. Current approaches treat the tracking problem as a classification task and use online learning techniques to update the object model. However, for these updates to happen one needs to convert the estimated object position into a set of labelled training examples, and it is not clear how best to perform this intermediate step. Furthermore, the objective for the classifier (label prediction) is not explicitly coupled to the objective for the tracker (estimation of object position). In this paper, we present a framework for adaptive visual object tracking based on structured output prediction. By explicitly allowing the output space to express the needs of the tracker, we avoid the need for an intermediate classification step. Our method uses a kernelised structured output support vector machine (SVM), which is learned online to provide adaptive tracking. To allow our tracker to run at high frame rates, we (a) introduce a budgeting mechanism that prevents the unbounded growth in the number of support vectors that would otherwise occur during tracking, and (b) show how to implement tracking on the GPU. Experimentally, we show that our algorithm is able to outperform state-of-the-art trackers on various benchmark videos. Additionally, we show that we can easily incorporate additional features and kernels into our framework, which results in increased tracking performance.},
  number = {10},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {2016-10},
  pages = {2096-2109},
  keywords = {computer vision,Adaptation models,adaptive tracking-by-detection methods,adaptive visual object tracking,budget maintenance,GPU-based tracking,Graphics processing units,intermediate classification step,Kernel,kernelised structured output support vector machine,object detection,object tracking,online learning techniques,structured output prediction,structured output SVMs,structured output tracking with kernels,support vector machines,SVM,Target tracking,Tracking-by-detection,Training},
  author = {Hare, S. and Golodetz, S. and Saffari, A. and Vineet, V. and Cheng, M. M. and Hicks, S. L. and Torr, P. H. S.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/EXD2U7JC/7360205.html}
}

@article{tsochantaridis2005large,
  title = {Large {{Margin Methods}} for {{Structured}} and {{Interdependent Output Variables}}},
  volume = {6},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v6/tsochantaridis05a.html},
  issue = {Sep},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2017-07-04},
  date = {2005},
  pages = {1453-1484},
  author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Tsochantaridis_et_al_2005_Large_Margin_Methods_for_Structured_and_Interdependent_Output_Variables.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/DWTSAA7Q/tsochantaridis05a.html}
}

@article{kocev2013tree,
  title = {Tree Ensembles for Predicting Structured Outputs},
  volume = {46},
  issn = {0031-3203},
  url = {http://www.sciencedirect.com/science/article/pii/S003132031200430X},
  doi = {10.1016/j.patcog.2012.09.023},
  abstract = {In this paper, we address the task of learning models for predicting structured outputs. We consider both global and local predictions of structured outputs, the former based on a single model that predicts the entire output structure and the latter based on a collection of models, each predicting a component of the output structure. We use ensemble methods and apply them in the context of predicting structured outputs. We propose to build ensemble models consisting of predictive clustering trees, which generalize classification trees: these have been used for predicting different types of structured outputs, both locally and globally. More specifically, we develop methods for learning two types of ensembles (bagging and random forests) of predictive clustering trees for global and local predictions of different types of structured outputs. The types of outputs considered correspond to different predictive modeling tasks: multi-target regression, multi-target classification, and hierarchical multi-label classification. Each of the combinations can be applied both in the context of global prediction (producing a single ensemble) or local prediction (producing a collection of ensembles). We conduct an extensive experimental evaluation across a range of benchmark datasets for each of the three types of structured outputs. We compare ensembles for global and local prediction, as well as single trees for global prediction and tree collections for local prediction, both in terms of predictive performance and in terms of efficiency (running times and model complexity). The results show that both global and local tree ensembles perform better than the single model counterparts in terms of predictive power. Global and local tree ensembles perform equally well, with global ensembles being more efficient and producing smaller models, as well as needing fewer trees in the ensemble to achieve the maximal performance.},
  number = {3},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  date = {2013-03-01},
  pages = {817-833},
  keywords = {Ensemble methods,Hierarchical multi-label classification,Multi-target classification,Multi-target regression,Predictive clustering trees,Structured outputs},
  author = {Kocev, Dragi and Vens, Celine and Struyf, Jan and Džeroski, Sašo},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Kocev_et_al_2013_Tree_ensembles_for_predicting_structured_outputs.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/9H9BXVPW/S003132031200430X.html}
}

@article{chizat2015interpolating,
  title = {An Interpolating Distance between Optimal Transport and {{Fisher}}-{{Rao}}},
  url = {https://arxiv.org/abs/1506.06430},
  journaltitle = {arXiv preprint arXiv:1506.06430},
  date = {2015},
  author = {Chizat, Lenaic and Schmitzer, Bernhard and Peyré, Gabriel and Vialard, François-Xavier},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chizat_et_al_2015_An_interpolating_distance_between_optimal_transport_and_Fisher-Rao.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/HSN94F2E/1506.html}
}

@article{cuturi2017softdtw,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.01541},
  primaryClass = {stat},
  title = {Soft-{{DTW}}: A {{Differentiable Loss Function}} for {{Time}}-{{Series}}},
  url = {http://arxiv.org/abs/1703.01541},
  shorttitle = {Soft-{{DTW}}},
  abstract = {We propose in this paper a differentiable learning loss between time series. Our proposal builds upon the celebrated Dynamic Time Warping (DTW) discrepancy. Unlike the Euclidean distance, DTW is able to compare asynchronous time series of varying size and is robust to elastic transformations in time. To be robust to such invariances, DTW computes a minimal cost alignment between time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and its gradient can be computed with quadratic time/space complexity (DTW has quadratic time and linear space complexity). We show that our regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.},
  date = {2017-03-04},
  keywords = {Statistics - Machine Learning},
  author = {Cuturi, Marco and Blondel, Mathieu},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Cuturi_Blondel_2017_Soft-DTW_-_a_Differentiable_Loss_Function_for_Time-Series.pdf}
}

@article{zhang2015learning,
  title = {Learning {{Multiple Linear Mappings}} for {{Efficient Single Image Super}}-{{Resolution}}},
  volume = {24},
  issn = {1057-7149},
  doi = {10.1109/TIP.2015.2389629},
  abstract = {Example learning-based superresolution (SR) algorithms show promise for restoring a high-resolution (HR) image from a single low-resolution (LR) input. The most popular approaches, however, are either time- or space-intensive, which limits their practical applications in many resource-limited settings. In this paper, we propose a novel computationally efficient single image SR method that learns multiple linear mappings (MLM) to directly transform LR feature subspaces into HR subspaces. In particular, we first partition the large nonlinear feature space of LR images into a cluster of linear subspaces. Multiple LR subdictionaries are then learned, followed by inferring the corresponding HR subdictionaries based on the assumption that the LR-HR features share the same representation coefficients. We establish MLM from the input LR features to the desired HR outputs in order to achieve fast yet stable SR recovery. Furthermore, in order to suppress displeasing artifacts generated by the MLM-based method, we apply a fast nonlocal means algorithm to construct a simple yet effective similarity-based regularization term for SR enhancement. Experimental results indicate that our approach is both quantitatively and qualitatively superior to other application-oriented SR methods, while maintaining relatively low time and space complexity.},
  number = {3},
  journaltitle = {IEEE Transactions on Image Processing},
  date = {2015-03},
  pages = {846-861},
  keywords = {Feature extraction,Training,Dictionaries,example learning-based superresolution algorithm,Fast non-local means,feature subspace,high-resolution image restoration,HR subspace,Image reconstruction,image resolution,image restoration,learning (artificial intelligence),LR feature subspace,LR subdictionaries learning,MLM learning,multiple linear mapping learning,multiple linear mappings (MLMs),Principal component analysis,similarity-based regularization term,single image super-resolution,single image super-resolution (SR),single image super-resolution (SR).,SR algorithm,SR enhancement,Transforms,Vectors},
  author = {Zhang, K. and Tao, D. and Gao, X. and Li, X. and Xiong, Z.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/HAJQXUHG/7003985.html}
}

@article{chen2016interpolation,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.05914},
  primaryClass = {math},
  title = {Interpolation of {{Density Matrices}} and {{Matrix}}-{{Valued Measures}}: {{The Unbalanced Case}}},
  url = {http://arxiv.org/abs/1612.05914},
  shorttitle = {Interpolation of {{Density Matrices}} and {{Matrix}}-{{Valued Measures}}},
  abstract = {In this note, we propose an unbalanced version of the quantum mechanical version of optimal mass transport that was based on the Lindblad equation. We formulate a natural interpolation framework between density matrices and matrix-valued measures via a quantum mechanical formulation of Fisher-Rao information and the matricial Wasserstein distance.},
  date = {2016-12-18},
  keywords = {Mathematics - Functional Analysis},
  author = {Chen, Yongxin and Georgiou, Tryphon T. and Tannenbaum, Allen},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chen_et_al_2016_Interpolation_of_Density_Matrices_and_Matrix-Valued_Measures_-_The_Unbalanced.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/R87PD386/1612.html}
}

@article{chizat2016scaling,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.05816},
  primaryClass = {math},
  title = {Scaling {{Algorithms}} for {{Unbalanced Transport Problems}}},
  url = {http://arxiv.org/abs/1607.05816},
  abstract = {This article introduces a new class of fast algorithms to approximate variational problems involving unbalanced optimal transport. While classical optimal transport considers only normalized probability distributions, it is important for many applications to be able to compute some sort of relaxed transportation between arbitrary positive measures. A generic class of such "unbalanced" optimal transport problems has been recently proposed by several authors. In this paper, we show how to extend the, now classical, entropic regularization scheme to these unbalanced problems. This gives rise to fast, highly parallelizable algorithms that operate by performing only diagonal scaling (i.e. pointwise multiplications) of the transportation couplings. They are generalizations of the celebrated Sinkhorn algorithm. We show how these methods can be used to solve unbalanced transport, unbalanced gradient flows, and to compute unbalanced barycenters. We showcase applications to 2-D shape modification, color transfer, and growth models.},
  date = {2016-07-20},
  keywords = {_read,Mathematics - Optimization and Control,65K10},
  author = {Chizat, Lenaic and Peyré, Gabriel and Schmitzer, Bernhard and Vialard, François-Xavier},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chizat_et_al_2016_Scaling_Algorithms_for_Unbalanced_Transport_Problems.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/ZAX7JGST/1607.html}
}

@book{villani2008optimal,
  langid = {english},
  title = {Optimal {{Transport}}: {{Old}} and {{New}}},
  isbn = {978-3-540-71050-9},
  shorttitle = {Optimal {{Transport}}},
  abstract = {At the close of the 1980s, the independent contributions of Yann Brenier, Mike Cullen and John Mather launched a revolution in the venerable field of optimal transport founded by G. Monge in the 18th century, which has made breathtaking forays into various other domains of mathematics ever since. The author presents a broad overview of this area, supplying complete and self-contained proofs of all the fundamental results of the theory of optimal transport at the appropriate level of generality. Thus, the book encompasses the broad spectrum ranging from basic theory to the most recent research results.   PhD students or researchers can read the entire book without any prior knowledge of the field. A comprehensive bibliography with notes that extensively discuss the existing literature underlines the book’s value as a most welcome reference text on this subject.},
  pagetotal = {970},
  publisher = {{Springer Science \& Business Media}},
  date = {2008-10-26},
  keywords = {Mathematics / Calculus,Mathematics / Differential Equations / General,Mathematics / Functional Analysis,Mathematics / Geometry / Differential,Mathematics / Mathematical Analysis},
  author = {Villani, Cédric},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Villani_2008_Optimal_Transport_-_Old_and_New.pdf},
  eprinttype = {googlebooks}
}

@article{radford2015unsupervised,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  date = {2015-11-19},
  keywords = {Computer Science - Learning,Computer Science - Computer Vision and Pattern Recognition},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Radford_et_al_2015_Unsupervised_Representation_Learning_with_Deep_Convolutional_Generative.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/RN8JF54Q/1511.html}
}

@article{ramdas2017wasserstein,
  title = {On {{Wasserstein Two}}-{{Sample Testing}} and {{Related Families}} of {{Nonparametric Tests}}},
  volume = {19},
  url = {http://www.mdpi.com/1099-4300/19/2/47/htm},
  number = {2},
  journaltitle = {Entropy},
  date = {2017},
  pages = {47},
  author = {Ramdas, Aaditya and Trillos, Nicolás García and Cuturi, Marco},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Ramdas_et_al_2017_On_Wasserstein_Two-Sample_Testing_and_Related_Families_of_Nonparametric_Tests.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/MBTUN2QK/htm.html}
}

@article{genevay2017sinkhornautodiff,
  title = {Sinkhorn-{{AutoDiff}}: {{Tractable Wasserstein Learning}} of {{Generative Models}}},
  url = {https://arxiv.org/abs/1706.00292},
  shorttitle = {Sinkhorn-{{AutoDiff}}},
  journaltitle = {arXiv preprint arXiv:1706.00292},
  date = {2017},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Genevay_et_al_2017_Sinkhorn-AutoDiff_-_Tractable_Wasserstein_Learning_of_Generative_Models.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/3F6EHSJ4/1706.html}
}

@article{genevay2017gan,
  title = {{{GAN}} and {{VAE}} from an {{Optimal Transport Point}} of {{View}}},
  url = {https://arxiv.org/abs/1706.01807},
  journaltitle = {arXiv preprint arXiv:1706.01807},
  date = {2017},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Genevay_et_al_2017_GAN_and_VAE_from_an_Optimal_Transport_Point_of_View.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/W6F7WQBK/1706.html}
}

@article{gremse2016gpuaccelerated,
  title = {{{GPU}}-Accelerated Adjoint Algorithmic Differentiation},
  volume = {200},
  issn = {0010-4655},
  url = {http://www.sciencedirect.com/science/article/pii/S0010465515004099},
  doi = {10.1016/j.cpc.2015.10.027},
  abstract = {Many scientific problems such as classifier training or medical image reconstruction can be expressed as minimization of differentiable real-valued cost functions and solved with iterative gradient-based methods. Adjoint algorithmic differentiation (AAD) enables automated computation of gradients of such cost functions implemented as computer programs. To backpropagate adjoint derivatives, excessive memory is potentially required to store the intermediate partial derivatives on a dedicated data structure, referred to as the “tape”. Parallelization is difficult because threads need to synchronize their accesses during taping and backpropagation. This situation is aggravated for many-core architectures, such as Graphics Processing Units (GPUs), because of the large number of light-weight threads and the limited memory size in general as well as per thread. We show how these limitations can be mediated if the cost function is expressed using GPU-accelerated vector and matrix operations which are recognized as intrinsic functions by our AAD software. We compare this approach with naive and vectorized implementations for CPUs. We use four increasingly complex cost functions to evaluate the performance with respect to memory consumption and gradient computation times. Using vectorization, CPU and GPU memory consumption could be substantially reduced compared to the naive reference implementation, in some cases even by an order of complexity. The vectorization allowed usage of optimized parallel libraries during forward and reverse passes which resulted in high speedups for the vectorized CPU version compared to the naive reference implementation. The GPU version achieved an additional speedup of 7.5±4.4, showing that the processing power of GPUs can be utilized for AAD using this concept. Furthermore, we show how this software can be systematically extended for more complex problems such as nonlinear absorption reconstruction for fluorescence-mediated tomography. Program title: AD-GPU Catalogue identifier: AEYX\_v1\_0 Program summary URL: http://cpc.cs.qub.ac.uk/summaries/AEYX\_v1\_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 16715 No. of bytes in distributed program, including test data, etc.: 143683 Distribution format: tar.gz Programming language: C++ and CUDA. Computer: Any computer with a compatible C++ compiler and a GPU with CUDA capability 3.0 or higher. Operating system: Windows 7 or Linux. RAM: 16 Gbyte Classification: 4.9, 4.12, 6.1, 6.5. External routines: CUDA 6.5, Intel MKL (optional) and routines from BLAS, LAPACK and CUBLAS Nature of problem: Gradients are required for many optimization problems, e.g.~classifier training or nonlinear image reconstruction. Often, the function, of which the gradient is required, can be implemented as a computer program. Then, algorithmic differentiation methods can be used to compute the gradient. Depending on the approach this may result in excessive requirements of computational resources, i.e.~memory and arithmetic computations. GPUs provide massive computational resources but require special considerations to distribute the workload onto many light-weight threads. Solution method: Adjoint algorithmic differentiation allows efficient computation of gradients of cost functions given as computer programs. The gradient can be theoretically computed using a similar amount of arithmetic operations as one function evaluation. Optimal usage of parallel processors and limited memory is a major challenge which can be mediated by the use of vectorization. Restrictions: To use the GPU-accelerated adjoint algorithmic differentiation method, the cost function must be implemented using the provided AD-GPU intrinsics for matrix and vector operations. Unusual features: GPU-acceleration. Additional comments: The code uses some features of C++11, e.g.~std::shared ptr. Alternatively, the boost library can be used. Running time: The time to run the example program is a few minutes or up to a few hours to reproduce the performance measurements.},
  journaltitle = {Computer Physics Communications},
  shortjournal = {Computer Physics Communications},
  date = {2016-03-01},
  pages = {300-311},
  keywords = {Adjoint algorithmic differentiation,GPU programming},
  author = {Gremse, Felix and Höfter, Andreas and Razik, Lukas and Kiessling, Fabian and Naumann, Uwe},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gremse_et_al_2016_GPU-accelerated_adjoint_algorithmic_differentiation.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/TP5F5RCE/S0010465515004099.html}
}

@inproceedings{trouve2000diffeomorphic,
  langid = {english},
  title = {Diffeomorphic {{Matching Problems}} in {{One Dimension}}: {{Designing}} and {{Minimizing Matching Functionals}}},
  isbn = {978-3-540-67685-0 978-3-540-45054-2},
  url = {https://link.springer.com/chapter/10.1007/3-540-45054-8_37},
  doi = {10.1007/3-540-45054-8_37},
  shorttitle = {Diffeomorphic {{Matching Problems}} in {{One Dimension}}},
  abstract = {This paper focuses on matching 1D structures by variational methods. We provide rigorous rules for the construction of the cost function, on the basis of an analysis of properties which should be satisfied by the optimal matching. A new, exact, dynamic programming algorithm is then designed for the minimization. We conclude with experimental results on shape comparison.},
  eventtitle = {European {{Conference}} on {{Computer Vision}}},
  booktitle = {Computer {{Vision}} - {{ECCV}} 2000},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {{Springer, Berlin, Heidelberg}},
  urldate = {2017-07-03},
  date = {2000-06-26},
  pages = {573-587},
  author = {Trouvé, Alain and Younes, Laurent},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Trouvé_Younes_2000_Diffeomorphic_Matching_Problems_in_One_Dimension_-_Designing_and_Minimizing.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/PPIEVGRX/10.html}
}

@article{suorderpreserving,
  title = {Order-Preserving {{Wasserstein Distance}} for {{Sequence Matching}}},
  url = {http://www.ganghua.org/publication/CVPR17d.pdf},
  author = {Su, Bing and Hua, Gang},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Su_Hua_Order-preserving_Wasserstein_Distance_for_Sequence_Matching.pdf}
}

@article{turkin2016benchmarking,
  title = {Benchmarking {{Python Tools}} for {{Automatic Differentiation}}},
  volume = {4},
  issn = {2307-8162},
  url = {http://cyberleninka.ru/article/n/benchmarking-python-tools-for-automatic-differentiation},
  abstract = {In this paper, we compare several Python tools for automatic differentiation. In order to assess the difference in performance and their precision, the problem of finding the optimal geometrical structure of a cluster of identical atoms is used as follows. First, we compare the performance of calculating gradients for the objective function. We showed that the PyADOL-C and PyCppAD tools have much better performance for big clusters than the other ones. Second, we assess the precision of these two tools by calculating the difference between the obtained at the optimal configuration gradient norms. We conclude that PyCppAD has the best performance among others, while having almost the same precision as the second-best performing tool PyADOL-C.},
  number = {9},
  journaltitle = {International Journal of Open Information Technologies},
  urldate = {2017-07-03},
  date = {2016},
  author = {Turkin, Andrei and Thu, Aung},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Turkin_Thu_2016_Benchmarking_Python_Tools_for_Automatic_Differentiation.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/UCVUX2V9/benchmarking-python-tools-for-automatic-differentiation.html}
}

@article{gilbert1993automatic,
  title = {Automatic Differentiation and the Step Computation in the Limited Memory {{BFGS}} Method},
  volume = {6},
  issn = {0893-9659},
  url = {http://www.sciencedirect.com/science/article/pii/089396599390032I},
  doi = {10.1016/0893-9659(93)90032-I},
  abstract = {It is shown that the two-loop recursion for computing the search direction of a limited memory method for optimization can be derived by means of the reverse mode of automatic differentiation applied to an auxilliary function.},
  number = {3},
  journaltitle = {Applied Mathematics Letters},
  shortjournal = {Applied Mathematics Letters},
  date = {1993-05-01},
  pages = {47-50},
  author = {Gilbert, Jean Charles and Nocedal, Jorge},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gilbert_Nocedal_1993_Automatic_differentiation_and_the_step_computation_in_the_limited_memory_BFGS.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/DFGUTPCC/089396599390032I.html}
}

@article{baydin2015automatic,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.05767},
  primaryClass = {cs},
  title = {Automatic Differentiation in Machine Learning: A Survey},
  url = {http://arxiv.org/abs/1502.05767},
  shorttitle = {Automatic Differentiation in Machine Learning},
  abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD) is a technique for calculating derivatives of numeric functions expressed as computer programs efficiently and accurately, used in fields such as computational fluid dynamics, nuclear engineering, and atmospheric sciences. Despite its advantages and use in other fields, machine learning practitioners have been little influenced by AD and make scant use of available tools. We survey the intersection of AD and machine learning, cover applications where AD has the potential to make a big impact, and report on some recent developments in the adoption of this technique. We aim to dispel some misconceptions that we contend have impeded the use of AD within the machine learning community.},
  date = {2015-02-19},
  keywords = {Computer Science - Learning,Computer Science - Symbolic Computation,68W30; 65D25; 68T05,G.1.4,I.2.6},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Baydin_et_al_2015_Automatic_differentiation_in_machine_learning_-_a_survey.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/3XW4QAU8/1502.html}
}

@article{wang1997alignment,
  title = {Alignment of Curves by Dynamic Time Warping},
  volume = {25},
  issn = {0090-5364, 2168-8966},
  url = {http://projecteuclid.org/euclid.aos/1069362747},
  doi = {10.1214/aos/1069362747},
  abstract = {When studying some process or development in different subjects or units--be it biological, chemical or physical--we usually see a typical pattern, common to all curves. Yet there is variation both in amplitude and dynamics between curves. Following some ideas of structural analysis introduced by Kneip and Gasser, we study a method--dynamic time warping with a proper cost function--for estimating the shift or warping function from one curve to another to align the two functions. For some models this method can identify the true shift functions if the data are noise free. Noisy data are smoothed by a nonparametric function estimate such as a kernel estimate. It is shown that the proposed estimator is asymptotically normal and converges to the true shift function as the sample size per subject goes to infinity. Some simulation results are presented to illustrate the performance of this method.},
  number = {3},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  urldate = {2017-07-03},
  date = {1997-06},
  pages = {1251-1276},
  keywords = {Curves,dynamic time warping,kernel estimation,shift functions,structural analysis},
  author = {Wang, Kongming and Gasser, Theo},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Wang_Gasser_1997_Alignment_of_curves_by_dynamic_time_warping.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/XGRPVAK3/1069362747.html}
}

@article{bogachev2012mongekantorovich,
  langid = {english},
  title = {The {{Monge}}-{{Kantorovich}} Problem: Achievements, Connections, and Perspectives},
  volume = {67},
  issn = {0036-0279},
  url = {http://iopscience.iop.org/article/10.1070/RM2012v067n05ABEH004808/meta},
  doi = {10.1070/RM2012v067n05ABEH004808},
  shorttitle = {The {{Monge}}-{{Kantorovich}} Problem},
  number = {5},
  journaltitle = {Russian Mathematical Surveys},
  shortjournal = {Russ. Math. Surv.},
  urldate = {2017-07-03},
  date = {2012},
  pages = {785},
  author = {Bogachev, Vladimir I and Kolesnikov, Aleksandr V},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Bogachev_Kolesnikov_2012_The_Monge-Kantorovich_problem_-_achievements,_connections,_and_perspectives.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/DWM74D3I/meta.html}
}

@article{caffarelli2010free,
  eprinttype = {jstor},
  eprint = {20752228},
  title = {Free Boundaries in Optimal Transport and {{Monge}}-{{Ampère}} Obstacle Problems},
  volume = {171},
  issn = {0003-486X},
  abstract = {Given compactly supported 0 ≤ f, g ∈ L¹(ℝ n ), the problem of transporting a fraction m ≤ min\{∥ f ∥ L 1 , ∥ g ∥ L 1 \} of the mass of f onto g as cheaply as possible is considered, where cost per unit mass transported is given by a cost function c, typically quadratic c(x, y) = ǀx — yǀ²/2. This question is shown to be equivalent to a double obstacle problem for the Monge-Ampère equation, for which sufficient conditions are given to guarantee uniqueness of the solution, such as f vanishing on spt g in the quadratic case. The part of f to be transported increases monotonically with m, and if spt f and spt g are separated by a hyperplane H, then this part will be separated from the balance of f by a semiconcave Lipschitz graph over the hyperplane. If f = fχ Ω and g = gχ Λ are bounded away from zero and infinity on separated strictly convex domains Ω, Λ ⊂ R n , for the quadratic cost this graph is shown to be a \$C\_\{\textbackslash{}text\{loc\}\}\^\{1,\textbackslash{}alpha\}\$ hypersurface in Ω whose normal coincides with the direction transported; the optimal map between f and g is shown to be Hölder continuous up to this free boundary, and to those parts of the fixed boundary ∂Ω which map to locally convex parts of the path-connected target region.},
  number = {2},
  journaltitle = {Annals of Mathematics},
  date = {2010},
  pages = {673-730},
  author = {Caffarelli, Luis A. and McCann, Robert J.}
}

@article{mainini2012description,
  langid = {english},
  title = {A Description of Transport Cost for Signed Measures},
  volume = {181},
  issn = {1072-3374, 1573-8795},
  url = {https://link.springer.com/article/10.1007/s10958-012-0718-2},
  doi = {10.1007/s10958-012-0718-2},
  abstract = {In this paper, we develop the analysis started in a paper by Ambrosio, Mainini, and Serfaty about the extension of the optimal transport framework to the space of real measures. The main motivation comes from the study of nonpositive solutions to some evolution PDEs. Although a canonical optimal transport distance does not seem to be available, we may describe the cost for transporting signed measures in various ways and with interesting properties. Bibliography: 22 titles.},
  number = {6},
  journaltitle = {Journal of Mathematical Sciences},
  shortjournal = {J Math Sci},
  urldate = {2017-07-03},
  date = {2012-03-01},
  pages = {837-855},
  author = {Mainini, E.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Mainini_2012_A_description_of_transport_cost_for_signed_measures.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/376BSQ6P/10.html}
}

@article{chitescu2014mongekantorovich,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.4980},
  primaryClass = {math},
  title = {Monge-{{Kantorovich}} Norms on Spaces of Vector Measures},
  url = {http://arxiv.org/abs/1404.4980},
  abstract = {One considers Hilbert space valued measures on the Borel sets of a compact metric space. A natural numerical valued integral of vector valued continuous functions with respect to vector valued functions is defined. Using this integral, different norms (we called them Monge-Kantorovich norm, modified Monge-Kantorovich norm and Hanin norm) on the space of measures are introduced, generalizing the theory of (weak) convergence for probability measures on metric spaces. These norms introduce new (equivalent) metrics on the initial compact metric space.},
  date = {2014-04-19},
  keywords = {Mathematics - Functional Analysis,Primary: 28B05; 46G10; 46E10; 28C15. Secondary: 46B25; 46C05},
  author = {Chitescu, Ion and Miculescu, Radu and Nita, Lucian and Ioana, Loredana},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Chitescu_et_al_2014_Monge-Kantorovich_norms_on_spaces_of_vector_measures.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/I2UB3PPU/1404.html}
}

@article{munkres1957algorithms,
  title = {Algorithms for the {{Assignment}} and {{Transportation Problems}}},
  volume = {5},
  issn = {0368-4245},
  url = {http://epubs.siam.org/doi/abs/10.1137/0105003},
  doi = {10.1137/0105003},
  number = {1},
  journaltitle = {Journal of the Society for Industrial and Applied Mathematics},
  shortjournal = {Journal of the Society for Industrial and Applied Mathematics},
  date = {1957-03-01},
  pages = {32-38},
  author = {Munkres, J.},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/CUFGDEAE/0105003.html}
}

@article{carlier2015numerical,
  langid = {english},
  title = {Numerical Methods for Matching for Teams and {{Wasserstein}} Barycenters},
  volume = {49},
  issn = {0764-583X, 1290-3841},
  url = {http://dx.doi.org/10.1051/m2an/2015033},
  doi = {10.1051/m2an/2015033},
  abstract = {ESAIM: Mathematical Modelling and Numerical Analysis, an international journal on applied mathematics},
  number = {6},
  journaltitle = {ESAIM: Mathematical Modelling and Numerical Analysis},
  shortjournal = {ESAIM: M2AN},
  urldate = {2017-07-03},
  date = {2015-11-01},
  pages = {1621-1642},
  author = {Carlier, Guillaume and Oberman, Adam and Oudet, Edouard},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/OT/Carlier_et_al_2015_Numerical_methods_for_matching_for_teams_and_Wasserstein_barycenters.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2X4DVAGM/m2an150084.html}
}

@inproceedings{hochreiter2005optimal,
  title = {Optimal Kernels for Unsupervised Learning},
  volume = {3},
  booktitle = {Neural {{Networks}}, 2005. {{IJCNN}}'05. {{Proceedings}}. 2005 {{IEEE International Joint Conference}} On},
  publisher = {{IEEE}},
  date = {2005},
  pages = {1895--1899},
  keywords = {read},
  author = {Hochreiter, Sepp and Obermayer, Klaus},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Hochreiter_Obermayer_2005_Optimal_kernels_for_unsupervised_learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/WN7V6XVS/1556169.html}
}

@misc{khemakhem2018analysis,
  langid = {english},
  title = {Analysis of {{Fourier MMD}} and a Possible Application to Sampling},
  abstract = {Sampling from a transformation of the MMD from an analytical pdf.},
  date = {2018-06},
  author = {Khemakhem, Ilyes},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Random/Khemakhem_2018_Analysis_of_Fourier_MMD_and_a_possible_application_to_sampling.pdf}
}

@book{arwini2008information,
  langid = {english},
  location = {{Berlin Heidelberg}},
  title = {Information {{Geometry}}: {{Near Randomness}} and {{Near Independence}}},
  isbn = {978-3-540-69391-8},
  url = {//www.springer.com/gb/book/9783540693918},
  shorttitle = {Information {{Geometry}}},
  abstract = {This volume will be useful to practising scientists and students working in the application of statistical models to real materials or to processes with perturbations of a Poisson process, a uniform process, or a state of independence for a bivariate process. We use information geometry to provide a common differential geometric framework for a wide range of illustrative applications including amino acid sequence spacings in protein chains, cryptology studies, clustering of communications and galaxies, cosmological voids, coupled spatial statistics in stochastic fibre networks and stochastic porous media, quantum chaology. Introduction sections are provided to mathematical statistics, differential geometry and the information geometry of spaces of probability density functions.},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  publisher = {{Springer-Verlag}},
  urldate = {2018-09-25},
  date = {2008},
  author = {Arwini, Khadiga and Dodson, Christopher T. J.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Arwini_Dodson_2008_Information_Geometry_-_Near_Randomness_and_Near_Independence.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/FMKWD7TD/9783540693918.html}
}

@book{buoncristiano2003fragments,
  langid = {english},
  title = {Fragments of Geometric Topology from the Sixties},
  pagetotal = {book},
  publisher = {{University of Warwick, Mathematics Institute}},
  date = {2003},
  author = {Buoncristiano, S.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Buoncristiano_2003_Fragments_of_geometric_topology_from_the_sixties.pdf},
  eprinttype = {googlebooks}
}

@book{lee2003introduction,
  langid = {english},
  location = {{New York}},
  title = {Introduction to {{Smooth Manifolds}}},
  isbn = {978-0-387-21752-9},
  url = {//www.springer.com/gb/book/9780387217529},
  abstract = {Manifolds are everywhere. These generalizations of curves and surfaces to arbitrarily many dimensions provide the mathematical context for under­ standing "space" in all of its manifestations. Today, the tools of manifold theory are indispensable in most major subfields of pure mathematics, and outside of pure mathematics they are becoming increasingly important to scientists in such diverse fields as genetics, robotics, econometrics, com­ puter graphics, biomedical imaging, and, of course, the undisputed leader among consumers (and inspirers) of mathematics-theoretical physics. No longer a specialized subject that is studied only by differential geometers, manifold theory is now one of the basic skills that all mathematics students should acquire as early as possible. Over the past few centuries, mathematicians have developed a wondrous collection of conceptual machines designed to enable us to peer ever more deeply into the invisible world of geometry in higher dimensions. Once their operation is mastered, these powerful machines enable us to think geometrically about the 6-dimensional zero set of a polynomial in four complex variables, or the lO-dimensional manifold of 5 x 5 orthogonal ma­ trices, as easily as we think about the familiar 2-dimensional sphere in ]R3.},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer-Verlag}},
  urldate = {2018-10-04},
  date = {2003},
  author = {Lee, John M.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Lee_2003_Introduction_to_Smooth_Manifolds.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/T6X5N8XV/9780387217529.html}
}

@book{docarmo1992riemannian,
  langid = {english},
  title = {Riemannian {{Geometry}}},
  isbn = {978-0-8176-3490-2},
  url = {//www.springer.com/us/book/9780817634902},
  abstract = {Riemannian Geometry is an expanded edition of a highly acclaimed and successful textbook (originally published in Portuguese) for first-year graduate students in mathematics and physics. The author's treatment goes very directly to the basic language of Riemannian geometry and immediately presents some of its most fundamental theorems. It is elementary, assuming only a modest background from readers, making it suitable for a wide variety of students and course structures. Its selection of topics has been deemed "superb" by teachers who have used the text. A significant feature of the book is its powerful and revealing structure, beginning simply with the definition of a differentiable manifold and ending with one of the most important results in Riemannian geometry, a proof of the Sphere Theorem. The text abounds with basic definitions and theorems, examples, applications, and numerous exercises to test the student's understanding and extend knowledge and insight into the subject. Instructors and students alike will find the work to be a significant contribution to this highly applicable and stimulating subject.},
  series = {Mathematics: {{Theory}} \& {{Applications}}},
  publisher = {{Birkhäuser Basel}},
  urldate = {2018-10-15},
  date = {1992},
  author = {do Carmo, Manfredo},
  options = {useprefix=true},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/do_Carmo_1992_Riemannian_Geometry.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2NMSFJWI/9780817634902.html}
}

@book{strichartz2000way,
  langid = {english},
  title = {The {{Way}} of {{Analysis}}},
  isbn = {978-0-7637-1497-0},
  abstract = {The Way of Analysis gives a thorough account of real analysis in one or several variables, from the construction of the real number system to an introduction of the Lebesgue integral. The text provides proofs of all main results, as well as motivations, examples, applications, exercises, and formal chapter summaries. Additionally, there are three chapters on application of analysis, ordinary differential equations, Fourier series, and curves and surfaces to show how the techniques of analysis are used in concrete settings.},
  pagetotal = {764},
  publisher = {{Jones \& Bartlett Learning}},
  date = {2000},
  keywords = {Mathematics / Mathematical Analysis},
  author = {Strichartz, Robert S.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Strichartz_2000_The_Way_of_Analysis.pdf},
  eprinttype = {googlebooks}
}

@article{burel1992blind,
  title = {Blind Separation of Sources: {{A}} Nonlinear Neural Algorithm},
  volume = {5},
  issn = {0893-6080},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608005800905},
  doi = {10.1016/S0893-6080(05)80090-5},
  shorttitle = {Blind Separation of Sources},
  abstract = {In many signal processing applications, the signals provided by the sensors are mixtures of many sources. The problem of separation of sources is to extract the original signals from these mixtures. A new algorithm, based on ideas of back propagation learning, is proposed for source separation. No a priori information on the sources themselves is required, and the algorithm can deal even with nonlinear mixtures. After a short overview of previous works in that field, we will describe the proposed algorithm, then some experimental results will be discussed.},
  number = {6},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  urldate = {2018-11-28},
  date = {1992-11-01},
  pages = {937-947},
  keywords = {ICA,Back propagation,High order moments,Independent component analysis,Mixture of sources,Neural networks,Nonlinear algorithms,Separation of sources,NICA,BSS,old NICA},
  author = {Burel, Gilles},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Burel_1992_Blind_separation_of_sources_-_A_nonlinear_neural_algorithm.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2DX5AXIU/S0893608005800905.html}
}

@online{publications,
  title = {Publications by {{Aapo Hyvärinen}}: Nonlinear {{ICA}}},
  url = {https://www.cs.helsinki.fi/u/ahyvarin/papers/udl.shtml},
  urldate = {2018-11-29},
  keywords = {NICA},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/T3MZS84Q/udl.html}
}

@inproceedings{hirayama2017splice,
  title = {{{SPLICE}}: {{Fully}} Tractable Hierarchical Extension of {{ICA}} with Pooling},
  volume = {70},
  shorttitle = {{{SPLICE}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning}}},
  publisher = {{Machine Learning Research}},
  date = {2017},
  pages = {1491--1500},
  author = {Hirayama, Jun-ichiro and Hyvarinen, A. J. and Kawanabe, Motoaki}
}

@inproceedings{hyvarinen2017nonlinear,
  title = {Nonlinear {{ICA}} of Temporally Dependent Stationary Sources},
  publisher = {{Proceedings of Machine Learning Research}},
  date = {2017},
  keywords = {ICA,NICA},
  author = {Hyvärinen, Aapo and Morioka, Hiroshi},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Morioka_2017_Nonlinear_ICA_of_temporally_dependent_stationary_sources.pdf}
}

@article{yang1998informationtheoretic,
  title = {Information–Theoretic Approach to Blind Separation of Sources in Non-Linear Mixture},
  volume = {64},
  issn = {0165-1684},
  url = {http://www.sciencedirect.com/science/article/pii/S0165168497001965},
  doi = {10.1016/S0165-1684(97)00196-5},
  abstract = {The linear mixture model is assumed in most of the papers devoted to blind separation. A more realistic model for mixture should be non-linear. In this paper, a two-layer perceptron is used as a de-mixing system to separate sources in non-linear mixture. The learning algorithms for the de-mixing system are derived by two approaches: maximum entropy and minimum mutual information. The algorithms derived from the two approaches have a common structure. The new learning equations for the hidden layer are different from the learning equations for the output layer. The natural gradient descent method is applied in maximizing entropy and minimizing mutual information. The information (entropy or mutual information) back-propagation method is proposed to derive the learning equations for the hidden layer.
Zusammenfassung
In den meisten Arbeiten zur blinden Separation von Quellen wird von einem linearen Mischmodell ausgegangen. In dieser Arbeit hingegen wird ein Zweischicht-Perzeptron als System zur Trennung von Quellen bei nichtlinearer Mischung verwendet. Die Lernalgorithmen für das trennende System werden ausgehend von zwei Ansätzen abgeleitet: Maximale Entropie und minimale wechselseitige Information. Die Algorithmen, die von den beiden Ansätzen abgeleitet werden, haben eine gemeinsame Struktur. Die resultierenden Gleichungen für die versteckte Schicht weichen von denen für die Ausgangsschicht ab. Das herkömmliche Gradientenabstiegsverfahren wird dazu verwendet, die Entropie zu maximieren und die wechselseitige Information zu minimieren. Für die versteckte Schicht wird die Methode des Informations-Backpropagation (Entropie oder wechselseitige Information) zur Ableitung der Gleichungen des Lernverfahrens vorgeschlagen.
Résumé
Un modèle linéaire de mélange est supposé dans la plupart des articles dévolus à la séparation aveugle. Un modèle plus réaliste de mélange devrait être non-linéaire. Dans cet article, un perceptron à deux couches est utilisé comme système de séparation de sources dans un mélange non-linéaire. Les algorithmes pour le système de séparation sont dérivés à l’aide de deux approches: entropie maximum et information mutuelle minimum. Les algorithmes dérivés dans ces deux approches ont une structure commune. Les nouvelles équations d’apprentissage pour la couche cachée sont différentes de celles pour la couche de sortie. La méthode du gradient naturelle est appliquée pour la maximisation et la minimisation de l’information mutuelle. La méthode de rétro-propagation de l’information (entropie ou information mutuelle) est proposée pour dériver les équations d’apprentissage de la couche cachée.},
  number = {3},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Processing},
  urldate = {2018-11-29},
  date = {1998-02-26},
  pages = {291-300},
  keywords = {ICA,NICA,Blind separation,Information back propagation,Maximum entropy,Minimum mutual information,Non-linear mixture,BSS,Old NICA},
  author = {Yang, Howard Hua and Amari, Shun-ichi and Cichocki, Andrzej},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Yang_et_al_1998_Information–theoretic_approach_to_blind_separation_of_sources_in_non-linear.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/AA6ELCRD/S0165168497001965.html}
}

@online{publicationsa,
  title = {Publications by {{Aapo Hyvarinen}}: {{FastICA}}},
  url = {https://www.cs.helsinki.fi/u/ahyvarin/papers/fastica.shtml},
  urldate = {2018-11-29},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/3HSRI3IV/fastica.html}
}

@article{hyvarinen1999fast,
  title = {Fast and Robust Fixed-Point Algorithms for Independent Component Analysis},
  volume = {10},
  number = {3},
  journaltitle = {IEEE transactions on Neural Networks},
  date = {1999},
  pages = {626--634},
  keywords = {ICA},
  author = {Hyvärinen, Aapo},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_1999_Fast_and_robust_fixed-point_algorithms_for_independent_component_analysis.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/E7L63HHY/761722.html}
}

@article{hyvarinen2000independent,
  title = {Independent Component Analysis: Algorithms and Applications},
  volume = {13},
  issn = {0893-6080},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608000000265},
  doi = {10.1016/S0893-6080(00)00026-5},
  shorttitle = {Independent Component Analysis},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  number = {4},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  urldate = {2018-11-29},
  date = {2000-06-01},
  pages = {411-430},
  keywords = {ICA,Independent component analysis,Blind signal separation,Factor analysis,Projection pursuit,Representation,Source separation},
  author = {Hyvärinen, Aapo and Oja, E.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Oja_2000_Independent_component_analysis_-_algorithms_and_applications.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/S5EZ9MQS/S0893608000000265.html}
}

@article{burgess2018understanding,
  title = {Understanding Disentangling in β-{{VAE}}},
  journaltitle = {arXiv preprint arXiv:1804.03599},
  date = {2018},
  keywords = {VAE,disentanglement},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Burgess_et_al_2018_Understanding_disentangling_in_β-VAE.pdf}
}

@article{korbar2018cooperative,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.00230},
  primaryClass = {cs},
  title = {Cooperative {{Learning}} of {{Audio}} and {{Video Models}} from {{Self}}-{{Supervised Synchronization}}},
  url = {http://arxiv.org/abs/1807.00230},
  abstract = {There is a natural correlation between the visual and auditive elements of a video. In this work we leverage this connection to learn general and effective models for both audio and video analysis from self-supervised temporal synchronization. We demonstrate that a calibrated curriculum learning scheme, a careful choice of negative examples, and the use of a contrastive loss are critical ingredients to obtain powerful multi-sensory representations from models optimized to discern temporal synchronization of audio-video pairs. Without further finetuning, the resulting audio features achieve performance superior or comparable to the state-of-the-art on established audio classification benchmarks (DCASE2014 and ESC-50). At the same time, our visual subnet provides a very effective initialization to improve the accuracy of video-based action recognition models: compared to learning from scratch, our self-supervised pretraining yields a remarkable gain of +19.9\% in action recognition accuracy on UCF101 and a boost of +17.7\% on HMDB51.},
  urldate = {2018-11-29},
  date = {2018-06-30},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  author = {Korbar, Bruno and Tran, Du and Torresani, Lorenzo},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Korbar_et_al_2018_Cooperative_Learning_of_Audio_and_Video_Models_from_Self-Supervised.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/9CI6WA34/1807.html}
}

@article{misra2016shuffle,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.08561},
  primaryClass = {cs},
  title = {Shuffle and {{Learn}}: {{Unsupervised Learning}} Using {{Temporal Order Verification}}},
  url = {http://arxiv.org/abs/1603.08561},
  shorttitle = {Shuffle and {{Learn}}},
  abstract = {In this paper, we present an approach for learning a visual representation from the raw spatiotemporal signals in videos. Our representation is learned without supervision from semantic labels. We formulate our method as an unsupervised sequential verification task, i.e., we determine whether a sequence of frames from a video is in the correct temporal order. With this simple task and no semantic labels, we learn a powerful visual representation using a Convolutional Neural Network (CNN). The representation contains complementary information to that learned from supervised image datasets like ImageNet. Qualitative results show that our method captures information that is temporally varying, such as human pose. When used as pre-training for action recognition, our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51. To demonstrate its sensitivity to human pose, we show results for pose estimation on the FLIC and MPII datasets that are competitive, or better than approaches using significantly more supervision. Our method can be combined with supervised representations to provide an additional boost in accuracy.},
  urldate = {2018-11-29},
  date = {2016-03-28},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Misra, Ishan and Zitnick, C. Lawrence and Hebert, Martial},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Misra_et_al_2016_Shuffle_and_Learn_-_Unsupervised_Learning_using_Temporal_Order_Verification.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/5ZCTE446/1603.html}
}

@article{hjelm2018learning,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1808.06670},
  primaryClass = {cs, stat},
  title = {Learning Deep Representations by Mutual Information Estimation and Maximization},
  url = {http://arxiv.org/abs/1808.06670},
  abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
  urldate = {2018-11-29},
  date = {2018-08-20},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hjelm_et_al_2018_Learning_deep_representations_by_mutual_information_estimation_and_maximization.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/GSLCIGMP/1808.html}
}

@article{bengio2013representation,
  title = {Representation Learning: {{A}} Review and New Perspectives},
  volume = {35},
  shorttitle = {Representation Learning},
  number = {8},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  date = {2013},
  pages = {1798--1828},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Bengio_et_al_2013_Representation_learning_-_A_review_and_new_perspectives.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/XTQS6QIR/6472238.html}
}

@article{doersch2016tutorial,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.05908},
  primaryClass = {cs, stat},
  title = {Tutorial on {{Variational Autoencoders}}},
  url = {http://arxiv.org/abs/1606.05908},
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  urldate = {2018-12-03},
  date = {2016-06-19},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,VAE,Variational autoencoders},
  author = {Doersch, Carl},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Doersch_2016_Tutorial_on_Variational_Autoencoders.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/XRVC9BW2/1606.html}
}

@online{13126114,
  title = {[1312.6114] {{Auto}}-{{Encoding Variational Bayes}}},
  url = {https://arxiv.org/abs/1312.6114},
  urldate = {2018-12-03},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/G5D3QN92/1312.html}
}

@article{kingma2013autoencoding,
  langid = {english},
  title = {Auto-{{Encoding Variational Bayes}}},
  url = {https://arxiv.org/abs/1312.6114},
  urldate = {2018-12-03},
  date = {2013-12-20},
  keywords = {VAE,Variational autoencoders},
  author = {Kingma, Diederik P. and Welling, Max},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Kingma_Welling_2013_Auto-Encoding_Variational_Bayes.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/TUSF5A7H/1312.html}
}

@article{tishby2000information,
  langid = {english},
  title = {The Information Bottleneck Method},
  url = {https://arxiv.org/abs/physics/0004057},
  urldate = {2018-12-03},
  date = {2000-04-24},
  keywords = {Information Bottleneck,IB},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Tishby_et_al_2000_The_information_bottleneck_method.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/UIZK7RTX/0004057.html}
}

@article{gal2014variational,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1402.1412},
  primaryClass = {stat},
  title = {Variational {{Inference}} in {{Sparse Gaussian Process Regression}} and {{Latent Variable Models}} - a {{Gentle Tutorial}}},
  url = {http://arxiv.org/abs/1402.1412},
  abstract = {In this tutorial we explain the inference procedures developed for the sparse Gaussian process (GP) regression and Gaussian process latent variable model (GPLVM). Due to page limit the derivation given in Titsias (2009) and Titsias \& Lawrence (2010) is brief, hence getting a full picture of it requires collecting results from several different sources and a substantial amount of algebra to fill-in the gaps. Our main goal is thus to collect all the results and full derivations into one place to help speed up understanding this work. In doing so we present a re-parametrisation of the inference that allows it to be carried out in parallel. A secondary goal for this document is, therefore, to accompany our paper and open-source implementation of the parallel inference scheme for the models. We hope that this document will bridge the gap between the equations as implemented in code and those published in the original papers, in order to make it easier to extend existing work. We assume prior knowledge of Gaussian processes and variational inference, but we also include references for further reading where appropriate.},
  urldate = {2018-12-06},
  date = {2014-02-06},
  keywords = {Statistics - Machine Learning},
  author = {Gal, Yarin and van der Wilk, Mark},
  options = {useprefix=true},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Gal_van_der_Wilk_2014_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/NT9IKWIN/1402.html}
}

@article{chechik2005information,
  title = {Information {{Bottleneck}} for {{Gaussian Variables}}},
  volume = {6},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v6/chechik05a.html},
  issue = {Jan},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2018-12-06},
  date = {2005},
  pages = {165-188},
  keywords = {Information Bottleneck,IB},
  author = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chechik_et_al_2005_Information_Bottleneck_for_Gaussian_Variables.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/ANHUTSGS/chechik05a.html}
}

@article{higgins2016betavae,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  shorttitle = {Beta-{{VAE}}},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial...},
  urldate = {2018-12-10},
  date = {2016-11-04},
  keywords = {VAE,disentanglement},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Higgins_et_al_2016_Beta-VAE_-_Learning_Basic_Visual_Concepts_with_a_Constrained_Variational.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/DGWTCAL6/forum.html}
}

@article{esmaeili2018structured,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.02086},
  primaryClass = {cs, stat},
  title = {Structured {{Disentangled Representations}}},
  url = {http://arxiv.org/abs/1804.02086},
  abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
  urldate = {2018-12-12},
  date = {2018-04-05},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,VAE,disentanglement},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and van de Meent, Jan-Willem},
  options = {useprefix=true},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Esmaeili_et_al_2018_Structured_Disentangled_Representations.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/QIEMDHYJ/1804.html}
}

@article{higgins2018definition,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.02230},
  primaryClass = {cs, stat},
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  url = {http://arxiv.org/abs/1812.02230},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  urldate = {2018-12-12},
  date = {2018-12-05},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,disentanglement},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Higgins_et_al_2018_Towards_a_Definition_of_Disentangled_Representations.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/4759NAVM/1812.html}
}

@article{kim2018disentangling,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.05983},
  primaryClass = {cs, stat},
  title = {Disentangling by {{Factorising}}},
  url = {http://arxiv.org/abs/1802.05983},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \$\textbackslash{}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  urldate = {2018-12-17},
  date = {2018-02-16},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,VAE,disentanglement},
  author = {Kim, Hyunjik and Mnih, Andriy},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Kim_Mnih_2018_Disentangling_by_Factorising.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/GTHJGFSD/1802.html}
}

@article{alemi2017fixing,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.00464},
  primaryClass = {cs, stat},
  title = {Fixing a {{Broken ELBO}}},
  url = {http://arxiv.org/abs/1711.00464},
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  urldate = {2018-12-17},
  date = {2017-11-01},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,VAE},
  author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Alemi_et_al_2017_Fixing_a_Broken_ELBO.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/XUT58EFC/1711.html}
}

@book{lee2011introduction,
  langid = {english},
  location = {{New York}},
  title = {Introduction to {{Topological Manifolds}}},
  edition = {2},
  isbn = {978-1-4419-7939-1},
  url = {//www.springer.com/gb/book/9781441979391},
  abstract = {This book is an introduction to manifolds at the beginning graduate level. It contains the essential topological ideas that are needed for the further study of manifolds, particularly in the context of differential geometry, algebraic topology, and related fields. Its guiding philosophy is to develop these ideas rigorously but economically, with minimal prerequisites and plenty of geometric intuition.Although this second edition has the same basic structure as the first edition, it has been extensively revised and clarified; not a single page has been left untouched. The major changes include a new introduction to CW complexes (replacing most of the material on simplicial complexes in Chapter 5); expanded treatments of manifolds with boundary, local compactness, group actions, and proper maps; and a new section on paracompactness.This text is designed to be used for an introductory graduate course on the geometry and topology of manifolds. It should be accessible to any student who has completed a solid undergraduate degree in mathematics. The author’s book Introduction to Smooth Manifolds is meant to act as a sequel to this book.},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  publisher = {{Springer-Verlag}},
  urldate = {2019-01-28},
  date = {2011},
  author = {Lee, John},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Lee_2011_Introduction_to_Topological_Manifolds.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/9S2QN3YZ/9781441979391.html}
}

@article{hyvarinen1999nonlinear,
  title = {Nonlinear Independent Component Analysis: {{Existence}} and Uniqueness Results},
  volume = {12},
  issn = {0893-6080},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608098001403},
  doi = {10.1016/S0893-6080(98)00140-3},
  shorttitle = {Nonlinear Independent Component Analysis},
  abstract = {The question of existence and uniqueness of solutions for nonlinear independent component analysis is addressed. It is shown that if the space of mixing functions is not limited there exists always an infinity of solutions. In particular, it is shown how to construct parameterized families of solutions. The indeterminacies involved are not trivial, as in the linear case. Next, it is shown how to utilize some results of complex analysis to obtain uniqueness of solutions. We show that for two dimensions, the solution is unique up to a rotation, if the mixing function is constrained to be a conformal mapping together with some other assumptions. We also conjecture that the solution is strictly unique except in some degenerate cases, as the indeterminacy implied by the rotation is essentially similar to estimating the model of linear ICA.},
  number = {3},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  urldate = {2019-02-18},
  date = {1999-04-01},
  pages = {429-439},
  keywords = {Feature extraction,Independent component analysis,Blind source separation,Redundancy reduction},
  author = {Hyvärinen, Aapo and Pajunen, Petteri},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Hyvärinen_Pajunen_1999_Nonlinear_independent_component_analysis_-_Existence_and_uniqueness_results.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/X5AP36UX/S0893608098001403.html}
}

@article{tolstikhin2018wasserstein,
  title = {Wasserstein {{Auto}}-{{Encoders}}},
  url = {https://openreview.net/forum?id=HkL7n1-0b},
  abstract = {We propose the Wasserstein Auto-Encoder (WAE)---a new algorithm for building a generative model of the data distribution. WAE minimizes a penalized form of the Wasserstein distance between the...},
  urldate = {2019-03-01},
  date = {2018-02-15},
  author = {Tolstikhin, Ilya and Bousquet, Olivier and Gelly, Sylvain and Schoelkopf, Bernhard},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Tolstikhin_et_al_2018_Wasserstein_Auto-Encoders.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/NBX3QPWD/forum.html}
}

@article{pu2017vae,
  langid = {english},
  title = {{{VAE Learning}} via {{Stein Variational Gradient Descent}}},
  url = {https://arxiv.org/abs/1704.05155v3},
  abstract = {A new method for learning variational autoencoders (VAEs) is developed, based
on Stein variational gradient descent. A key advantage of this approach is that
one need not make parametric assumptions about the form of the encoder
distribution. Performance is further enhanced by integrating the proposed
encoder with importance sampling. Excellent performance is demonstrated across
multiple unsupervised and semi-supervised problems, including semi-supervised
analysis of the ImageNet data, demonstrating the scalability of the model to
large datasets.},
  urldate = {2019-03-01},
  date = {2017-04-18},
  author = {Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Li, Chunyuan and Han, Shaobo and Carin, Lawrence},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Pu_et_al_2017_VAE_Learning_via_Stein_Variational_Gradient_Descent.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/7YY7VXDW/1704.html}
}

@inproceedings{le2011ica,
  title = {{{ICA}} with {{Reconstruction Cost}} for {{Efficient Overcomplete Feature Learning}}},
  url = {http://papers.nips.cc/paper/4467-ica-with-reconstruction-cost-for-efficient-overcomplete-feature-learning.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-03-05},
  date = {2011},
  pages = {1017--1025},
  author = {Le, Quoc V. and Karpenko, Alexandre and Ngiam, Jiquan and Ng, Andrew Y.},
  editor = {Shawe-Taylor, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Le_et_al_2011_ICA_with_Reconstruction_Cost_for_Efficient_Overcomplete_Feature_Learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/9R7X9GKB/4467-ica-with-reconstruction-cost-for-efficient-overcomplete-feature-learning.html}
}

@article{locatello2018challenging,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1811.12359},
  primaryClass = {cs, stat},
  title = {Challenging {{Common Assumptions}} in the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  url = {http://arxiv.org/abs/1811.12359},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look on recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties `encouraged' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  urldate = {2019-03-29},
  date = {2018-11-29},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Rätsch, Gunnar and Gelly, Sylvain and Schölkopf, Bernhard and Bachem, Olivier},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Locatello_et_al_2018_Challenging_Common_Assumptions_in_the_Unsupervised_Learning_of_Disentangled.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/RFXIEM2U/1811.html}
}

@article{stuhmer2018isavae,
  title = {{{ISA}}-{{VAE}}: {{Independent Subspace Analysis}} with {{Variational Autoencoders}}},
  url = {https://openreview.net/forum?id=rJl_NhR9K7},
  shorttitle = {{{ISA}}-{{VAE}}},
  abstract = {Recent work has shown increased interest in using the Variational Autoencoder (VAE) framework to discover interpretable representations of data in an unsupervised way. These methods have focussed...},
  urldate = {2019-04-02},
  date = {2018-09-27},
  author = {Stühmer, Jan and Turner, Richard and Nowozin, Sebastian},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Stühmer_et_al_2018_ISA-VAE_-_Independent_Subspace_Analysis_with_Variational_Autoencoders.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/5DHD7EKX/forum.html}
}

@article{sinz2009characterization,
  title = {Characterization of the P-Generalized Normal Distribution},
  volume = {100},
  issn = {0047-259X},
  url = {http://www.sciencedirect.com/science/article/pii/S0047259X08001681},
  doi = {10.1016/j.jmva.2008.07.006},
  abstract = {It is a well known fact that invariance under the orthogonal group and marginal independence uniquely characterizes the isotropic normal distribution. Here, a similar characterization is provided for the more general class of differentiable bounded Lp-spherically symmetric distributions: Every factorial distribution in this class is necessarily p-generalized normal.},
  number = {5},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  urldate = {2019-04-12},
  date = {2009-05-01},
  pages = {817-820},
  keywords = {-spherically symmetric distributions,62E10,Characterization,Exponential power distribution,Generalized normal distribution,teatalk,mljc},
  author = {Sinz, Fabian and Gerwinn, Sebastian and Bethge, Matthias},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Sinz_et_al_2009_Characterization_of_the_p-generalized_normal_distribution.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/SG42WLSM/S0047259X08001681.html}
}

@article{olshausen1996emergence,
  langid = {english},
  title = {Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images},
  volume = {381},
  issn = {1476-4687},
  url = {https://www.nature.com/articles/381607a0},
  doi = {10.1038/381607a0},
  abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1–4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7–12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13–18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
  number = {6583},
  journaltitle = {Nature},
  urldate = {2019-04-12},
  date = {1996-06},
  pages = {607},
  author = {Olshausen, Bruno A. and Field, David J.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Olshausen_Field_1996_Emergence_of_simple-cell_receptive_field_properties_by_learning_a_sparse_code.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/2NFYWLXF/381607a0.html}
}

@article{olshausen1997sparse,
  title = {Sparse Coding with an Overcomplete Basis Set: {{A}} Strategy Employed by {{V1}}?},
  volume = {37},
  issn = {0042-6989},
  url = {http://www.sciencedirect.com/science/article/pii/S0042698997001697},
  doi = {10.1016/S0042-6989(97)00169-7},
  shorttitle = {Sparse Coding with an Overcomplete Basis Set},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  number = {23},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  urldate = {2019-04-12},
  date = {1997-12-01},
  pages = {3311-3325},
  keywords = {Coding,Gabor-wavelet,Natural images,V1},
  author = {Olshausen, Bruno A. and Field, David J.},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Olshausen_Field_1997_Sparse_coding_with_an_overcomplete_basis_set_-_A_strategy_employed_by_V1.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/N9IWHMNW/S0042698997001697.html}
}

@article{shimizu2006linear,
  title = {A {{Linear Non}}-{{Gaussian Acyclic Model}} for {{Causal Discovery}}},
  volume = {7},
  issn = {ISSN 1533-7928},
  url = {http://www.jmlr.org/papers/v7/shimizu06a.html},
  issue = {Oct},
  journaltitle = {Journal of Machine Learning Research},
  urldate = {2019-04-12},
  date = {2006},
  pages = {2003-2030},
  author = {Shimizu, Shohei and Hoyer, Patrik O. and Hyvärinen, Aapo and Kerminen, Antti},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Shimizu_et_al_2006_A_Linear_Non-Gaussian_Acyclic_Model_for_Causal_Discovery.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/KEIYBHTJ/shimizu06a.html}
}

@article{chen2018isolating,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.04942},
  primaryClass = {cs, stat},
  title = {Isolating {{Sources}} of {{Disentanglement}} in {{Variational Autoencoders}}},
  url = {http://arxiv.org/abs/1802.04942},
  abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \$\textbackslash{}beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \$\textbackslash{}beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
  urldate = {2019-04-24},
  date = {2018-02-13},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,_tablet},
  author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Chen_et_al_2018_Isolating_Sources_of_Disentanglement_in_Variational_Autoencoders.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/3S2WZXT2/1802.html}
}

@article{pu2017adversarial,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.04915},
  primaryClass = {cs},
  title = {Adversarial {{Symmetric Variational Autoencoder}}},
  url = {http://arxiv.org/abs/1711.04915},
  abstract = {A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (\$i\$) from observed data fed through the encoder to yield codes, and (\$ii\$) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (\$i\$) and (\$ii\$), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmark datasets.},
  urldate = {2019-04-30},
  date = {2017-11-13},
  keywords = {Computer Science - Machine Learning},
  author = {Pu, Yunchen and Wang, Weiyao and Henao, Ricardo and Chen, Liqun and Gan, Zhe and Li, Chunyuan and Carin, Lawrence},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Pu_et_al_2017_Adversarial_Symmetric_Variational_Autoencoder.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/3FJNHA9L/1711.html}
}

@article{nalisnick2017variational,
  title = {Variational {{Reference Priors}}},
  url = {https://openreview.net/forum?id=rJnjwsYde},
  abstract = {In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are...},
  urldate = {2019-04-30},
  date = {2017-02-09},
  author = {Nalisnick, Eric and Smyth, Padhraic},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Nalisnick_Smyth_2017_Variational_Reference_Priors.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/JPJJG2YC/forum.html}
}

@article{li2017alice,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01215},
  primaryClass = {cs, stat},
  title = {{{ALICE}}: {{Towards Understanding Adversarial Learning}} for {{Joint Distribution Matching}}},
  url = {http://arxiv.org/abs/1709.01215},
  shorttitle = {{{ALICE}}},
  abstract = {We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.},
  urldate = {2019-04-30},
  date = {2017-09-04},
  keywords = {Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yunchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Li_et_al_2017_ALICE_-_Towards_Understanding_Adversarial_Learning_for_Joint_Distribution.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/WQYGXKHU/1709.html}
}

@article{kim2018semiamortized,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.02550},
  primaryClass = {cs, stat},
  title = {Semi-{{Amortized Variational Autoencoders}}},
  url = {http://arxiv.org/abs/1802.02550},
  abstract = {Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.},
  urldate = {2019-04-30},
  date = {2018-02-07},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Computation and Language},
  author = {Kim, Yoon and Wiseman, Sam and Miller, Andrew C. and Sontag, David and Rush, Alexander M.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Kim_et_al_2018_Semi-Amortized_Variational_Autoencoders.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/8APNXGV9/1802.html}
}

@article{mescheder2017adversarial,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.04722},
  primaryClass = {cs},
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1701.04722},
  shorttitle = {Adversarial {{Variational Bayes}}},
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
  urldate = {2019-04-30},
  date = {2017-01-17},
  keywords = {Computer Science - Machine Learning},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Mescheder_et_al_2017_Adversarial_Variational_Bayes_-_Unifying_Variational_Autoencoders_and_Generative.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/KEYZXBZD/1701.html}
}

@article{chen2016infogan,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03657},
  primaryClass = {cs, stat},
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  url = {http://arxiv.org/abs/1606.03657},
  shorttitle = {{{InfoGAN}}},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  urldate = {2019-04-30},
  date = {2016-06-11},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Chen_et_al_2016_InfoGAN_-_Interpretable_Representation_Learning_by_Information_Maximizing.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/YRT56DUE/1606.html}
}

@article{goodfellow2014generative,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2019-04-30},
  date = {2014-06-10},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Goodfellow_et_al_2014_Generative_Adversarial_Networks.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/B5YRXQ5T/1406.html}
}

@article{dumoulin2016adversarially,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00704},
  primaryClass = {cs, stat},
  title = {Adversarially {{Learned Inference}}},
  url = {http://arxiv.org/abs/1606.00704},
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
  urldate = {2019-04-30},
  date = {2016-06-02},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Dumoulin_et_al_2016_Adversarially_Learned_Inference.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/JSWJMCRM/1606.html}
}

@article{donahue2016adversarial,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09782},
  primaryClass = {cs, stat},
  title = {Adversarial {{Feature Learning}}},
  url = {http://arxiv.org/abs/1605.09782},
  abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
  urldate = {2019-04-30},
  date = {2016-05-31},
  keywords = {Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Donahue_et_al_2016_Adversarial_Feature_Learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/8LFPDHD7/1605.html}
}

@article{kingma2016improving,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04934},
  primaryClass = {cs, stat},
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  url = {http://arxiv.org/abs/1606.04934},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  urldate = {2019-04-30},
  date = {2016-06-15},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Kingma_et_al_2016_Improving_Variational_Inference_with_Inverse_Autoregressive_Flow.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/MRQH4N3Y/1606.html}
}

@article{burda2015importance,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.00519},
  primaryClass = {cs, stat},
  title = {Importance {{Weighted Autoencoders}}},
  url = {http://arxiv.org/abs/1509.00519},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  urldate = {2019-04-30},
  date = {2015-09-01},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Burda_et_al_2015_Importance_Weighted_Autoencoders.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/K49LE6KY/1509.html}
}

@article{rezende2015variational,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05770},
  primaryClass = {cs, stat},
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  url = {http://arxiv.org/abs/1505.05770},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  urldate = {2019-04-30},
  date = {2015-05-21},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Methodology},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Rezende_Mohamed_2015_Variational_Inference_with_Normalizing_Flows.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/T87J5BGC/1505.html}
}

@article{rezende2014stochastic,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1401.4082},
  primaryClass = {cs, stat},
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  url = {http://arxiv.org/abs/1401.4082},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  urldate = {2019-04-30},
  date = {2014-01-16},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Methodology},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/VAE/Rezende_et_al_2014_Stochastic_Backpropagation_and_Approximate_Inference_in_Deep_Generative_Models.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/RMYH2A4P/1401.html}
}

@article{dinh2014nice,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1410.8516},
  primaryClass = {cs},
  title = {{{NICE}}: {{Non}}-Linear {{Independent Components Estimation}}},
  url = {http://arxiv.org/abs/1410.8516},
  shorttitle = {{{NICE}}},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  urldate = {2019-05-02},
  date = {2014-10-30},
  keywords = {Computer Science - Machine Learning},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Dinh_et_al_2014_NICE_-_Non-linear_Independent_Components_Estimation.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/K5EVWRZ8/1410.html}
}

@article{bengio2012representation,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1206.5538},
  primaryClass = {cs},
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  url = {http://arxiv.org/abs/1206.5538},
  shorttitle = {Representation {{Learning}}},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  urldate = {2019-05-03},
  date = {2012-06-24},
  keywords = {Computer Science - Machine Learning},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Bengio_et_al_2012_Representation_Learning_-_A_Review_and_New_Perspectives.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/VUD48SSK/1206.html}
}

@inproceedings{hinton1994autoencoders,
  title = {Autoencoders, Minimum Description Length and {{Helmholtz}} Free Energy},
  booktitle = {Advances in Neural Information Processing Systems},
  date = {1994},
  pages = {3--10},
  author = {Hinton, Geoffrey E. and Zemel, Richard S.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Hinton_Zemel_1994_Autoencoders,_minimum_description_length_and_Helmholtz_free_energy.pdf}
}

@article{comon1994independent,
  title = {Independent Component Analysis, a New Concept?},
  volume = {36},
  number = {3},
  journaltitle = {Signal processing},
  date = {1994},
  pages = {287--314},
  author = {Comon, Pierre},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Comon_1994_Independent_component_analysis,_a_new_concept.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/JACVHKGF/0165168494900299.html}
}

@article{kingma2014adam,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-05-12},
  date = {2014-12-22},
  keywords = {Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Kingma_Ba_2014_Adam_-_A_Method_for_Stochastic_Optimization.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/5LZF4VRP/1412.html}
}

@misc{zotero-758,
  type = {misc}
}

@misc{khemakhem2019quick,
  title = {Quick Proof of Identifiability for Noisy Nonlinear {{ICA}} Using Auxiliary Variables},
  date = {2019-05-17},
  author = {Khemakhem, Ilyes},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Mine/Khemakhem_2019_quick_proof_of_identifiability_for_noisy_nonlinear_ICA_using_auxiliary_variables.pdf}
}

@misc{zotero-760,
  type = {misc}
}

@article{ben-israel1999changeofvariables,
  title = {The {{Change}}-of-{{Variables Formula Using Matrix Volume}}},
  volume = {21},
  issn = {0895-4798},
  url = {http://dx.doi.org/10.1137/S0895479895296896},
  doi = {10.1137/S0895479895296896},
  abstract = {The matrix volume is a generalization, to rectangular matrices, of the absolute value of the determinant. In particular, the matrix volume can be used in change-of-variables formulae, instead of the determinant (if the Jacobi matrix of the underlying transformation is rectangular). This result is applicable to integration on surfaces, illustrated here by several examples.},
  number = {1},
  journaltitle = {SIAM J. Matrix Anal. Appl.},
  urldate = {2019-05-19},
  date = {1999-10},
  pages = {300--312},
  keywords = {change-of-variables in integration,determinants,Fourier transform,generalized Pythagorean theorem,Jacobians,matrix volume,Radon transform,surface integrals},
  author = {Ben-Israel, Adi},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Ben-Israel_1999_The_Change-of-Variables_Formula_Using_Matrix_Volume.pdf}
}

@inproceedings{pajunen1996nonlinear,
  title = {Nonlinear {{Blind Source Separation}} by {{Self}}-{{Organizing Maps}}},
  abstract = {In neural blind source separation most approaches have considered the linear source separation problem where the input data consist of unknown linear mixtures of unknown independent source signals. The solution is a linear transformation which makes the output vector components statistically independent. More generally we can consider nonlinear mixtures of sources. Then we can try to separate the sources by constructing mappings that make the components of the output vectors independent. We show that such a mapping can be approximately realized using self-organizing maps with rectangular map topology. We apply these mappings to the separation of nonlinear mixtures of sub-Gaussian sources.},
  booktitle = {In {{Proc}}. {{Int}}. {{Conf}}. on {{Neural Information Processing}}},
  date = {1996},
  pages = {1207--1210},
  author = {Pajunen, Petteri and Hyvärinen, Aapo and Karhunen, Juha},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Pajunen_et_al_1996_Nonlinear_Blind_Source_Separation_by_Self-Organizing_Maps.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/TBAFIC2L/summary.html}
}

@article{maddison2016concrete,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.00712},
  primaryClass = {cs, stat},
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  url = {http://arxiv.org/abs/1611.00712},
  shorttitle = {The {{Concrete Distribution}}},
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  urldate = {2019-05-22},
  date = {2016-11-02},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Maddison_et_al_2016_The_Concrete_Distribution_-_A_Continuous_Relaxation_of_Discrete_Random_Variables.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/XW8JUVUU/1611.html}
}

@article{gresele2019incomplete,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.06642},
  primaryClass = {cs, stat},
  title = {The {{Incomplete Rosetta Stone Problem}}: {{Identifiability Results}} for {{Multi}}-{{View Nonlinear ICA}}},
  url = {http://arxiv.org/abs/1905.06642},
  shorttitle = {The {{Incomplete Rosetta Stone Problem}}},
  abstract = {We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.},
  urldate = {2019-05-29},
  date = {2019-05-16},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {Gresele, Luigi and Rubenstein, Paul K. and Mehrjou, Arash and Locatello, Francesco and Schölkopf, Bernhard},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Gresele_et_al_2019_The_Incomplete_Rosetta_Stone_Problem_-_Identifiability_Results_for_Multi-View.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/EULSJDT2/1905.html}
}

@article{henaff2019dataefficient,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1905.09272},
  primaryClass = {cs},
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  url = {http://arxiv.org/abs/1905.09272},
  abstract = {Large scale deep learning excels when labeled images are abundant, yet data-efficient learning remains a longstanding challenge. While biological vision is thought to leverage vast amounts of unlabeled data to solve classification problems with limited supervision, computer vision has so far not succeeded in this `semi-supervised' regime. Our work tackles this challenge with Contrastive Predictive Coding, an unsupervised objective which extracts stable structure from still images. The result is a representation which, equipped with a simple linear classifier, separates ImageNet categories better than all competing methods, and surpasses the performance of a fully-supervised AlexNet model. When given a small number of labeled images (as few as 13 per class), this representation retains a strong classification performance, outperforming state-of-the-art semi-supervised methods by 10\% Top-5 accuracy and supervised methods by 20\%. Finally, we find our unsupervised representation to serve as a useful substrate for image detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset. We expect these results to open the door to pipelines that use scalable unsupervised representations as a drop-in replacement for supervised ones for real-world vision tasks where labels are scarce.},
  urldate = {2019-06-03},
  date = {2019-05-22},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  author = {Hénaff, Olivier J. and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and van den Oord, Aaron},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Hénaff_et_al_2019_Data-Efficient_Image_Recognition_with_Contrastive_Predictive_Coding.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/C5MXJ8VL/1905.html}
}

@article{oord2018representation,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.03748},
  primaryClass = {cs, stat},
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  url = {http://arxiv.org/abs/1807.03748},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  urldate = {2019-06-03},
  date = {2018-07-10},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Oord_et_al_2018_Representation_Learning_with_Contrastive_Predictive_Coding.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/W2YVNBC6/1807.html}
}

@book{wikibooks2016latex,
  title = {{{LaTeX}}},
  publisher = {{Wikibooks}},
  date = {2016},
  author = {Wikibooks},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Random/Wikibooks_2016_LaTeX.pdf}
}

@article{bertsekas1992auction,
  title = {Auction Algorithms for Network Flow Problems: {{A}} Tutorial Introduction},
  volume = {1},
  shorttitle = {Auction Algorithms for Network Flow Problems},
  number = {1},
  journaltitle = {Computational optimization and applications},
  date = {1992},
  pages = {7--66},
  author = {Bertsekas, Dimitri P.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Maths/Bertsekas_1992_Auction_algorithms_for_network_flow_problems_-_A_tutorial_introduction.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/VZ88WKKM/BF00247653.html}
}

@inproceedings{hyvarinen2019nonlinear,
  langid = {english},
  title = {Nonlinear {{ICA Using Auxiliary Variables}} and {{Generalized Contrastive Learning}}},
  url = {http://proceedings.mlr.press/v89/hyvarinen19a.html},
  abstract = {Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Rec...},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  urldate = {2019-07-01},
  date = {2019-04-11},
  pages = {859-868},
  author = {Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ICA/Hyvarinen_et_al_2019_Nonlinear_ICA_Using_Auxiliary_Variables_and_Generalized_Contrastive_Learning.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/5EVMSYK2/hyvarinen19a.html}
}

@inproceedings{podosinnikova2015rethinking,
  title = {Rethinking {{LDA}}: {{Moment Matching}} for {{Discrete ICA}}},
  url = {http://papers.nips.cc/paper/5671-rethinking-lda-moment-matching-for-discrete-ica.pdf},
  shorttitle = {Rethinking {{LDA}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-07-15},
  date = {2015},
  pages = {514--522},
  author = {Podosinnikova, Anastasia and Bach, Francis and Lacoste-Julien, Simon},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Podosinnikova_et_al_2015_Rethinking_LDA_-_Moment_Matching_for_Discrete_ICA.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/75SN7D7J/5671-rethinking-lda-moment-matching-for-discrete-ica.html}
}

@article{garnelo2018neural,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.01622},
  primaryClass = {cs, stat},
  title = {Neural {{Processes}}},
  url = {http://arxiv.org/abs/1807.01622},
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  urldate = {2019-07-19},
  date = {2018-07-04},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ML/Garnelo_et_al_2018_Neural_Processes.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/GLEX482H/1807.html}
}

@article{khemakhem2019variational,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.04809},
  primaryClass = {cs, stat},
  title = {Variational {{Autoencoders}} and {{Nonlinear ICA}}: {{A Unifying Framework}}},
  url = {http://arxiv.org/abs/1907.04809},
  shorttitle = {Variational {{Autoencoders}} and {{Nonlinear ICA}}},
  abstract = {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to a simple transformation, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.},
  urldate = {2019-07-19},
  date = {2019-07-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Khemakhem, Ilyes and Kingma, Diederik P. and Hyvärinen, Aapo},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/ICA/Khemakhem_et_al_2019_Variational_Autoencoders_and_Nonlinear_ICA_-_A_Unifying_Framework.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/EITY37XV/1907.html}
}

@book{peters2015causality,
  title = {Causality Script},
  date = {2015},
  author = {Peters, Jonas},
  file = {/nfs/nhome/live/ilyesk/Zotero/storage/JT3B6HLE/scriptChapter1-4.pdf}
}

@article{louizos2017causal,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08821},
  primaryClass = {cs, stat},
  title = {Causal {{Effect Inference}} with {{Deep Latent}}-{{Variable Models}}},
  url = {http://arxiv.org/abs/1705.08821},
  abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a specific patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders (VAE) which follow the causal structure of inference with proxies. We show our method is significantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
  urldate = {2019-07-19},
  date = {2017-05-24},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
  file = {/nfs/nhome/live/ilyesk/Google Drive/zotero/Causality/Louizos_et_al_2017_Causal_Effect_Inference_with_Deep_Latent-Variable_Models.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/7KWBIZUA/1705.html}
}

@article{singh2019kernel,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.00232},
  primaryClass = {cs, econ, math, stat},
  title = {Kernel {{Instrumental Variable Regression}}},
  url = {http://arxiv.org/abs/1906.00232},
  abstract = {Instrumental variable regression is a strategy for learning causal relationships in observational data. If measurements of input X and output Y are confounded, the causal relationship can nonetheless be identified if an instrumental variable Z is available that influences X directly, but is conditionally independent of Y given X. The classic two-stage least squares algorithm (2SLS) simplifies the estimation problem by modeling all relationships as linear functions. We propose kernel instrumental variable regression (KIV), a nonparametric generalization of 2SLS, modeling relations among X, Y, and Z as nonlinear functions in reproducing kernel Hilbert spaces (RKHSs). We prove the consistency of KIV under mild assumptions, and derive conditions under which the convergence rate achieves the minimax optimal rate for unconfounded, one-stage RKHS regression. In doing so, we obtain an efficient ratio between training sample sizes used in the algorithm's first and second stages. In experiments, KIV outperforms state of the art alternatives for nonparametric instrumental variable regression.},
  urldate = {2019-07-22},
  date = {2019-06-01},
  keywords = {62,Computer Science - Machine Learning,Economics - Econometrics,Mathematics - Functional Analysis,Mathematics - Statistics Theory,Statistics - Machine Learning},
  author = {Singh, Rahul and Sahani, Maneesh and Gretton, Arthur},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/ML/Singh_et_al_2019_Kernel_Instrumental_Variable_Regression.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/894D3RGH/1906.html}
}

@article{arjovsky2019invariant,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.02893},
  primaryClass = {cs, stat},
  title = {Invariant {{Risk Minimization}}},
  url = {http://arxiv.org/abs/1907.02893},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  urldate = {2019-07-22},
  date = {2019-07-05},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Arjovsky_et_al_2019_Invariant_Risk_Minimization.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/LB9UT8SR/1907.html}
}

@article{buhlmann2018invariance,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.08233},
  primaryClass = {stat},
  title = {Invariance, {{Causality}} and {{Robustness}}},
  url = {http://arxiv.org/abs/1812.08233},
  abstract = {We discuss recent work for causal inference and predictive robustness in a unifying way. The key idea relies on a notion of probabilistic invariance or stability: it opens up new insights for formulating causality as a certain risk minimization problem with a corresponding notion of robustness. The invariance itself can be estimated from general heterogeneous or perturbation data which frequently occur with nowadays data collection. The novel methodology is potentially useful in many applications, offering more robustness and better `causal-oriented' interpretation than machine learning or estimation in standard regression or classification frameworks.},
  urldate = {2019-07-22},
  date = {2018-12-19},
  keywords = {Statistics - Methodology,62J99},
  author = {Bühlmann, Peter},
  file = {/Users/ilyes/Google Drive (mrlaysoun@gmail.com)/zotero/Causality/Bühlmann_2018_Invariance,_Causality_and_Robustness.pdf;/nfs/nhome/live/ilyesk/Zotero/storage/PMIMDHD2/1812.html}
}


